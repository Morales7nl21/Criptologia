mostraremos cómo los nombres en la representación intermedia pueden convertirse en direcciones en el código destino.Muchos generadores de código particionan las instrucciones de representación intermedia en“bloques básicos”, los cuales consisten en secuencias de instrucciones que siempre se ejecutanen conjunto. El particionamiento de la representación intermedia en bloques básicos es el temade la sección 8.4. La siguiente sección presenta las transformaciones locales simples que puedenusarse para convertir los bloques básicos en bloques básicos modificados, a partir de los cualesse puede generar un código más eficiente. Estas transformaciones son una forma rudimentaria dela optimización de código, aunque no trataremos la teoría más profunda de la optimización de código sino hasta el capítulo 9. Un ejemplo de una transformación local útil es el descubrimiento de las subexpresiones comunes al nivel del código intermedio, y la sustitución resultante de lasoperaciones aritméticas por operaciones de copia más simples.La sección 8.6 presenta un algoritmo de generación de código simple, el cual genera códigopara cada instrucción en turno, manteniendo los operandos en registros el mayor tiempo posible. La salida de este tipo de generador de código puede mejorarse con facilidad mediante lastécnicas de optimización de mirilla (peephole), como las que veremos en la sección 8.7.Las secciones restantes exploran la selección de instrucciones y la repartición de registros.8.1 Cuestiones sobre el diseño de un generador de códigoAunque los detalles dependen de las características específicas de la representación intermedia,el lenguaje destino y el sistema en tiempo de ejecución, las tareas como la selección de instrucciones, la repartición y asignación de registros, y el ordenamiento de instrucciones se encuentranen el diseño de casi todos los generadores de código.El criterio más importante para un generador de código es que debe producir código correcto. La precisión de este código es muy relevante, debido al número de casos especiales que podríaenfrentar un generador de código. Dada la importancia en la precisión del código, diseñar ungenerador de código de manera que pueda implementarse, probarse y mantenerse con facilidades una meta primordial de diseño.Maq. Cap_8_AHO.indd 506 11/10/07 12:58:33 AM8.1.1 Entrada del generador de códigoLa entrada del generador de código es la representación intermedia del programa fuente producido por la interfaz de usuario, junto con la información en la tabla de símbolos que se utilizapara determinar las direcciones en tiempo de ejecución de los objetos de datos denotados porlos nombres en la representación intermedia.Entre las diversas opciones para la representación intermedia se encuentran las representaciones de tres direcciones, como los cuádruplos, tripletas, tripletas indirectos; las representaciones de máquinas virtuales como bytecodes y código máquina apilado; las representacioneslineales como la notación postfija; y las representaciones gráficas, como los árboles sintácticos y los GDAs. Muchos de los algoritmos en este capítulo se formulan en términos de lasrepresentaciones consideradas en el capítulo 6: código de tres direcciones, árboles y GDAs.Sin embargo, las técnicas que veremos pueden aplicarse también a las demás representacionesintermedias.En este capítulo vamos a suponer que la interfaz de usuario ha explorado, realizado elanálisis sintáctico y traducido el programa fuente en una representación intermedia de un nivel relativamente bajo, para que los valores de los nombres que aparecen en la representaciónintermedia puedan representarse mediante cantidades que la máquina destino pueda manipularen forma directa, como los números enteros y de punto flotante. También supondremos que sehan detectado todos los errores sintácticos y semánticos estáticos, que se ha llevado a cabo lacomprobación de tipos necesaria, y que se han insertado operadores de conversión de tipos entodos los lugares necesarios. Así, el generador de código puede proceder en base a la suposiciónde que su entrada está libre de estos tipos de errores.8.1.2 El programa destinoLa arquitectura del conjunto de instrucciones de la máquina destino tiene un impacto considerable en la dificultad de construir un buen generador de código que produzca código máquinade alta calidad. Las arquitecturas más comunes de las máquinas de destino son RISC (reducedinstruction set computer), CISC (complex instruction set computer) y basadas en pilas.Por lo general, una máquina RISC tiene muchos registros, instrucciones de tres direcciones, modos de direccionamiento simple y una arquitectura del conjunto de instruccionesrelativamente sencilla. En contraste, una máquina CISC, por lo general, tiene menos registros,instrucciones de dos direcciones, una variedad de modos de direccionamiento, varias clases deregistros, instrucciones de longitud variable e instrucciones con efectos adicionales.En una máquina basada en pila, las operaciones se realizan metiendo operandos en una pila,y después llevando a cabo las operaciones con los operandos en la parte superior de la pila. Paralograr un alto rendimiento, la parte superior de la pila se mantiene, por lo general, en los registros. Estas máquinas desaparecieron casi por completo, ya que se creía que la organización de lapila era demasiado limitante y requería demasiadas operaciones de intercambio y copiado.No obstante, las arquitecturas basadas en pila revivieron con la introducción de la Máquinavirtual de Java (JVM). Ésta es un intérprete de software para bytecodes de Java, un lenguajeintermedio producido por los compiladores de Java. El intérprete proporciona compatibilidadde software a través de varias plataformas, un importante factor en el éxito de Java.8.1 Cuestiones sobre el diseño de un generador de código 507Maq. Cap_8_AHO.indd 507 11/10/07 12:58:33 AM508 Capítulo 8. Generación de códigoPara superar el castigo sobre el alto rendimiento de la interpretación, que puede estar en elorden de un factor de 10, se crearon los compiladores Java just-in-time (JIT). Estos compiladores JIT traducen los códigos de byte en tiempo de ejecución al conjunto de instrucciones dehardware nativo de la máquina de destino. Otro método para mejorar el rendimiento de Javaes construir un compilador que compile directamente en las instrucciones de máquina de lamáquina destino, pasando por alto los bytecodes de Java por completo.Producir un programa en lenguaje máquina absoluto como salida tiene la ventaja de que puede colocarse en una ubicación fija en la memoria, y ejecutarse de inmediato. Los programas puedencompilarse y ejecutarse con rapidez.Producir un programa en lenguaje máquina reubicable (que a menudo se le conoce comomódulo objeto) como salida permite que los subprogramas se compilen por separado. Un conjunto de módulos objeto reubicables puede enlazarse y cargarse para que lo ejecute un cargadorde enlace. Aunque debemos sufrir la sobrecarga adicional de enlazar y cargar si producimosmódulos objeto reubicables, obtenemos mucha flexibilidad al poder compilar las subrutinas porseparado y llamar a otros programas (compilados con anterioridad) desde un módulo objeto. Sila máquina destino no maneja la reubicación de manera automática, el compilador debe proporcionar información de reubicación explícita al cargador para enlazar los módulos del programaque se compilaron por separado.Producir un programa en lenguaje ensamblador como salida facilita de alguna manera elproceso de la generación de código. Podemos generar instrucciones simbólicas y utilizar las facilidades de las macros del ensamblador para ayudar en la generación de código. El precio quese paga es el paso de ensamblado después de la generación de código.En este capítulo utilizaremos una computadora tipo RISC muy simple como nuestra máquina destino. Le agregaremos algunos modos de direccionamiento tipo CISC, para poder hablartambién sobre las técnicas de generación de código para las máquinas CISC. Por cuestión delegibilidad, usaremos código ensamblador como el lenguaje destino. Siempre y cuando puedancalcularse direcciones a partir de desplazamientos y que la demás información se almacene enla tabla de símbolos, el generador de código puede producir direcciones reubicables o absolutaspara los nombres, con la misma facilidad que las direcciones simbólicas.8.1.3 Selección de instruccionesEl generador de código debe asignar el programa de representación intermedia a una secuencia de código que la máquina de destino pueda ejecutar. La complejidad de realizar esta asignación se determina mediante factores como:• El nivel de la representación intermedia.• La naturaleza de la arquitectura del conjunto de instrucciones.• La calidad deseada del código generado.Si la representación intermedia es de alto nivel, el generador de código puede traducir cada instrucción de este tipo en una secuencia de instrucciones de máquina, usando plantillas de código.Sin embargo, tal generación de código instrucción por instrucción produce a menudo código debaja calidad, que requiere de más optimización. Si la representación intermedia refleja algunosMaq. Cap_8_AHO.indd 508 11/10/07 12:58:34 AMde los detalles de bajo nivel de la máquina subyacente, entonces el generador de código puedeusar esta información para generar secuencias de código más eficientes.La naturaleza del conjunto de instrucciones de la máquina destino tiene un fuerte efectosobre la dificultad de la selección de instrucciones. Por ejemplo, la uniformidad y precisióndel conjunto de instrucciones son factores importantes. Si la máquina destino no soporta cadatipo de datos de manera uniforme, entonces cada excepción a la regla general requiere de unmanejo especial. Por ejemplo, en ciertas máquinas las operaciones de punto flotante se realizanmediante registros separados.Las velocidades de las instrucciones y las características específicas de las máquinas sonotros factores importantes. Si no nos preocupa la eficiencia del programa destino, la selección deinstrucciones es un proceso simple. Para cada tipo de instrucción de tres direcciones, podemosdiseñar un esqueleto de código que defina el código destino a generar para esa construcción. Porejemplo, toda instrucción de tres direcciones de la forma x = y + z, en donde x, y y z se asignanen forma estática, puede traducirse en la siguiente secuencia de código:LD R0, y // R0 = y (carga y en el registro R0)ADD R0, R0, z // R0 = R0 + z (suma z a R0)ST x, R0 // x = R0 (almacena R0 en x)A menudo, esta estrategia produce operaciones de carga y almacenamiento redundantes.Por ejemplo, la siguiente secuencia de instrucciones de tres direcciones:a = b + cd = a + ese traduciría en el siguiente código:LD R0, b // R0 = bADD R0, R0, c // R0 = R0 + cST a, R0 // a = R0LD R0, a // R0 = aADD R0, R0, e // R0 = R0 + eST d, R0 // d = R0Aquí, la cuarta instrucción es redundante, ya que carga un valor que acaba de almacenarse,y de igual forma la tercera, si no se utiliza la a más adelante.Por lo general, la calidad del código generado se determina en base a su velocidad y tamaño.En la mayoría de las máquinas se puede implementar un programa representación intermediamediante muchas secuencias de código distintas, con diferencias considerables en cuanto al rendimiento de las distintas implementaciones. Así, una traducción simple del código intermediopuede producir un código de destino correcto, pero demasiado ineficiente.Por ejemplo, si la máquina de destino tiene una instrucción de “incremento” (INC), entoncesla instrucción de tres direcciones a = a + 1 puede implementarse con más eficiencia mediante lainstrucción individual INC a, en vez de hacerlo mediante una secuencia más obvia para cargara en un registro, sumar uno al registro y después almacenar el resultado de vuelta en a:8.1 Cuestiones sobre el diseño de un generador de código 509Maq. Cap_8_AHO.indd 509 11/10/07 12:58:34 AM510 Capítulo 8. Generación de códigoLD R0, a // R0 = aADD R0, R0, #1 // R0 = R0 + 1ST a, R0 // a = R0Debemos saber qué tan eficientes son las instrucciones para poder diseñar buenas secuencias de código pero, por desgracia, a menudo esta información es difícil de obtener. Para decidirqué secuencia de código máquina es mejor para una construcción de tres direcciones dada, talvez también sea necesario tener un conocimiento acerca del contexto en el que aparece esaconstrucción.En la sección 8.9 veremos que la selección de instrucciones puede modelarse como unproceso de búsqueda de coincidencias con un patrón tipo árbol, en donde representamos a larepresentación intermedia y a las instrucciones de máquina como árboles. Después tratamos de“revestir” el árbol de la representación intermedia con un conjunto de subárboles que correspondan a las instrucciones de máquina. Si asociamos un costo a cada subárbol de instruccionesde máquina, podemos usar la programación dinámica para generar secuencias de código óptimas. En la sección 8.11 hablaremos sobre la programación dinámica.8.1.4 Asignación de registrosUn problema clave en la generación de código es decidir qué valores guardar en qué registros.Los registros son la unidad computacional más veloz en la máquina destino pero, por lo general,no tenemos suficientes como para contener todos los valores. Los valores que no se guardan enregistros deben residir en la memoria. Las instrucciones que involucran operandos de registrosson invariablemente más cortas y rápidas que las que involucran el uso de operandos en lamemoria, por lo que la utilización eficiente de los registros es muy importante.A menudo, el uso de los registros se divide en dos subproblemas:1. Repartición de registros, durante la cual seleccionamos el conjunto de variables que residirán en los registros, en cada punto del programa.2. Asignación de registros, durante la cual elegimos el registro específico en el que residiráuna variable.Es difícil encontrar una asignación óptima de registros a variables, incluso en las máquinascon un solo registro. En sentido matemático, el problema es NP-completo. El problema se complica aún más debido a que el hardware y el sistema operativo de la máquina destino puedenrequerir que se sigan ciertas convenciones de uso de registros.Ejemplo 8.1: Ciertas máquinas requieren pares de registros (un registro con numeración pary el siguiente con numeración impar) para ciertos operandos y resultados. Por ejemplo, en algunas máquinas la multiplicación y la división de enteros implican el uso de pares de registros.La instrucción de multiplicación es de la siguiente forma:M x, yen donde x, el multiplicando, es el registro par de un par de registros par/impar y y, el multiplicador, es el registro impar. El producto ocupa todo el par de registros par/impar completo.La instrucción de división es de la siguiente forma:Maq. Cap_8_AHO.indd 510 11/10/07 12:58:35 AMD x, yen donde el dividendo ocupa un par de registros par/impar cuyo registro par es x; el divisor es y.Después de la división, el registro par contiene el residuo y el registro impar el cociente.Ahora, considere las dos secuencias de código de tres direcciones en la figura 8.2, en donde laúnica diferencia entre (a) y (b) es el operador en la segunda instrucción. Las secuencias de códigoensamblador más cortas para (a) y (b) se muestran en la figura 8.3.Figura 8.2: Dos secuencias de código de tres direccionesFigura 8.3: Secuencias de código máquina óptimasRi representa al registro i. SRDA significa Aritmética doble de desplazamiento a la derecha ySRDA R0,32 desplaza el dividendo en R1 y borra R0, para que todos los bits sean iguales a subit de signo. L, ST y A significan cargar, almacenar y sumar, respectivamente. Observe que laelección óptima para el registro en el que se va a cargar a depende de lo que pasará en últimainstancia a t. ✷En la sección 8.8 veremos estrategias para la repartición y asignación de registros. La sección 8.10 muestra que para ciertas clases de máquinas podemos construir secuencias de códigoque evalúen expresiones, usando la menor cantidad posible de registros.8.1.5 Orden de evaluaciónEl orden en el que se realizan los cálculos puede afectar la eficiencia del código destino. Comoveremos, ciertos órdenes de los cálculos requieren menos registros que otros para contener resultados inmediatos. Sin embargo, el proceso de elegir el mejor orden en el caso general es unproblema NP-completo difícil. Al principio, debemos evitar el problema mediante la generación8.1 Cuestiones sobre el diseño de un generador de código 511Maq. Cap_8_AHO.indd 511 11/10/07 12:58:35 AM512 Capítulo 8. Generación de códigode código para las instrucciones de tres direcciones, en el orden en el que el generador de códigointermedio las produjo. En el capítulo 10 estudiaremos la programación del código para las máquinas con canalizaciones, que pueden ejecutar varias operaciones en un solo ciclo del reloj.8.2 El lenguaje destinoUn requisito previo para diseñar un buen generador de código es estar familiarizado con lamáquina destino y su conjunto de instrucciones. Por desgracia, en una explicación generalsobre la generación de código no es posible describir una máquina destino con suficientedetalle como para generar un buen código para un lenguaje completo en esa máquina. Eneste capítulo vamos a usar código ensamblador como lenguaje destino para una computadora simple que representa a muchas máquinas con registros. Sin embargo, las técnicas de generación que presentaremos en este capítulo pueden usarse en muchas otras clases de máquinastambién.8.2.1 Un modelo simple de máquina destinoNuestra computadora destino modela una máquina de tres direcciones con operaciones decarga y almacenamiento, operaciones de cálculo, operaciones de salto y saltos condicionales.La computadora subyacente es una máquina con direccionamiento por bytes y n registros depropósito general, R0, R1,…, Rn − 1. Un lenguaje ensamblador completo tendría muchas instrucciones. Para evitar ocultar los conceptos en una avalancha de detalles, vamos a usar unconjunto bastante limitado de instrucciones, y supondremos que todos los operandos son enteros. La mayoría de las instrucciones consisten en un operador, seguido de un destino, seguidode una lista de operandos de origen. Puede ir una etiqueta después de una instrucción. Vamosa suponer que existen los siguientes tipos de instrucciones:• Operaciones de carga: La instrucción LD dst, dir carga el valor que hay en la ubicación diry lo coloca en la ubicación dst. Esta instrucción denota la asignación dst = dir. La formamás común de esta instrucción es LD r, x, que carga el valor que hay en la ubicación x ylo coloca en el registro r. Una instrucción de la forma LD r1, r 2 es una copia de registro aregistro, en la cual se copia el contenido del registro r 2 al registro r 1.• Operaciones de almacenamiento: La instrucción ST x, r almacena el valor que hay en elregistro r y lo coloca en la ubicación x. Esta instrucción denota la asignación x = r.• Operaciones de cálculo de la forma OP dst, orig 1, orig 2, en donde OP es un operador comoADD o SUB, y dst, orig 1 y orig 2 son ubicaciones, no necesariamente distintas. El efecto deesta instrucción de máquina es aplicar la operación representada por OP a los valores enlas ubicaciones orig 1 y orig 2, y colocar el resultado de esta operación en la ubicación dst.Por ejemplo, SUB r1, r 2, r 3 calcula r 1 = r 2 − r 3. Cualquier valor que haya estado almacenado en r 1 se pierde, pero si r 1 es r 2 o r 3, el valor anterior se lee primero. Los operadoresunarios que sólo reciben un operando no tienen orig 2.Maq. Cap_8_AHO.indd 512 11/10/07 12:58:36 AM• Saltos incondicionales: La instrucción BR L provoca que el control se bifurque hacia lainstrucción de máquina con la etiqueta L. (BR significa bifurcación.)• Saltos condicionales de la forma Bcond r, L, en donde r es un registro, L una etiqueta ycond representa a cualquiera de las evaluaciones comunes sobre los valores del registro r.Por ejemplo, BLTZ r, L produce un salto a la etiqueta L si el valor en el registro r es menorque cero, y permite que el control pase a la siguiente instrucción de máquina si no es así.Vamos a suponer que nuestra máquina destino tiene una variedad de modos de direccionamiento:• En las instrucciones, una ubicación puede ser el nombre de una variable x que haga referencia a la ubicación de memoria que está reservada para x (es decir, el valor l de x ).• Una ubicación también puede ser una dirección indexada de la forma a(r ), en donde aes una variable y r un registro. La ubicación de memoria denotada por a(r ) se calculatomando el valor l de a y sumándolo al valor en el registro r. Por ejemplo, la instrucciónLD R1,a(R2) tiene el efecto de establecer R1 = contenido (a + contenido (R2)), en dondecontenido (x ) denota el contenido del registro o la ubicación de memoria representadapor x. Este modo de direccionamiento es útil para acceder a los arreglos, en donde a esla dirección base del arreglo (es decir, la dirección del primer elemento) y r contiene elnúmero de bytes después de esa dirección que deseamos avanzar para llegar a uno de loselementos del arreglo a.• Una ubicación de memoria puede ser un entero indexado por un registro. Por ejemplo,LD R1, 100(R2) tiene el efecto de establecer R1 = contenido (100 + contenido (R2)); esdecir, carga en R1 el valor que hay en la ubicación de memoria que se obtiene al sumar100 al contenido del registro R2. Esta característica es útil para seguir apuntadores, comoveremos en el ejemplo que viene a continuación.• También permitimos dos modos de direccionamiento indirecto: *r significa que la ubicación de memoria que se encuentra en la ubicación representada por el contenido delregistro r, y *100(r) indica la ubicación de memoria que se encuentra en la ubicación quese obtiene al sumar 100 al contenido de r. Por ejemplo, LD R1, *100(R2) tiene el efectode establecer R1 = contenido (contenido (100 + contenido (R2))); es decir, carga en R1 elvalor que hay en la ubicación de memoria almacenada en la ubicación de memoria que seobtiene al sumar 100 al contenido del registro R2.• Por último, permitimos un modo de direccionamiento constante inmediato. La constantelleva el prefijo #. La instrucción LD R1, #100 carga el entero 100 en el registro R1, y ADDR1,R1,#100 carga el entero 100 en el registro R1.Los comentarios al final de las instrucciones van precedidos por //.Ejemplo 8.2: La instrucción de tres direcciones x = y − z puede implementarse mediantelas siguientes instrucciones de máquina:8.2 El lenguaje destino 513Maq. Cap_8_AHO.indd 513 11/10/07 12:58:37 AM514 Capítulo 8. Generación de códigoLD R1, y // R1 = yLD R2, z // R2 = zSUB R1, R1, R2 // R1 = R1 − R2ST x, R1 // x = R1Tal vez podamos hacerlo mejor. Una de las metas de un buen algoritmo de generación decódigo es evitar el uso de estas cuatro instrucciones, siempre que sea posible. Por ejemplo, yy z podrían calcularse en un registro, para así evitar el (los) paso (s) con LD. De igual forma,podríamos evitar almacenar x si su valor se utiliza dentro del conjunto de registros y no senecesita posteriormente.Suponga que a es un arreglo cuyos elementos son valores de 8 bytes, tal vez números reales.Suponga además que los elementos de a se indexan empezando en 0. Podemos ejecutar la instrucción de tres direcciones b = a[i] mediante las siguientes instrucciones de máquina:LD R1, i // R1 = iMUL R1, R1, 8 // R1 = R1 * 8LD R2, a(R1) // R2 = contenido(a + contenido(R1))ST b, R2 // b = R2Es decir, el segundo paso calcula 8i, y el tercer paso coloca en el registro R2 el valor en el i-ésimo elemento de a; el que se encuentra en la ubicación que está 8i bytes después de la direcciónbase del arreglo a.De manera similar, la asignación en el arreglo a, representada por la instrucción de tres direcciones a[j] = c, se implementa mediante:LD R1, c // R1 = cLD R2, j // R2 = jMUL R2, R2, 8 // R2 = R2 * 8ST a(R2), R1 // contenido(a + contenido(R2)) = R1Para implementar una indirección simple de un apuntador, como la instrucción de tresdirecciones x = *p, podemos usar instrucciones de máquina como:LD R1, p // R1 = pLD R2, 0(R1) // R2 = contenido(0 + contenido(R1))ST x, R2 // x = R2La asignación a través de un apuntador *p = y se implementa de manera similar en códigomáquina, mediante:LD R1, p // R1 = pLD R2, y // R2 = yST 0(R1), R2 // contenido(0 + contenido(R1)) = R2Maq. Cap_8_AHO.indd 514 11/10/07 12:58:37 AMPor último, considere una instrucción de tres direcciones de salto condicional como:if x < y goto LEl equivalente en código máquina sería algo como:LD R1, x // R1 = xLD R2, y // R2 = ySUB R1, R1, R2 // R1 = R1 − R2BLTZ R1, M // if R1 < 0 salta a MAquí, M es la etiqueta que representa a la primera instrucción de máquina generada a partir dela instrucción de tres direcciones que tiene la etiqueta L. Al igual que para cualquier instrucciónde tres direcciones, esperamos poder ahorrarnos algunas de estas instrucciones de máquina, debido a que las operaciones necesarias ya se encuentran en los registros, o porque el resultadono debe almacenarse. ✷8.2.2 Costos del programa y las instruccionesA menudo asociamos un costo con la compilación y la ejecución de un programa. Dependiendodel aspecto del programa que nos interese optimizar, algunas medidas de costos comunes sonla longitud del tiempo de compilación y el tamaño, el tiempo de ejecución y el consumo deenergía del programa de destino.La determinación del costo actual de compilar y ejecutar un programa es un problema complejo. Buscar un programa destino óptimo para un programa fuente dado es un problema indecidible, y muchos de los subproblemas involucrados son NP-hard. Como hemos indicado antes, enla generación de código debemos a menudo contentarnos con las técnicas de heurística que producen buenos programas destino, pero que no necesariamente son óptimos.Durante el resto de este capítulo, vamos a suponer que cada instrucción en lenguaje destinotiene un costo asociado. Por simplicidad, consideraremos que el costo de una instrucción seráde uno más los costos asociados con los modos de direccionamiento de los operandos. Este costocorresponde a la longitud en palabras de la instrucción. Los modos de direccionamiento queimplican el uso de registros tienen un costo adicional de cero, mientras que los que implican eluso de una ubicación de memoria o constante tienen un costo adicional de uno, ya que dichosoperandos tienen que almacenarse en las palabras que van después de la instrucción. He aquíalgunos ejemplos:• La instrucción LD R0, R1 copia el contenido del registro R1 al registro R0. Esta instrucción tiene un costo de uno, ya que no se requieren palabras adicionales de memoria.• La instrucción LD R0, M carga el contenido de la ubicación de memoria M al registro R0.El costo es de dos, ya que la dirección de la ubicación de memoria M está en la palabraque va después de la instrucción.• La instrucción LD R1, *100(R2) carga al registro R1 el valor proporcionado por contenido(contenido (100 + contenido (R2))). El costo es de tres, debido a que la constante 100se almacena en la palabra que va después de la instrucción.8.2 El lenguaje destino 515Maq. Cap_8_AHO.indd 515 11/10/07 12:58:38 AM516 Capítulo 8. Generación de códigoEn este capítulo vamos a suponer que el costo de un programa en lenguaje destino, dadacierta entrada, es la suma de los costos de las instrucciones individuales que se ejecutan cuandoel programa se ejecuta sobre esa entrada. Los buenos algoritmos de generación de código buscan disminuir al mínimo la suma de los costos de las instrucciones ejecutadas por el programadestino que se genera sobre las entradas ordinarias. Más adelante veremos que en algunos casos,podemos realmente generar código óptimo para las expresiones en ciertas clases de máquinasde registro.8.2.3 Ejercicios para la sección 8.2Ejercicio 8.2.1: Genere el código para las siguientes instrucciones de tres direcciones, suponiendo que todas las variables se almacenan en ubicaciones de memoria.a) x = 1b) x = ac) x = a + 1d) x = a + be) Las siguientes dos instrucciones:x = b * cy = a + xEjercicio 8.2.2: Genere el código para las siguientes instrucciones de tres direcciones, suponiendo que a y b son arreglos cuyos elementos son valores de 4 bytes.a) La siguiente secuencia de cuatro instrucciones:x = a[i]y = b[j]a[i] = yb[j] = xb) La siguiente secuencia de tres instrucciones:x = a[i]y = b[i]z = x * yc) La siguiente secuencia de tres direcciones:x = a[i]y = b[x]a[i] = yMaq. Cap_8_AHO.indd 516 11/10/07 12:58:38 AMEjercicio 8.2.3: Genere el código para la siguiente secuencia de tres direcciones, suponiendoque p y q se encuentran en ubicaciones de memoria:Ejercicio 8.2.4: Genere el código para la siguiente secuencia, suponiendo que x, y y z se encuentran en ubicaciones de memoria:Ejercicio 8.2.5: Genere el código para la siguiente secuencia, suponiendo que n está en unaubicación de memoria:Ejercicio 8.2.6: Determine los costos de las siguientes secuencias de instrucciones:8.2 El lenguaje destino 517Maq. Cap_8_AHO.indd 517 11/10/07 12:58:39 AM518 Capítulo 8. Generación de código8.3 Direcciones en el código destinoEn esta sección le mostraremos cómo pueden convertirse los nombres en la representación intermedia en direcciones en el código destino, analizando la generación de código para las llamadassimples a procedimientos y sus retornos, mediante el uso de la asignación estática y de pila. Enla sección 7.1 describimos la forma en que cada programa se ejecuta en su propio espacio dedirecciones lógicas, que se particionaba en cuatro áreas de código y cuatro de datos:1. Un área de Código determinada en forma estática, la cual contiene el código destino ejecutable. El tamaño del código destino se puede determinar en tiempo de compilación.2. Un área de datos Estática determinada en forma estática, para contener constantes globales y demás datos que genera el compilador. El tamaño de las constantes globales y delos datos del compilador también puede determinarse en tiempo de compilación.3. Un área Montículo administrada en forma dinámica, para contener objetos de datos quese asignan y se liberan durante la ejecución del programa. El tamaño del Montículo nopuede determinarse en tiempo de compilación.4. Un área Pila administrada en forma dinámica, para contener los registros de activacióna medida que se crean y se destruyen, durante las llamadas a los procedimientos y susretornos. Al igual que el Montículo, el tamaño de la Pila no puede determinarse en tiempo de compilación.8.3.1 Asignación estáticaPara ilustrar la generación de código para las llamadas y retornos simplificados de procedimientos, vamos a enfocarnos en las siguientes instrucciones de tres direcciones:• call receptor• return• halt• action, que es un receptáculo para otras instrucciones de tres direcciones.El tamaño y la distribución de los registros de activación se determinan mediante el generadorde código, a través de la información de los nombres almacenados en la tabla de símbolos. Primero vamos a ilustrar cómo almacenar la dirección de retorno en un registro de activación duranteMaq. Cap_8_AHO.indd 518 11/10/07 12:58:40 AMla llamada a un procedimiento, y cómo devolverle el control después de la llamada. Por conveniencia, asumiremos que la primera ubicación en la activación contiene la dirección de retorno.Primero vamos a considerar el código necesario para implementar el caso más simple, la asignación estática. Aquí, una instrucción call receptor en el código intermedio puede implementarsemediante una secuencia de dos instrucciones de máquina destino:ST receptor.areaEstatica, #aqui + 20BR receptor.areaCodigoLa instrucción ST guarda la dirección de retorno al principio del registro de activación parareceptor, y la instrucción BR transfiere el control al código de destino para el procedimiento receptor al que se llamó. El atributo antes de receptor.areaEstatica es una constante que proporcionala dirección del inicio del registro de activación para receptor, y el atributo receptor.areaCodigo esuna constante que se refiere a la dirección de la primera instrucción del procedimiento receptor alque se llamó en el área Código de la memoria en tiempo de ejecución.El operando #aqui + 20 en la instrucción ST es la dirección de retorno literal; es la direcciónde la instrucción que sigue después de la instrucción BR. Suponemos que #aqui es la dirección de lainstrucción actual, y que las tres constantes más las dos instrucciones en la secuencia de llamada tienen una longitud de 5 palabras, o 20 bytes.El código para un procedimiento termina con un retorno al procedimiento que hizo lallamada, sólo que el primer procedimiento no tiene emisor, por lo que su instrucción final esHALT, la cual devuelve el control al sistema operativo. Una instrucción return receptor puedeimplementarse mediante una instrucción simple de salto:BR *receptor.areaEstaticala cual transfiere el control a la dirección guardada al principio del registro de activación parareceptor.Ejemplo 8.3: Suponga que tenemos el siguiente código de tres direcciones:// código para caction 1call paction 2halt // código para paction 3returnLa figura 8.4 muestra el programa destino para este código de tres direcciones. Usamos laseudoinstrucción ACTION para representar la secuencia de instrucciones de máquina que van aejecutar la instrucción action, que representa el código de tres direcciones que no es relevantepara esta discusión. Empezamos de manera arbitraria el código para el procedimiento c en ladirección 100, y para el procedimiento p en la dirección 200. Suponemos que cada instrucciónACTION requiere 20 bytes. Además, suponemos que los registros de activación para estos procedimientos se asignación de manera estática en las ubicaciones 300 y 364, respectivamente.Las instrucciones que empiezan en la dirección 100 implementan a las siguientes instrucciones:8.3 Direcciones en el código destino 519Maq. Cap_8_AHO.indd 519 11/10/07 12:58:41 AM520 Capítulo 8. Generación de códigoaction 1; call p; action 2; haltdel primer procedimiento c. Por lo tanto, la ejecución empieza con la instrucción ACTION 1 enla dirección 100. La instrucción ST en la dirección 120 guarda la dirección de retorno 140 en elcampo de estado de la máquina, que es la primera palabra en el registro de activación de p. Lainstrucción BR en la dirección 132 transfiere el control a la primera instrucción en el código dedestino del procedimiento p que se llamó. // código para c 100: ACTION 1 // código para action 1 120: ST 364, #140 // guarda la dirección de retorno 140 en la ubicación 364 132: BR 200 // llama a p 140: ACTION 2 160: HALT // regresa al sistema operativo ... // código para p 200: ACTION 3 220: BR *364 // regresa a la dirección guardada en la ubicación 364 ... // 300-363 contienen el registro de activación para c 300: // dirección de retorno 304: // datos locales para c ... // 364-451 contienen el registro de activación para p 364: // dirección de retorno 368: // datos locales para pFigura 8.4: Código destino para la asignación estáticaDespués de ejecutar ACTION3, se ejecuta la instrucción de salto en la ubicación 220. Comola ubicación 140 se guardó en la dirección 364 mediante la secuencia de llamada anterior, *364representa 140 cuando se ejecuta la instrucción BR en la dirección 220. Por lo tanto, cuandotermina el procedimiento p, el control regresa a la dirección 140 y se reanuda la ejecución delprocedimiento c. ✷8.3.2 Asignación de pilaLa asignación estática se puede convertir en asignación de pila mediante el uso de direccionesrelativas para el almacenamiento en los registros de activación. Sin embargo, en la asignaciónde pila la posición de un registro de activación para un procedimiento no se conoce sino hastael tiempo de ejecución. Por lo general, esta posición se almacena en un registro, por lo que sepuede acceder a las palabras en el registro de activación como desplazamientos a partir delvalor en este registro. El modo de direccionamiento indexado para nuestra máquina destino esconveniente para este fin.Las direcciones relativas en un registro de activación pueden tomarse como desplazamientosa partir de cualquier posición conocida en el registro de activación, como vimos en el capítulo 7.Maq. Cap_8_AHO.indd 520 11/10/07 12:58:42 AMPor conveniencia, vamos a usar desplazamientos positivos, manteniendo en un registro SP unapuntador al inicio del registro de activación en la parte superior de la pila. Cuando ocurre unallamada a un procedimiento, el procedimiento que llama incrementa a SP y transfiere el controlal procedimiento al que llamó. Una vez que el control regresa al emisor, decrementamos SP, conlo cual se desasigna el registro de activación del procedimiento al que se llamó.El código para el primer procedimiento inicializa la pila, estableciendo SP al inicio del áreade la pila en la memoria:LD SP, #inicioPila // inicializa la pilacódigo para el primer procedimientoHALT // termina la ejecuciónLa secuencia de la llamada a un procedimiento incrementa a SP, guarda la dirección de retornoy transfiere el control al procedimiento al que se llamó:ADD SP, SP, #emisor.tamRegistro // incrementa el apuntador de la pilaST *SP, #aqui + 16 // guarda la dirección de retornoBR receptor.areaCodigo // regresa al emisorEl operando #emisor.tamRegistro representa el tamaño de un registro de activación, por loque la instrucción ADD hace que SP apunte al siguiente registro de activación. El operando #aqui+ 16 en la instrucción ST es la dirección de la instrucción que va después de BR; se guarda enla dirección a la que apunta SP.La secuencia de retorno consiste en dos partes. El procedimiento al que se llamó transfiereel control a la dirección de retorno, usando lo siguiente:BR *0(SP) // regresa al emisorLa razón de usar *0(SP) en la instrucción BR es que necesitamos dos niveles de indirección:0(SP) es la dirección de la primera palabra en el registro de activación, y *0(SP) es la direcciónde retorno que está guardada ahí.La segunda parte de la secuencia de retorno está en el emisor, el cual decrementa a SP, conlo cual SP se restaura a su valor anterior. Es decir, después de la resta SP apunta al inicio delregistro de activación del emisor:SUB SP, SP, #emisor.tamRegistro // decrementa el apuntador de la pilaEl capítulo 7 contiene una explicación más amplia sobre las secuencias de las llamadas y lasconcesiones que se deben hacer en la división de la labor entre el procedimiento que llama y elprocedimiento al que se llamó.Ejemplo 8.4: El programa en la figura 8.5 es una abstracción del programa quicksort del capítulo anterior. El procedimiento q es recursivo, por lo que puede haber más de una activaciónde q viva al mismo tiempo.Suponga que los tamaños de los registros de activación para los procedimientos m, p y q se handeterminado como mtam, ptam y qtam, respectivamente. La primera palabra en cada registro de8.3 Direcciones en el código destino 521Maq. Cap_8_AHO.indd 521 11/10/07 12:58:42 AM522 Capítulo 8. Generación de código // código para m action 1 call q action 2 halt // código para p action 3 return // código para q action 4 call p action 5 call q action 6 call q returnFigura 8.5: Código para el ejemplo 8.4activación guardará una dirección de retorno. Suponemos en forma arbitraria que el código paraestos procedimientos empieza en las direcciones 100, 200 y 300, respectivamente, y que la pila empieza en la dirección 600. El programa destino se muestra en la figura 8.6.Vamos a suponer que ACTION4 contiene un salto condicional a la dirección 456 de la secuencia de retorno de q; de no ser así, el procedimiento recursivo q se condena a llamarse a símismo por siempre.Si mtam, ptam y qtam son 20, 40 y 60, respectivamente, la primera instrucción en la dirección100 inicializa SP a 600, la dirección inicial de la pila. SP contiene 620 justo antes que el controlse transfiera de m a q, ya que mtam es 20. Más adelante, cuando q llama a p, la instrucción enla dirección 320 incrementa SP a 680, en donde empieza el registro de activación para p; SP seregresa a 620 después de que el control regresa a q. Si las dos siguientes llamadas de q regresande inmediato, el valor máximo de SP durante esta ejecución es de 680. Sin embargo, observeque la última ubicación de la pila que se utiliza es 739, ya que el registro de activación de q queempieza en la ubicación 680 se extiende 60 bytes. ✷8.3.3 Direcciones para los nombres en tiempo de ejecuciónLa estrategia de asignación de almacenamiento y la distribución de los datos locales en unregistro de activación para un procedimiento son los que determinan cómo se accede al almacenamiento para los nombres. En el capítulo 6, asumimos que un nombre en una instrucciónde tres direcciones es en realidad un apuntador a una entrada en la tabla de símbolos para esenombre. Este método tiene una ventaja considerable; hace que el compilador sea más portable,ya que el front-end no tiene que modificarse, ni siquiera cuando el compilador se mueve a unamáquina distinta, en donde se requiere una organización distinta en tiempo de ejecución. Porotro lado, la acción de generar la secuencia específica de pasos de acceso mientras se genera elcódigo intermedio puede representar una ventaja considerable en un compilador optimizador,Maq. Cap_8_AHO.indd 522 11/10/07 12:58:43 AM // código para m 100: LD SP, #600 // inicializa la pila 108: ACTION 1 // código para action 1 128: ADD SP, SP, #mtam // empieza la secuencia de llamadas 136: ST *SP, #152 // mete la dirección de retorno 144: BR 300 // llama a q 152: SUB SP, SP, #mtam // restaura SP 160: ACTION 2 180: HALT... // código para p 200: ACTION 3 220: BR *0(SP) // regresa... // código para q 300: ACTION 4 // contiene un salto condicional a 456 320: ADD SP, SP, #qtam 328: ST *SP, #344 // mete la dirección de retorno 336: BR 200 // llama a p 344: SUB SP, SP, #qtam 352: ACTION 5 372: ADD SP, SP #qtam 380: BR *SP, #396 // mete la dirección de retorno 388: BR 300 // llama a q 396: SUB SP, SP, #qtam 404: ACTION 6 424: ADD SP, SP, #qtam 432: ST *SP, #440 // mete la dirección de retorno 440: BR 300 // llama a q 448: SUB SP, SP, #qtam 456: BR *0(SP) // regresa ... 600: // la pila empieza aquíFigura 8.6: Código destino para la asignación de pila8.3 Direcciones en el código destino 523Maq. Cap_8_AHO.indd 523 11/10/07 12:58:43 AM524 Capítulo 8. Generación de códigoya que permite al optimizador aprovechar los detalles que no vería en la instrucción simple detres direcciones.En cualquier caso, los nombres deben sustituirse en algún momento dado por código paraacceder a las ubicaciones de almacenamiento. Por ende, consideramos ciertas elaboraciones dela instrucción de copia simple de tres direcciones x = 0. Después de procesar las declaracionesen un procedimiento, suponga que la entrada en la tabla de símbolos para x contiene una dirección relativa 12 para x. Consideremos el caso en el que x se encuentra en un área asignación enforma estática, que empieza en la dirección estatica. Entonces, la verdadera dirección en tiempo de ejecución de x es estatica + 12. Aunque el compilador puede determinar en un momentodado el valor de estatica + 12 en tiempo de compilación, tal vez no se conozca la posición delárea estática al generar el código intermedio para acceder al nombre. En ese caso, tiene sentidogenerar un código de tres direcciones para “calcular” estatica + 12, con el entendimiento de queeste cálculo se llevará a cabo durante la fase de generación de código, o quizá por medio delcargador, antes de que se ejecute el programa. Así, la asignación x = 0 se traduce en:estatica[12] = 0Si el área estática empieza en la dirección 100, el código de destino para esta instrucción es:LD 112, #08.3.4 Ejercicios para la sección 8.3Ejercicio 8.3.1: Genere el código para las siguientes instrucciones de tres direcciones, suponiendo la asignación de pila en donde el registro SP apunta a la parte superior de la pila.call pcall qreturncall rreturnreturnEjercicio 8.3.2: Genere el código para las siguientes instrucciones de tres direcciones, asumiendo la asignación de pila en donde el registro SP apunta a la parte superior de la pila.a) x = 1b) x = ac) x = a + 1d) x = a + be) Las siguientes dos instrucciones:x = b * cy = a + xMaq. Cap_8_AHO.indd 524 11/10/07 12:58:43 AMEjercicio 8.3.3: Genere el código para las siguientes instrucciones de tres direcciones, asumiendo de nuevo la asignación de pila y asumiendo que a y b son arreglos, cuyos elementos sonvalores de 4 bytes.a) La siguiente secuencia de cuatro instrucciones:x = a[i]y = b[j]a[i] = yb[j] = xb) La siguiente secuencia de tres instrucciones:x = a[i]y = b[i]z = x * yc) La siguiente secuencia de tres instrucciones:x = a[i]y = b[x]a[i] = y8.4 Bloques básicos y grafos de flujoEn esta sección presentaremos una representación gráfica del código intermedio, útil parahablar sobre la generación de código, incluso si el grafo no se construye en forma explícitamediante un algoritmo de generación de código. La generación de código se beneficia del contexto. Podemos realizar un mejor trabajo de asignar los registros si sabemos cómo se defineny utilizan los valores, como veremos en la sección 8.8. Podemos realizar un mejor trabajo deseleccionar instrucciones si analizamos las secuencias de las instrucciones de tres direcciones,como veremos en la sección 8.9.La representación se construye de la siguiente forma:1. Se particiona el código intermedio en bloques básicos, los cuales son secuencias máximasde instrucciones consecutivas de tres direcciones, con las siguientes propiedades: (a) El flujo de control sólo puede entrar en el bloque básico a través de la primera instrucción en el bloque. Es decir, no hay saltos hacia la parte media del bloque. (b) El control saldrá del bloque sin detenerse o bifurcarse, excepto tal vez en la últimainstrucción en el bloque.2. Los bloques básicos se convierten en los nodos de un grafo de flujo, cuyas flechas indicanqué bloques pueden ir después de otros.8.4 Bloques básicos y grafos de fl ujo 525Maq. Cap_8_AHO.indd 525 11/10/07 12:58:44 AM526 Capítulo 8. Generación de códigoEl efecto de las interrupcionesLa noción de que el control, una vez que llega al inicio de un bloque básico, continuarásin duda hasta el final, requiere un poco de consideración. Existen muchas razones porlas que una interrupción, que no se refleje en forma explícita en el código, podría hacerque el control saliera del bloque, tal vez para nunca regresar. Por ejemplo, una instrucción como x = y/z parece no afectar el flujo de control, pero si z es 0, en realidad podríahacer que el programa abortara su ejecución.No debemos preocuparnos por esas posibilidades. La razón se describe a continuación.El propósito de construir bloques básicos es para optimizar el código. En general, cuandoocurre una interrupción, ésta se maneja y el control regresa a la instrucción que produjola interrupción, como si el control nunca se hubiera desviado, o el programa se detienecon un error. En este último caso, no importa cómo hayamos optimizado el código, auncuando esto dependiera de que el control llegara al final del bloque básico, ya que detodas formas el programa no produjo su resultado esperado.Desde el capítulo 9 empezaremos a hablar sobre las transformaciones en los grafos de flujoque convierten el código original intermedio en código intermedio “optimizado”, a partir delcual se puede generar un mejor código destino. El código intermedio “optimizado” se convierteen código de máquina usando las técnicas de generación código que se describen en este capítulo.8.4.1 Bloques básicosNuestra primera tarea es particionar una secuencia de instrucciones de tres direcciones en bloquesbásicos. Empezamos un nuevo bloque básico con la primera instrucción y seguimos agregando instrucciones hasta llegar a un salto, un salto condicional o una etiqueta en la siguiente instrucción.En la ausencia de saltos y etiquetas, el control procede en forma secuencial, de una instrucción ala siguiente. Esta idea se formaliza en el siguiente algoritmo.Algoritmo 8.5: Particionar instrucciones de tres direcciones en bloques básicos.ENTRADA: Una secuencia de instrucciones de tres direcciones.SALIDA: Una lista de los bloques básicos para la secuencia en la que cada instrucción se asigna exactamente a un bloque básico.MÉTODO: En primer lugar, determinamos las instrucciones en el código intermedio que sonlíderes; es decir, las primeras instrucciones en algún bloque básico. La instrucción que va justodespués del programa intermedio no se incluye como líder. Las reglas para buscar líderes son:1. La primera instrucción de tres direcciones en el código intermedio es líder.Maq. Cap_8_AHO.indd 526 11/10/07 12:58:44 AM2. Cualquier instrucción que sea el destino de un salto condicional o incondicional es líder.3. Cualquier instrucción que siga justo después de un salto condicional o incondicional eslíder.Así, para cada instrucción líder, su bloque básico consiste en sí misma y en todas las instrucciones hasta, pero sin incluir a la siguiente instrucción líder o el final del programa intermedio. ✷Figura 8.7: Código intermedio para establecer una matriz de 10 × 10 a una matriz de identidadEjemplo 8.6: El código intermedio en la figura 8.7 convierte una matriz de 10 × 10 a en unamatriz de identidad. Aunque no importa de dónde proviene este código, podría ser la traducción del seudocódigo de la figura 8.8. Para generar el código intermedio, hemos asumido que loselementos del arreglo con valores reales ocupan 8 bytes cada uno, y que la matriz a se almacenaen forma de orden por filas.for i de 1 a 10 dofor j de 1 a 10 do a[i, j] = 0.0;for i de 1 a 10 doa[i, j] = 1.0;Figura 8.8: Código fuente para la figura 8.78.4 Bloques básicos y grafos de fl ujo 527Maq. Cap_8_AHO.indd 527 11/10/07 12:58:45 AM528 Capítulo 8. Generación de códigoEn primer lugar, la instrucción 1 es líder según la regla (1) del Algoritmo 8.5. Para encontrarlas otras instrucciones líderes, primero debemos encontrar los saltos. En este ejemplo hay tressaltos, todos condicionales, en las instrucciones 9, 11 y 17. Según la regla (2), los destinos deestos saltos son instrucciones líderes; éstas vienen siendo las instrucciones 3, 2 y 13, respectivamente. Ahora, según la regla (3), cada instrucción que va después de un salto es líder; éstasson las instrucciones 10 y 12. Observe que no hay ninguna instrucción después de la 17 en estecódigo, pero si hubiera más código, la instrucción 18 sería también líder.Concluimos que las instrucciones líderes son 1, 2, 3, 10, 12 y 13. El bloque básico de cadalíder contiene todas las instrucciones a partir de esa instrucción líder, y hasta justo antes dela siguiente instrucción líder. Por ende, el bloque básico de 1 sólo contiene la instrucción 1,para la instrucción líder 2 el bloque sólo contiene la instrucción 2. Sin embargo, la instrucciónlíder 3 tiene un bloque básico que consiste en las instrucciones de la 3 a la 9, inclusive. Elbloque de la instrucción 10 consiste en las instrucciones 10 y 11; el bloque de la instrucción 12sólo contiene a la instrucción 12, y el bloque de la instrucción 13 contiene las instrucciones dela 13 a la 17. ✷8.4.2 Información de siguiente usoEs esencial saber cuándo se usará el valor de una variable para generar buen código. Si el valorde una variable que se encuentra en un registro nunca se utilizará más adelante, entonces eseregistro puede asignarse a otra variable.El uso de un nombre en una instrucción de tres direcciones se define a continuación. Suponga que la instrucción de tres direcciones i asigna un valor a x. Si la instrucción j tiene a x comooperando, y el control puede fluir de la instrucción i hacia j a través de una ruta que no tieneinstrucciones en las que intervenga x, entonces decimos que la instrucción j utiliza el valor de xcalculado en la instrucción i. Además, decimos que x está viva en la instrucción i.Deseamos determinar para la instrucción de tres direcciones x = y + z cuáles van a ser lossiguientes usos de x, y y z. Por el momento, no nos preocuparemos por los usos fuera del bloquebásico que contiene esta instrucción de tres direcciones.Nuestro algoritmo para determinar la información sobre la vida y el siguiente uso realizauna pasada invertida sobre cada bloque básico. Almacenamos la información en la tabla desímbolos. Podemos explorar con facilidad un flujo de instrucciones de tres direcciones paraencontrar el final de los bloques básicos, como en el Algoritmo 8.5. Ya que los procedimientospueden tener efectos adicionales, asumimos por conveniencia que cada llamada a un procedimiento empieza un nuevo bloque básico.Algoritmo 8.7: Determinar la información sobre la vida y el uso siguiente para cada instrucción en un bloque básico.ENTRADA: Un bloque básico B de instrucciones de tres direcciones. Suponemos que, al principio, la tabla de símbolos muestra que todas las variables no temporales en B están vivas alsalir.SALIDA: En cada instrucción i: x = y + z en B, adjuntamos a i la información sobre la viday el siguiente uso de x, y y z.Maq. Cap_8_AHO.indd 528 11/10/07 12:58:46 AMMÉTODO: Empezamos en la última instrucción en B, y exploramos en forma invertida hastael inicio de B. En cada instrucción i: x = y + z en B, hacemos lo siguiente:1. Adjuntar a la instrucción i la información que se encuentra en ese momento en la tablade símbolos, en relación con el siguiente uso y la vida de x, y y z.2. En la tabla de símbolos, establecer x a “no viva” y “sin siguiente uso”.3. En la tabla de símbolos, establecer y y z a “viva” y los siguientes usos de y y z a i.Aquí hemos usado a + como un símbolo que representa a cualquier operador. Si la instrucciónde tres direcciones i es de la forma x = + y o x = y, los pasos son los mismos, sólo que ignoramos z. Observe que el orden de los pasos (2) y (3) no puede intercambiarse, ya que x podríaser y o z. ✷8.4.3 Grafos de flujoUna vez que un programa de código intermedio se particiona en bloques básicos, representamosel flujo de control entre ellos mediante un grafo de flujo. Los nodos del grafo de flujo son losnodos básicos. Hay una flecha del bloque B al bloque C sí, y sólo si es posible que la primerainstrucción en el bloque C vaya justo después de la última instrucción en el bloque B. Hay dosformas en las que podría justificarse dicha flecha:• Hay un salto condicional o incondicional desde el final de B hasta el inicio de C.• C sigue justo después de B en el orden original de las instrucciones de tres direcciones, yB no termina en un salto incondicional.Decimos que B es un predecesor de C, y que C es un sucesor de B.A menudo agregamos dos nodos, conocidos como entrada y salida, los cuales no corresponden a instrucciones intermedias ejecutables. Hay una flecha que va desde la entrada hasta elprimer nodo ejecutable del grafo de flujo; es decir, al bloque básico que surge de la primera instrucción del código intermedio. Hay una flecha que va a la salida desde cualquier bloque básicoque contenga una instrucción, que pudiera ser la última instrucción ejecutada del programa. Sila instrucción final del programa no es un salto incondicional, entonces el bloque que contiene lainstrucción final del programa es un predecesor de la salida, pero también lo es cualquier bloque básico que tenga un salto hacia código que no forme parte del programa.Ejemplo 8.8: El conjunto de bloques básicos construido en el ejemplo 8.6 produce el grafo deflujo de la figura 8.9. La entrada apunta al bloque básico B 1, ya que éste contiene la primera instrucción del programa. El único sucesor de B 1 es B 2, debido a que B 1 no termina en un saltoincondicional y la instrucción líder de B 2 sigue justo después del final de B 1.El bloque B 3 tiene dos sucesores. Uno es el mismo B 3, ya que la instrucción líder de B 3 (lainstrucción 3) es el destino del salto condicional al final de B 3, en la instrucción 9. El otro sucesor es B 4, ya que el control puede pasar a través del salto condicional al final de B 3 y entrara continuación a la instrucción líder de B 4.Sólo B 6 apunta a la salida del grafo de flujo, ya que la única manera de ir al código quesigue después del programa a partir del cual construimos el grafo de flujo es pasar a través delsalto condicional que termina el bloque B 6. ✷8.4 Bloques básicos y grafos de fl ujo 529Maq. Cap_8_AHO.indd 529 11/10/07 12:58:46 AM530 Capítulo 8. Generación de códigoENTRADASALIDAFigura 8.9: Grafo de flujo de la figura 8.78.4.4 Representación de los grafos de flujoEn primer lugar, observe en la figura 8.9 que en el grafo de flujo, es normal sustituir los saltoshacia números de instrucción o etiquetas, por saltos hacia bloques básicos. Recuerde que todosalto condicional o incondicional va hacia la instrucción líder de algún bloque básico, y que el salto ahora hará referencia a este bloque. La razón de este cambio es que después de construir elgrafo de flujo, es común realizar cambios considerables a las instrucciones en los diversos bloques básicos. Si los saltos fueran a instrucciones, tendríamos que corregir los destinos de éstoscada vez que se modificara una de las instrucciones de destino.Siendo grafos bastante ordinarios, los grafos de flujo pueden representarse mediante una delas estructuras de datos apropiadas para grafos. El contenido de los nodos (bloques básicos)necesita su propia representación. Podríamos representar el contenido de un nodo mediante unMaq. Cap_8_AHO.indd 530 11/10/07 12:58:47 AMapuntador a la instrucción líder en el arreglo de instrucciones de tres direcciones, junto con unconteo del número de instrucciones, o un segundo apuntador a la última instrucción. Sin embargo, como tal vez cambiemos el número de instrucciones en un bloque básico con frecuencia,es probable que sea más eficiente crear una lista enlazada de instrucciones para cada bloquebásico.8.4.5 CiclosLas construcciones de los lenguajes de programación, como las instrucciones while, do-while yfor, ocasionan de manera natural el surgimiento de ciclos en los programas. Como casi cualquier programa invierte la mayor parte de su tiempo en ejecutar sus ciclos, es muy importantepara un compilador generar un buen código para los ciclos. Muchas transformaciones de códigodependen de la identificación de los “ciclos” en un grafo de flujo. Decimos que un conjunto denodos L en un grafo de flujo es un ciclo si:1. Hay un nodo en L llamado entrada al ciclo, con la propiedad de que ningún otro nodo enL tiene un predecesor fuera de L. Es decir, cada ruta desde la entrada de todo el grafode flujo completo, hacia cualquier nodo en L, pasa a través de la entrada al ciclo.2. Cada nodo en L tiene una ruta que no está vacía, completamente dentro de L, que vahacia la entrada de L.Ejemplo 8.9: El grafo de flujo de la figura 8.9 tiene tres ciclos:1. B 3 por sí solo.2. B 6 por sí solo.3. {B 2, B 3, B 4 }.Los primeros dos son nodos individuales con una flecha que va al mismo nodo. Por ejemplo, B 3forma un ciclo con B 3 como su entrada. Observe que el segundo requerimiento para un ciclo esque haya una ruta que no esté vacía, desde B 3 hacia sí mismo. Por ende, un nodo individualcomo B 2, que no tiene una flecha B 2 → B 2, no es un ciclo, ya que no hay una ruta no vacíadesde B 2 hacia sí mismo dentro de {B 2 }.El tercer ciclo, L = {B 2, B 3, B 4}, tiene a B 2 como su entrada al ciclo. Observe que entreestos tres nodos, sólo B 2 tiene un predecesor, B 1, que no está en L. Además, cada uno de lostres nodos tiene una ruta no vacía hacia B 2, que permanece dentro de L. Por ejemplo, B 2 tienela ruta B 2 → B 3 → B 4 → B 2. ✷8.4.6 Ejercicios para la sección 8.4Ejercicio 8.4.1: La figura 8.10 es un programa simple de multiplicación de matrices.a) Traduzca el programa en instrucciones de tres direcciones, del tipo que hemos estadousando en esta sección. Suponga que las entradas de las matrices son números que requieren 8 bytes, y que las matrices se almacenan en orden por filas.8.4 Bloques básicos y grafos de fl ujo 531Maq. Cap_8_AHO.indd 531 11/10/07 12:58:48 AM532 Capítulo 8. Generación de códigob) Construya el grafo de flujo para su código a partir de (a).c) Identifique los ciclos en su grafo de flujo de (b).Figura 8.10: Un algoritmo de multiplicación de matricesEjercicio 8.4.2: La figura 8.11 es código para contar los números primos desde 2 hasta n,usando el método de criba en un arreglo a con un tamaño adecuado. Es decir, a[i] es TRUE alfinal sólo si no hay un número primo ¶i¯ o menor, que se divida de manera uniforme entre i.Inicializamos todos los valores de a[i] a TRUE y después establecemos a[j] a FALSE si encontramos un divisor de j.a) Traduzca el programa en instrucciones de tres direcciones, del tipo que hemos estadoutilizando en esta sección. Asuma que los enteros requieren 4 bytes.b) Construya el grafo de flujo para su código de (a).c) Identifique los ciclos en su grafo de flujo de (b).for (i=2; i<=n; i++) a[i] = TRUE;cuenta = 0;s = sqrt(n);for (i=2; i<=s; i++) if (a[i]) /* se encontro que i es un numero primo */ { cuenta++; for (j=2*i; j<=n; j = j+i) a[j] = FALSE; /* ningun multiplo de i es primo */ }Figura 8.11: Código para cribar en busca de números primosMaq. Cap_8_AHO.indd 532 11/10/07 12:58:48 AM8.5 Optimización de los bloques básicos 5338.5 Optimización de los bloques básicosA menudo podemos obtener una considerable mejora sobre el tiempo de ejecución del código,con sólo realizar una optimización local dentro de cada bloque básico por sí mismo. En capítulos posteriores, empezando desde el capítulo 9, cubriremos la optimización global con más detalle, en la que se analiza la forma en que fluye la información a través de los bloques básicos deun programa. Es un tema complejo, en el cual hay que considerar muchas técnicas distintas.8.5.1 La representación en GDA de los bloques básicosMuchas técnicas importantes para la optimización local empiezan por transformar un bloquebásico en un GDA (grafo dirigido acíclico; DAG, en sus siglas en inglés). En la sección 6.1.1presentamos el GDA como una representación para las expresiones simples. La idea se extiendeen forma natural hasta la colección de expresiones que se crean dentro de un bloque básico.Construimos un GDA para bloques básicos de la siguiente manera:1. Hay un nodo en el GDA para cada uno de los valores iniciales de las variables que aparecen en el bloque básico.2. Hay un nodo N asociado con cada instrucción s dentro del bloque. Los hijos de N sonaquellos nodos que corresponden a las instrucciones que son las últimas definiciones,anteriores a s, de los operandos utilizados por s.3. El nodo N se etiqueta mediante el operador que se aplica en s, y también a N se adjuntala lista de variables para las cuales es la última definición dentro del bloque.4. Ciertos nodos se designan como nodos de salida. Éstos son los nodos cuyas variablesestán vivas al salir del bloque; es decir, sus valores pueden utilizarse después, en otrobloque del grafo de flujo. El cálculo de estas “variables vivas” corresponde al análisis deflujo global, que veremos en la sección 9.2.5.La representación en GDA de un bloque básico nos permite realizar varias transformacionespara mejora de código, en el código representado por el bloque.a) Podemos eliminar las subexpresiones locales comunes; es decir, las instrucciones que calculan un valor que ya se ha calculado.b) Podemos eliminar el código muerto; es decir, las instrucciones que calculan un valor quenunca se utiliza.c) Podemos reordenar las instrucciones que no dependen unas de otras; dicho reordenamiento puede reducir el tiempo que requiere un valor temporal para preservarse en unregistro.d) Podemos aplicar leyes algebraicas para reordenar los operandos de las instrucciones detres direcciones, lo cual algunas veces simplifica los cálculos.Maq. Cap_8_AHO.indd 533 11/10/07 12:58:49 AM534 Capítulo 8. Generación de código8.5.2 Búsqueda de subexpresiones locales comunesPara detectar las subexpresiones comunes hay que observar, en el momento en el que un nuevo nodo M está a punto de agregarse, si hay un nodo N existente con los mismos hijos, en elmismo orden y con el mismo operador. De ser así, N calcula el mismo valor que M y puedeusarse en su lugar. Esta técnica se introdujo como el método “número de valor” para detectarsubexpresiones comunes en la sección 6.1.1.Ejemplo 8.10: En la figura 8.12 se muestra un GDA para el siguiente bloque:a = b + cb = a − dc = b + cd = a − dAl construir el nodo para la tercera instrucción c=b+c, sabemos que el uso de b en b+c serefiere al nodo de la figura 8.12 etiquetado como −, ya que es la definición más reciente de b.Por ende, no confundimos los valores calculados en las instrucciones uno y tres.Figura 8.12: GDA para el bloque básico en el ejemplo 8.10Sin embargo, el nodo que corresponde a la cuarta instrucción d=a−d tiene el operador −y los nodos con las variables adjuntas a y d 0 como hijos. Como el operador y los hijos son losmismos que para el nodo correspondiente a la instrucción dos, no creamos este nodo, sino queagregamos d a la lista de definiciones para el nodo etiquetado como −. ✷Podría parecer que, como sólo hay tres nodos que no son hojas en el GDA de la figura 8.12,el bloque básico en el ejemplo 8.10 puede sustituirse mediante un bloque con sólo tres instrucciones. De hecho, si b no está viva al salir del bloque, entonces no necesitamos calcular esavariable, y podemos usar d para recibir el valor representado por el nodo etiquetado como −en la figura 8.12. Así, el bloque se convierte en:a = b + cd = a − dc = d + cMaq. Cap_8_AHO.indd 534 11/10/07 12:58:50 AM8.5 Optimización de los bloques básicos 535No obstante, si tanto b como d están vivas al salir, entonces debemos usar una cuarta instrucción para copiar el valor de una a la otra.1Ejemplo 8.11: Cuando buscamos subexpresiones comunes, en realidad buscamos expresiones enlas que se garantiza que calculen el mismo valor, sin importar cómo se calcule ese valor. Por ende,el método del GDA ignorará el hecho de que la expresión calculada por las instrucciones primeray cuarta en la secuencia a = b + c;b = b − dc = c + de = b + ces la misma, a saber b 0 + c 0. Es decir, aun cuando b y c cambian entre la primera y la últimainstrucción, su suma permanece igual, ya que b + c = (b − d) + (c + d). El GDA para estasecuencia se muestra en la figura 8.13, pero no exhibe subexpresiones comunes. Sin embargo,las identidades algebraicas que se aplican al GDA, como vimos en la sección 8.5.4, pueden exponer esta equivalencia. ✷Figura 8.13: GDA para el bloque básico en el ejemplo 8.118.5.3 Eliminación del código muertoLa operación sobre GDAs que corresponde a la eliminación de código muerto se puede implementar de la siguiente forma. Eliminamos de un GDA cualquier raíz (nodo sin ancestros) que no tengavariables vivas adjuntas. La aplicación repetida de esta transformación eliminará todos los nodosdel GDA que corresponden al código muerto.Ejemplo 8.12: Si en la figura 8.13, a y b están vivas pero c y e no, podemos eliminar deinmediato la raíz etiquetada como e. Así, el nodo etiquetado como c se convierte en una raízy puede eliminarse. Quedan las raíces etiquetadas como a y b, ya que cada una tiene variablesvivas adjuntas. ✷1 En general, debemos tener cuidado, al reconstruir código de los GDAs, de la forma en que elegimos los nombresde las variables. Si una variable x se define dos veces, o si se asigna una vez y se utiliza también el valor inicial x 0,entonces debemos asegurarnos de no modificar el valor de x sino hasta que hayamos efectuado todos los usos del nodocuyo valor x contenía anteriormente.Maq. Cap_8_AHO.indd 535 11/10/07 12:58:51 AM536 Capítulo 8. Generación de código8.5.4 El uso de identidades algebraicasLas identidades algebraicas representan otra clase importante de optimizaciones en los bloquesbásicos. Por ejemplo, podemos aplicar identidades aritméticas, como: x + 0 = 0 + x = x x − 0 = x x × 1 = 1 × x = x x/1 = xpara eliminar los cálculos de un bloque básico.Otra clase de optimizaciones algebraicas incluye la reducción por fuerza local; es decir, sustituir un operador más costoso por uno más económico, como en: COSTOSO ECONÓMICOx2 = x × x 2 × x = x + xx/2 = x × 0.5El plegado de constantes es una tercera clase de optimizaciones relacionadas. Aquí evaluamos las expresiones constantes en tiempo de compilación y las sustituimos por sus valores.2 Porlo tanto, la expresión 2 ∗ 3.14 se sustituiría por 6.28. Muchas expresiones constantes surgen en lapráctica debido al uso frecuente de las constantes simbólicas en los programas.El proceso de construcción de un GDA puede ayudarnos a aplicar estas y otras transformaciones algebraicas más generales, como la conmutatividad y la asociatividad. Por ejemplo,suponga que el manual de referencia del lenguaje especifica que ∗ es conmutativo; es decir, x ∗y= y ∗x. Antes de crear un nuevo nodo etiquetado como ∗, con un hijo izquierdo M y un hijoderecho N, siempre verificamos que ya exista un nodo así. No obstante, como ∗ es conmutativo,entonces debemos verificar que haya un nodo con el operador ∗, un hijo izquierdo N y un hijoderecho M.Algunas veces, los operadores relacionales como < y = generan subexpresiones comunesinesperadas. Por ejemplo, podemos probar también la condición x > y si restamos los argumentos y realizamos una evaluación sobre el conjunto de códigos de condición mediante la resta.3Por ende, tal vez sólo haya que generar un nodo del GDA para x − y y para x > y.Las leyes asociativas también podrían aplicarse para exponer subexpresiones comunes. Porejemplo, si el código fuente tiene las siguientes asignaciones:a = b + c;e = c + d + b;podría generarse el siguiente código intermedio:2 Las expresiones aritméticas deberían evaluarse de la misma forma en tiempo de compilación que en tiempode ejecución. K. Thompson sugirió una solución elegante al plegado de constantes: compilar la expresión constante,ejecutar el código de destino ahí mismo y sustituir la expresión con el resultado. Por ende, el compilador no necesitacontener un intérprete. 3 Sin embargo, la resta puede introducir desbordamientos y subdesbordamientos, mientras que una instrucción decomparación no lo haría.Maq. Cap_8_AHO.indd 536 11/10/07 12:58:51 AM8.5 Optimización de los bloques básicos 537a = b + ct = c + de = t + bSi t no se necesita fuera de este bloque, podemos modificar esta secuencia así:a = b + ce = a + dutilizando tanto la ley asociativa como la conmutativa de +.El desarrollador de compiladores debería examinar con cuidado el manual de referencia dellenguaje, para determinar qué reordenamientos de cálculos se permiten, ya que (debido a losposibles desbordamientos o subdesbordamientos) la aritmética de computadora no siempreobedece a las identidades algebráicas de las matemáticas. Por ejemplo, el estándar de Fortranestablece que un compilador puede evaluar cualquier expresión de manera matemática equivalente, siempre y cuando no se viole la integridad de los paréntesis. Así, un compilador puedeevaluar x ∗ y − x ∗ z como x ∗ (y − z ), pero no puede evaluar a + (b − c ) como (a + b ) − c.Por lo tanto, un compilador de Fortran debe mantener el registro de la ubicación de los paréntesis en las expresiones del lenguaje fuente, si va a optimizar los programas de acuerdo con ladefinición del lenguaje.8.5.5 Representación de referencias a arreglosA primera vista, podría parecer que las instrucciones para indexar arreglos pueden tratarsecomo cualquier otro operador. Por ejemplo, considere la siguiente secuencia de instruccionesde tres direcciones:x = a[i]a[j] = y z = a[i]Si consideramos a a[i] como una operación en la que se involucran a e i, similar a a + i,entonces podría parecer que los dos usos de a[i] son una subexpresión común. En este caso,podríamos vernos tentados a “optimizar” mediante la sustitución de la tercera instrucciónz = a[i] por una más simple, z = x. Sin embargo, como j podría ser igual a i, la instrucción deen medio puede de hecho modificar el valor de a[i]; por ende, no es legal realizar esta modificación.La manera apropiada de representar los accesos a arreglos en un GDA es la siguiente:1. Una asignación de un arreglo como x = a[i] se representa mediante la creación de unnodo con el operador =[ ] y dos hijos, los cuales representan el valor inicial del arreglo,en este caso a 0, y el índice i. La variable x se convierte en una etiqueta de este nuevonodo.2. Una asignación a un arreglo, como a[j] = y, se representa mediante un nuevo nodo con eloperador [ ]= y tres hijos que representan a a 0, j y y. No hay una variable para etiquetarMaq. Cap_8_AHO.indd 537 11/10/07 12:58:52 AM538 Capítulo 8. Generación de código este nodo. La diferencia es que la creación de este nodo elimina a todos los nodos actuales construidos, cuyo valor depende de a 0. Un nodo que se ha eliminado no puede recibirmás etiquetas; es decir, no puede convertirse en una expresión común.Ejemplo 8.13: El GDA para el siguiente bloque básico:x = a[i]a[j] = yz = a[i]se muestra en la figura 8.14. Primero se crea el nodo N para x, pero cuando se crea el nodoetiquetado como [ ]=, N se elimina. Así, cuando se crea el nodo para z, no puede identificarsecon N y en su lugar debe crearse un nuevo nodo con los mismos operandos a 0 e i 0. ✷eliminadoFigura 8.14: El GDA para una secuencia de asignaciones de arreglosEjemplo 8.14: Algunas veces, debe eliminarse un nodo aun cuando ninguno de sus hijos tenga un arreglo como a 0 en el ejemplo 8.13, como una variable adjunta. De igual forma, un nodopuede eliminar si tiene un descendiente que sea un arreglo, aun cuando ninguno de sus hijossean nodos de arreglos. Por ejemplo, considere el siguiente código de tres direcciones:b = 12 + ax = b[i]b[j] = yLo que está ocurriendo aquí es que, por razones de eficiencia, b se ha definido para ser unaposición en un arreglo a. Por ejemplo, si los elementos de a son de cuatro bytes, entonces b representa el cuarto elemento de a. Si j e i representan el mismo valor, entonces b[i] y b[j]representan la misma ubicación. Por lo tanto, es importante hacer que la tercera instrucción,b[j] = y, elimine el nodo con x como su variable adjunta. Sin embargo y como veremos en lafigura 8.15, tanto el nodo eliminado como el nodo que realiza la eliminación tienen a a 0 comonieto, no como hijo. ✷Maq. Cap_8_AHO.indd 538 11/10/07 12:58:52 AM8.5 Optimización de los bloques básicos 539eliminadoFigura 8.15: Un nodo que elimina el uso de un arreglo, no tiene que tener ese arreglo como hijo8.5.6 Asignaciones de apuntadores y llamadas a procedimientosCuando realizamos una asignación indirecta a través de un apuntador, como en las siguientesasignaciones:x = *p*q = yno sabemos a dónde apuntan p o q. En realidad, x = *p es un uso de cualquier variable, y *q = yes una posible asignación a cualquier variable. Como consecuencia, el operador =∗ debe aceptartodos los nodos que se encuentran asociados con identificadores como argumentos, lo cual es relevante para la eliminación de código muerto. Lo más importante es que el operador ∗= elimina atodos los demás nodos que se han construido hasta ahora en el GDA.Existen análisis de apuntadores globales que podríamos realizar para limitar el conjuntode variables que un apuntador pudiera referenciar en una posición dada en el código. Inclusivehasta el análisis local podría restringir el alcance de un apuntador. Por ejemplo, en la siguientesecuencia:p = &x*p = ysabemos que sólo la variable x recibe el valor de y, por lo que no necesitamos eliminar ningúnnodo, sino el nodo al cual se adjunta x.Las llamadas a procedimientos se comportan en forma muy parecida a las asignaciones através de los apuntadores. En la ausencia de la información de flujo de datos global, debemosasumir que un procedimiento utiliza e intercambia los datos a los que tiene acceso. Por ende,si la variable x está en el alcance de un procedimiento P, una llamada a P utiliza el nodo conla variable adjunta x y elimina ese nodo.8.5.7 Reensamblado de los bloques básicos a partir de los GDAsDespués de realizar todas las optimizaciones posibles mientras construimos el GDA, o mediantela manipulación del GDA una vez construido, podemos reconstituir el código de tres direccionesMaq. Cap_8_AHO.indd 539 11/10/07 12:58:53 AM540 Capítulo 8. Generación de códigopara el bloque básico a partir del cual creamos el GDA. Para cada nodo que tenga una o másvariables adjuntas, construimos una instrucción de tres direcciones que calcula el valor de unade estas variables. Preferimos calcular el resultado en una variable que esté viva al salir delbloque. No obstante, si no tenemos información sobre las variables globales vivas en la cual podamos trabajar, debemos suponer que todas las variables del programa (pero no las temporalesque genera el compilador para procesar expresiones) están vivas al salir del bloque.Si el nodo tiene más de una variable viva adjunta, entonces tenemos que introducir instrucciones de copia para dar el valor correcto a cada una de esas variables. Algunas veces laeliminación global puede eliminar esas copias, si podemos arreglárnoslas para usar una de dosvariables en vez de la otra.Ejemplo 8.15: Recuerde el GDA de la figura 8.12. En la explicación que sigue del ejemplo8.10, decidimos que si b no está viva al salir del bloque, entonces las siguientes tres instrucciones:a = b + cd = a − dc = d + cbastan para reconstruir el bloque básico. La tercera instrucción, c = d + c, debe usar d comoun operando en vez de b, ya que el bloque optimizado nunca calcula b.Si tanto b como d están vivas al salir, o si no estamos seguros de si están vivas o no al salir,entonces debemos calcular b al igual que d. Para ello, utilizamos la siguiente secuencia:a = b + cd = a − db = dc = d + cEste bloque básico es aún más eficiente que el original. Aunque el número de instrucciones esigual, hemos sustituido una resta por una copia, la cual tiende a ser menos costosa en la mayoría de las máquinas. Además, si realizamos un análisis global, tal vez podamos eliminar eluso de este cálculo de b fuera del bloque, sustituyéndolo por usos de d. En ese caso, podemosregresar a este bloque básico y eliminar b = d después. Por intuición, podemos eliminar estacopia si en cualquier lugar en el que se utilice este valor de b, d todavía contiene el mismo valor.Esa situación puede ser verdadera o no, dependiendo de la forma en que el programa vuelva acalcular a d. ✷Al reconstruir el bloque básico a partir de un GDA, no sólo tenemos que preocuparnos acerca de qué variables se utilizan para contener los valores de los nodos del GDA; sino tambiénacerca del orden en el que listamos las instrucciones que calculan los valores de los diversosnodos. Las reglas a recordar son:1. El orden de las instrucciones debe respetar el orden de los nodos en el GDA. Es decir, nopodemos calcular el valor de un nodo, sino hasta que hayamos calculado un valor paracada uno de sus hijos.Maq. Cap_8_AHO.indd 540 11/10/07 12:58:54 AM8.5 Optimización de los bloques básicos 5412. Las asignaciones a un arreglo deben ir después de todas las asignaciones previas a, oevaluaciones de, el mismo arreglo, de acuerdo con el orden de estas instrucciones en elbloque básico original.3. Las evaluaciones de los elementos de los arreglos deben ir después de cualquier asignación previa (de acuerdo con el bloque original) al mismo arreglo. La única permutaciónpermitida es que dos evaluaciones del mismo arreglo pueden realizarse en cualquier orden, siempre y cuando ninguna se cruce sobre una asignación a ese arreglo.4. Cualquier uso de una variable debe ir después de todas las llamadas a procedimientosprevias (de acuerdo con el bloque original) o asignaciones indirectas a través de un apuntador.5. Cualquier llamada a un procedimiento o asignación indirecta a través de un apuntadordebe ir después de todas las evaluaciones previas (de acuerdo con el bloque original) decualquier variable.Es decir, al reordenar el código, ninguna instrucción puede cruzar una llamada a un procedimiento o asignación a través de un apuntador, y los usos del mismo arreglo pueden cruzarseentre sí, sólo si ambos son accesos a arreglos, pero no asignaciones a elementos del arreglo.8.5.8 Ejercicios para la sección 8.5Ejercicio 8.5.1: Construya el GDA para el siguiente bloque básico:d = b * ce = a + bb = b * ca = e − dEjercicio 8.5.2: Simplifique el código de tres direcciones del ejercicio 8.5.1, suponiendo losiguiente:a) Sólo a está viva al salir del bloque.b) a, b y c están vivas al salir del bloque.Ejercicio 8.5.3: Construya el bloque básico para el código en el bloque B 6 de la figura 8.9.No olvide incluir la comparación i ¥ 10.Ejercicio 8.5.4: Construya el bloque básico para el código en el bloque B 3 de la figura 8.9.Ejercicio 8.5.5: Extienda el Algoritmo 8.7 para procesar tres instrucciones de la forma:a) a[i] = bb) a = b[i]c) a = *bd) *a = bMaq. Cap_8_AHO.indd 541 11/10/07 12:58:54 AM542 Capítulo 8. Generación de códigoEjercicio 8.5.6: Construya el GDA para el siguiente bloque básico: a[i] = b*p = cd = a[j]e = *p*p = a[i]suponiendo que:a) p puede apuntar a cualquier parte.b) p sólo puede apuntar a b o a d.Ejercicio 8.5.7: Si un apuntador o expresión de arreglo, como a[i] o *p, se asigna y despuésse utiliza, sin la posibilidad de modificarse en el intervalo de instrucciones, podemos aprovecharla situación para simplificar el GDA. Por ejemplo, en el código del ejercicio 8.5.6, como p no seasigna entre la segunda y la cuarta instrucción, la instrucción e = *p puede sustituirse pore = c, sin importar hacia dónde apunta p. Modifique el algoritmo de construcción de GDAspara aprovechar dichas situaciones, y aplique su algoritmo al código del ejemplo 8.5.6.Ejercicio 8.5.8: Suponga que se forma un bloque básico a partir de las siguientes instrucciones en C: y = a + b + c + d + e + f;y = a + c + e;a) Proporcione las instrucciones de tres direcciones (sólo una suma por instrucción) para estebloque.b) Use las leyes asociativa y conmutativa para modificar el bloque y utilizar el menor númeroposible de instrucciones, suponiendo que tanto x como y están vivas al salir del bloque.8.6 Un generador de código simpleEn esta sección vamos a considerar un algoritmo que genera código para un solo bloque básico. Considera una instrucción de tres direcciones a la vez, y lleva la cuenta de qué valores seencuentran en qué registros, para poder evitar la generación de operaciones de carga y almacenamiento innecesarias.Una de las cuestiones principales durante la generación de código es decidir cómo usar losregistros para sacarles el máximo provecho. Hay cuatro usos principales de los registros:• En la mayoría de las arquitecturas de máquinas, algunos o todos los operandos de unaoperación deben estar en registros, para poder llevarla a cabo.• Los registros son excelentes variables temporales: lugares para guardar el resultado de unasubexpresión mientras se evalúa una expresión más grande o, dicho en forma más general, unlugar para guardar una variable que se utiliza sólo dentro de un bloque básico individual.!Maq. Cap_8_AHO.indd 542 11/10/07 12:58:55 AM• Los registros se utilizan para guardar valores (globales) que se calculan en un bloque básico y se utilizan en otros bloques; por ejemplo, un índice de ciclo que se incrementa alpasar por el ciclo y se utiliza varias veces dentro del mismo.• A menudo, los registros se utilizan para ayudar con la administración del almacenamientoen tiempo de ejecución, por ejemplo, para administrar la pila en tiempo de ejecución,incluyendo el mantenimiento de los apuntadores de la pila y posiblemente los elementossuperiores de ésta.Éstas son necesidades en competencia, ya que el número de registros disponibles es limitado.El algoritmo en esta sección supone que hay cierto conjunto de registros disponible para guardarlos valores que se utilizan dentro del bloque. Por lo general, este conjunto de registros no incluye atodos los registros de la máquina, ya que algunos registros se reservan para las variables globales ypara administrar la pila. Suponemos que el bloque básico ya se ha transformado en una secuenciapreferida de instrucciones de tres direcciones, mediante transformaciones como la combinación desubexpresiones comunes. Suponemos también que para cada operador, hay exactamente una instrucción de máquina que recibe los operandos necesarios en los registros y que realiza esa operación,dejando el resultado en un registro. Las instrucciones de máquina son de la siguiente forma:• LD reg, mem• ST mem, reg• OP reg, reg, reg8.6.1 Descriptores de registros y direccionesNuestro algoritmo de generación de código analiza una instrucción de tres direcciones a la vez y decide qué operaciones de carga son necesarias para meter los operandos requeridos en los registros.Después de generar las cargas, genera la operación en sí. Más tarde, si hay necesidad de almacenarel resultado en una ubicación de memoria, también genera esa operación de almacenamiento.Para poder realizar las decisiones necesarias, requerimos una estructura de datos que nosindique qué variables del programa tienen actualmente su valor en un registro, y en qué registro o registros están, si es así. También debemos saber si la ubicación de memoria para unavariable dada tiene actualmente el valor apropiado para esa variable, ya que tal vez se hayacalculado un nuevo valor para esa variable en un registro y todavía no se almacena. La estructura de datos deseada tiene los siguientes descriptores:1. Para cada registro disponible, un descriptor de registro lleva la cuenta de los nombres delas variables cuyo valor actual se encuentra en ese registro. Como sólo vamos a utilizarlos registros que estén disponibles para uso local dentro de un bloque básico, suponemosque al principio todos los descriptores de registro están vacíos. A medida que progrese lageneración de código, cada registro contendrá el valor de cero o más nombres.2. Para cada variable del programa, un descriptor de acceso lleva la cuenta de la ubicación oubicaciones en las que puede encontrarse el valor actual de esa variable. La ubicación podríaser un registro, una dirección de memoria, una ubicación en la pila o algún conjunto de másde uno de éstos. La información puede almacenarse en la entrada en la tabla de símbolospara ese nombre de variable.8.6 Un generador de código simple 543Maq. Cap_8_AHO.indd 543 11/10/07 12:58:55 AM544 Capítulo 8. Generación de código8.6.2 El algoritmo de generación de códigoUna parte esencial del algoritmo es una función llamada obtenReg(I), la cual selecciona losregistros para cada ubicación de memoria asociada con la instrucción de tres direcciones I. Lafunción obtenReg tiene acceso a los descriptores de registro y de acceso para todas las variablesdel bloque básico, y también puede tener acceso a cierta información útil del flujo de datos, como las variables que están vivas al salir del bloque. Después de presentar el algoritmo básico, hablaremos sobre obtenReg. Aunque no conocemos el número total de registros disponiblespara los datos locales pertenecientes a un bloque básico, suponemos que hay suficientes registros de forma que, después de liberar a todos los registros disponibles almacenando sus valoresen la memoria, hay suficientes registros como para realizar cualquier operación de tres direcciones.En una instrucción de tres direcciones como x = y + z, vamos a tratar a + como un operador genérico y a ADD como la instrucción de máquina equivalente. Por lo tanto, no aprovechamos la ley conmutativa de +. Así, al implementar la operación, el valor de y debe estar enel segundo registro mencionado en la instrucción ADD, nunca en el tercero. Una posible mejorapara el algoritmo es generar código para x = y + z y para x = z + y siempre que + sea unoperador conmutativo, y elegir la mejor secuencia de código.Instrucciones de máquina para las operacionesPara una instrucción de tres direcciones como x = y + z, haga lo siguiente:1. Usar obtenReg (x = y + z ) para seleccionar registros para x, y y z. Llamar a estos registros Rx, Ry y Rz.2. Si y no está en Ry (de acuerdo con el descriptor de registro para Ry ), entonces generaruna instrucción LD Ry, y, en donde y es una de las ubicaciones de memoria para y (deacuerdo con el descriptor de acceso para y).3. De manera similar, si z no está en Rz, generar una instrucción LD Rz, z, en donde z esuna ubicación para z.4. Generar la instrucción ADD Rx, Ry, Rz.Instrucciones de máquina para las instrucciones de copiaHay un caso especial importante: una instrucción de copia de tres direcciones, de la forma x = y.Suponemos que obtenReg siempre elegirá el mismo registro tanto para x como para y. Si y no seencuentra ya en el registro Ry, entonces se genera la instrucción de máquina LD Ry, y. Si y yase encontraba en Ry, no hacemos nada. Sólo es necesario ajustar la descripción de registro paraRy, de manera que incluya a x como una de las variables que se encuentran ahí.Maq. Cap_8_AHO.indd 544 11/10/07 12:58:56 AMTerminación del bloque básicoComo hemos descrito el algoritmo, las variables utilizadas por el bloque pueden terminar enel punto en el que su única ubicación sea un registro. Si la variable es temporal y se utilizasólo dentro del bloque, está bien; cuando termina el bloque, podemos olvidarnos del valor dela variable temporal y suponer que su registro está vacío. No obstante, si la variable está vivaal salir del bloque, o si no sabemos qué variables están vivas al salir, entonces tenemos quesuponer que el valor de la variable se necesita más adelante. En ese caso, para cada variable xcuyo descriptor de ubicación no indique que su valor se encuentra en la ubicación de memoriapara x, debemos generar la instrucción ST x, R, en donde R es un registro en el cual el valorde x existe al final del bloque.Administración de los descriptores de registro y de direcciónA medida que el algoritmo de generación de código genera instrucciones de carga, almacenamiento y demás instrucciones de máquina, debe actualizar los descriptores de registro y de dirección.Las reglas para ello son:1. Para la instrucción LD R, x (a) Modificar el descriptor de registro para el registro R, de manera que sólo contenga a x. (b) Modificar el descriptor de dirección para x, agregando el registro R como una ubicación adicional.2. Para la instrucción ST x, R, modificar el descriptor de dirección para x, de manera queincluya su propia ubicación de memoria.3. Para una operación como ADD R x , Ry , R z , implementar una instrucción de tres direcciones x = y + z (a) Modificar el descriptor de registro para R x , de manera que sólo contenga a x. (b) Modificar el descriptor de dirección para x, de manera que su única ubicación seaR x . Observe que la ubicación de memoria para x no se encuentra ahora en el descriptor de dirección para x. (c) Eliminar R x del descriptor de dirección de cualquier variable distinta de x.4. Al procesar una instrucción de copia x = y, después de generar la carga de y en el registroR y, si es necesario, y después de administrar los descriptores para todas las instruccionesde carga (de acuerdo con la regla 1): (a) Agregar x al descriptor de registro para Ry . (b) Modificar el descriptor de dirección para x, de manera que su única ubicación sea Ry.Ejemplo 8.16: Vamos a traducir el bloque básico que consiste en las siguientes instruccionesde tres direcciones:8.6 Un generador de código simple 545Maq. Cap_8_AHO.indd 545 11/10/07 12:58:56 AM546 Capítulo 8. Generación de códigot = a − bu = a − cv = t + ua = dd = v + uAquí suponemos que t, u y v son variables temporales, locales para el bloque, mientras que a,b, c y d son variables que están vivas al salir del bloque. Como no hemos hablado todavía sobrela forma en que podría trabajar la función obtenReg, sólo asumiremos que hay tantos registroscomo necesitemos, pero que cuando ya no se necesite el valor de un registro (por ejemplo, quesólo contenga un valor temporal, del cual hayan pasado ya todos los usos), entonces volvemosa utilizar su registro.En la figura 8.16 se muestra un resumen de todas las instrucciones de código máquina generadas. Esta figura también muestra los descriptores de registro y de dirección, antes y despuésde la traducción de cada instrucción de tres direcciones.salidaFigura 8.16: Instrucciones generadas y los cambios en los descriptores de registro y de direcciónPara la primera instrucción de tres direcciones, t = a − b, debemos generar tres instrucciones, ya que al principio no hay nada en un registro. Por ende, vemos que a y b se cargan en losMaq. Cap_8_AHO.indd 546 11/10/07 12:58:57 AMregistros R1 y R2, y se produce el valor t en el registro R2. Observe que podemos usar R2 para t,ya que el valor b que estaba antes en R2 no se necesita dentro del bloque. Como se supone queb está viva al salir del bloque, si no se hubiera encontrado en su propia ubicación de memoria(como lo indica su descriptor de dirección), hubiéramos tenido que almacenar R2 en b primero.La decisión de hacer esto, si hubiéramos necesitado a R2, sería responsabilidad de obtenReg.La segunda instrucción, u = a − c, no requiere una carga de a debido a que ya se encuentra enel registro R1. Además, podemos usar R1 para el resultado u, ya que el valor de a, que se encontraba antes en ese registro, ya no se necesita dentro del bloque, y su valor se encuentra en su propiaubicación de memoria, en caso de que a se necesite fuera del bloque. Observe que modificamos eldescriptor de dirección para a, para indicar que ya no se encuentra en R1, sino en la ubicación dememoria llamada a.La tercera instrucción, v = t + u, sólo requiere la suma. Además, podemos usar R3 para elresultado v, ya que el valor de c en ese registro no se necesita más dentro del bloque, y c tienesu valor en su propia ubicación de memoria.La instrucción de copia, a = d, requiere una carga de d, ya que no se encuentra en la memoria. Mostramos al descriptor de registro de R2, el cual contiene a a y d. La adición de a aldescriptor de registro es el resultado que obtenemos al procesar la instrucción de copia, y noel resultado de cualquier instrucción de máquina.La quinta instrucción, d = v + u, usa dos valores que se encuentran en registros. Como u esun valor temporal cuyo valor ya no se necesita, hemos optado por reutilizar su registro R1 parael nuevo valor de d. Observe que d ahora se encuentra sólo en R1, y no en su propia ubicación dememoria. Lo mismo se aplica para a, que se encuentra en R2 y no en la ubicación de memoriallamada a. Como resultado, necesitamos un “coda” al código de máquina para el bloque básicoque almacena las variables a y d, que están vivas al salir, en sus ubicaciones de memoria. Mostramos éstas como las últimas dos instrucciones. ✷8.6.3 Diseño de la función obtenRegPor último, vamos a considerar cómo implementar obtenReg (I ), para una instrucción I detres direcciones. Existen muchas opciones, aunque también hay ciertas prohibiciones absolutascontra las opciones que conducen a un código incorrecto, debido a la pérdida del valor de unao más variables vivas. Empezamos nuestro examen con el caso del paso de una operación, parael cual utilizamos de nuevo a x = y + z como ejemplo genérico. Primero, debemos elegir unregistro para y y un registro para z. Las cuestiones son las mismas, por lo que nos concentramosen elegir el registro R y para y. Las reglas son las siguientes:1. Si y se encuentra actualmente en un registro, elija un registro que ya contenga a y comoR y . No genere una instrucción de máquina para cargar este registro, ya que no se necesita ninguna.2. Si y no está en un registro, pero hay un registro vacío en ese momento, elija un registrocomo R y .3. El caso difícil ocurre cuando y no se encuentra en un registro, y no hay un registro vacío en ese momento. Debemos elegir uno de los registros permitidos de todas formas, ydebemos asegurarnos que sea seguro volver a utilizarlo. Suponga que R es un registrocandidato y que v es una de las variables que el descriptor de registro para R dice que8.6 Un generador de código simple 547Maq. Cap_8_AHO.indd 547 11/10/07 12:58:58 AM548 Capítulo 8. Generación de códigoestá en R. Tenemos que estar seguros de que el valor de v no se necesite realmente, o quehaya alguna otra parte a la que podamos ir para obtener el valor de R. Las posibilidadesson: (a) Si el descriptor de dirección para v dice que v está en alguna otra parte además deR, entonces estamos bien. (b) Si v es x, el valor calculado por la instrucción I, y x no es tampoco uno de los otrosoperandos de la instrucción I (z en este ejemplo), entonces estamos bien. La razónes que en este caso, sabemos que este valor de x nunca se va a volver a utilizar, porlo que tenemos la libertad de ignorarlo. (c) Por el contrario, si v no se utiliza más adelante (es decir, después de la instrucción Ino hay más usos de v, y si v está viva al salir del bloque, entonces v se recalcula dentrodel bloque), entonces estamos bien. (d) Si no estamos bien debido a uno de los dos primeros casos, entonces debemos generarla instrucción de almacenamiento ST v, R para colocar una copia de v en su propiaubicación de memoria. A esta operación se le conoce como un derrame.Como R puede contener diversas variables en un momento dado, repetimos los pasosanteriores para cada una de esas variables v. Al final, la “puntuación” de R es el numerode instrucciones de almacenamiento que tuvimos que generar. Elija uno de los registroscon la puntuación más baja.Ahora, considere la selección del registro Rx . Las cuestiones y opciones son casi iguales quepara y, por lo que sólo mencionaremos las diferencias.1. Como se va a calcular un nuevo valor de x, un registro que sólo contiene a x es siempreuna elección aceptable para R x . Esta instrucción se aplica incluso si x es y o z, ya quenuestras instrucciones de máquina permiten que dos registros sean iguales en una instrucción.2. Si y no se utiliza después de la instrucción I, en el sentido descrito para la variable v enel punto (3c), y Ry contiene sólo a y después de cargarse, si es necesario, entonces R ytambién puede utilizarse como R x. Se aplica una opción similar en z y R z .La última cuestión a considerar de manera especial es cuando I es una instrucción de copiax = y. Elegimos el registro Ry como vimos antes. Después, siempre elegimos R x = Ry .8.6.4 Ejercicios para la sección 8.6Ejercicio 8.6.1: Para cada una de las siguientes instrucciones de asignación en C:a) x = a + b*c;b) x = a/(b+c) − d*(e+f);c) x = a[i] + 1;Maq. Cap_8_AHO.indd 548 11/10/07 12:58:59 AMd) a[i] = b[c[i]];e) a[i][j] = b[i][k] + c[k][j];f) *p++ = *q++;genere código de tres direcciones, suponiendo que todos los elementos de los arreglos sean enteros que ocupan cuatro bytes cada uno. En las partes (d) y (e), asuma que a, b y c son constantesque proporcionan la ubicación de los primeros (0vo.) elementos de los arreglos con esos nombres,como en todos los ejemplos anteriores de los accesos a arreglos en este capítulo.Ejercicio 8.6.2: Repita los incisos (d) y (e) del ejercicio 8.6.1, suponiendo que los arreglos a,b y c se ubican mediante los apuntadores pa, pb y pc, respectivamente, apuntando a las ubicaciones de sus primeros elementos respectivamente.Ejercicio 8.6.3: Convierta su código de tres direcciones del ejercicio 8.6.1 en código máquinapara el modelo de la máquina de esta sección. Puede usar todos los registros que necesite.Ejercicio 8.6.4: Convierta su código de tres direcciones del ejercicio 8.6.1 en código máquina,usando el algoritmo de generación de código simple de esta sección; suponga que hay tres registros disponibles. Muestre los descriptores de registro y de dirección después de cada paso.Ejercicio 8.6.5: Repita el ejercicio 8.6.4, pero suponga que sólo hay dos registros disponibles.8.7 Optimización de mirilla (peephole)Aunque la mayoría de los compiladores de producción producen buen código a través de unproceso cuidadoso de selección de instrucciones y asignación de registros, unos cuantos utilizanuna estrategia alternativa: generan código simple y mejoran la calidad del código de destino,aplicando transformaciones de “optimización” al programa destino. El término “optimización”es un poco confuso, ya que no hay garantía de que el código resultante sea óptimo bajo ningunamedida matemática. Sin embargo, muchas transformaciones simples pueden mejorar en formaconsiderable el tiempo de ejecución o requerimiento de espacio del programa destino.Una técnica simple pero efectiva para mejorar el código destino en forma local es la optimización de mirilla (peephole), la cual se lleva a cabo mediante el análisis de una ventana deslizablede instrucciones de destino (conocida como la mirilla), y sustituyendo las secuencias de instrucciones dentro de la mirilla por una secuencia más corta o rápida, cada vez que sea posible. Laoptimización de mirilla también puede aplicarse de manera directa después de la generación decódigo, para mejorar la representación intermedia.La mirilla es una pequeña ventana deslizable en un programa. El código en la mirilla notiene que ser contiguo, aunque algunas implementaciones lo requieren. Una característica dela optimización de mirilla es que cada mejora puede engendrar oportunidades para mejorasadicionales. En general, es necesario realizar varias pasadas a través del código fuente para8.7 Optimización de mirilla (peephole) 549!Maq. Cap_8_AHO.indd 549 11/10/07 12:58:59 AM550 Capítulo 8. Generación de códigoobtener el beneficio máximo. En esta sección vamos a proporcionar los siguientes ejemplos detransformaciones de programas, que son característicos de las optimizaciones de mirilla:• Eliminación de instrucciones redundantes.• Optimizaciones del flujo de control.• Simplificaciones algebraicas.• Uso de características específicas de máquinas.8.7.1 Eliminación de instrucciones redundantes de cargay almacenamientoSi vemos la siguiente secuencia de instrucciones:LD a, R0ST R0, aen un programa destino, podemos eliminar la instrucción de almacenamiento, ya que cada vezque se ejecute, la primera instrucción se asegurará de que el valor de a se haya cargado en elregistro R0. Tenga en cuenta que si la instrucción de almacenamiento tuviera una etiqueta, nopodríamos estar seguros de que la primera instrucción se ejecutara siempre antes de la segunda,por lo que no podríamos eliminar la instrucción de almacenamiento. Dicho de otra forma, lasdos instrucciones tienen que estar en el mismo bloque básico para que esta transformación seasegura.Las instrucciones redundantes de carga y almacenamiento de este tipo no se generan mediante el algoritmo de generación de código simple de la sección anterior. Sin embargo, unalgoritmo de generación de código simple como el de la sección 8.1.3 puede generar secuenciasredundantes como éstas.8.7.2 Eliminación de código inalcanzableOtra oportunidad para la optimización de mirilla es la eliminación de instrucciones inalcanzables. Una instrucción sin etiqueta que va justo después de un salto incondicional puedeeliminarse. Esta operación se puede repetir para eliminar una secuencia de instrucciones. Porejemplo, para fines de depuración, un programa extenso puede tener en su interior ciertos fragmentos de código que se ejecuten sólo si una variable debug es igual a 1. En la representaciónintermedia, este código podría ser así: if debug == 1 goto L1 goto L2L1: imprimir información de depuraciónL2:Una optimización de mirilla obvia es la eliminación de saltos sobre saltos. Por ende, sinimportar cuál sea el valor de debug, la secuencia de código anterior puede sustituirse por:Maq. Cap_8_AHO.indd 550 11/10/07 12:59:00 AM if debug != 1 goto L2 imprimir información de depuraciónL2:Si debug se establece en 0 al principio del programa, la propagación de constantes transformaría esta secuencia en: if 0 != 1 goto L2 imprimir información de depuraciónL2:Ahora el argumento de la primera instrucción siempre se evalúa como verdadero, por lo quela instrucción puede sustituirse por goto L2. Así, todas las instrucciones que impriman información de depuración serán inalcanzables y podrán eliminarse una a la vez.8.7.3 Optimizaciones del flujo de controlCon frecuencia, los algoritmos de generación de código intermedio simples producen saltos haciasaltos, saltos hacia saltos condicionales, o saltos condicionales hacia saltos. Estos saltos innecesarios se pueden eliminar, ya sea en el código intermedio o en el código destino, mediante lossiguientes tipos de optimizaciones de mirilla. Podemos sustituir la siguiente secuencia: goto L1 ...L1: goto L2por la secuencia: goto L2 ...L1: goto L2Si ahora no hay saltos a L1, entonces tal vez sea posible eliminar la instrucción L1: goto L2,siempre y cuando vaya precedida de un salto incondicional.De manera similar, la secuencia: if a < b goto L1 ...L1: goto L2puede sustituirse por la secuencia: if a < b goto L2 ...L1: goto L2Por último, suponga que sólo hay un salto hacia L1, y que L1 va precedida de un goto incondicional. Entonces, la secuencia:8.7 Optimización de mirilla (peephole) 551Maq. Cap_8_AHO.indd 551 11/10/07 12:59:00 AM552 Capítulo 8. Generación de código goto L1 ...L1: if a < b goto L2L3:puede sustituirse por la secuencia: if a < b goto L2 goto L3 ...L3:Mientras que el número de instrucciones en las dos secuencias sea el mismo, algunas vecesomitimos el salto incondicional en la segunda secuencia, pero nunca en la primera. Por ende,la segunda secuencia es superior a la primera en tiempo de ejecución.8.7.4 Simplificación algebraica y reducción por fuerzaEn la sección 8.5 hablamos sobre las identidades algebraicas que pueden utilizarse para simplificar los GDAs. Un optimizador de mirilla puede utilizar estas identidades algebraicas paraeliminar las instrucciones de tres direcciones, como:x = x + 0ox = x * 1en la mirilla.De manera similar, pueden aplicarse transformaciones de reducción por fuerza a la mirilla para sustituir las operaciones costosas por expresiones equivalentes más económicas en lamáquina destino. Ciertas instrucciones de máquina son considerablemente más económicasque otras, y a menudo pueden usarse como casos especiales de operadores más costosos. Porejemplo, x 2 es sin duda más económica de implementarse como x ∗ x que una llamada a unarutina de exponenciación. La multiplicación de punto flotante o división entre un exponentede dos es más económica de implementar que un desplazamiento. La división de punto flotante entre una constante puede aproximarse como una multiplicación por una constante, que puedeser más económica.8.7.5 Uso de características específicas de máquinaLa máquina destino puede tener instrucciones de hardware para implementar ciertas operaciones específicas con eficiencia. La acción de detectar situaciones que permitan el uso de estasinstrucciones puede reducir el tiempo de ejecución en forma considerable. Por ejemplo, algunasmáquinas tienen modos de direccionamiento de autoincremento y autodecremento. Estos modos suman o restan uno a un operando, antes o después de usar su valor. El uso de estos modosmejora en forma considerable la calidad del código al meter o sacar datos de una pila, comoen el paso de parámetros. Estos modos pueden usarse también en el código para instruccionescomo x = x + 1.Maq. Cap_8_AHO.indd 552 11/10/07 12:59:00 AM8.7.6 Ejercicios para la sección 8.7Ejercicio 8.7.1: Construya un algoritmo que realice la eliminación de instrucciones redundantes en una mirilla deslizable, en el código de la máquina de destino.Ejercicio 8.7.2: Construya un algoritmo que realice optimizaciones de control de flujo en unamirilla deslizable, en el código de la máquina de destino.Ejercicio 8.7.3: Construya un algoritmo que realice simplificaciones algebraicas simples yreducciones en fuerza en una mirilla deslizable, en el código de la máquina de destino.8.8 Repartición y asignación de registrosLas instrucciones que sólo utilizan operandos tipo registro son más rápidas que las que utilizanoperandos de memoria. En las máquinas modernas, las velocidades de los procesadores son amenudo un orden o más de magnitud más rápidas que las velocidades de las memorias. Por lotanto, la utilización eficiente de los registros es de extrema importancia para generar un buencódigo. Esta sección presenta diversas estrategias para decidir en cada punto de un programaqué valores deben residir en registros (repartición de registros) y en qué registro debe residircada valor (asignación de registros).Un método para la repartición y asignación de registros es asignar valores específicos enel programa de destino a ciertos registros. Por ejemplo, podríamos decidir asignar direccionesbase a un grupo de registros, cálculos aritméticos a otro, la parte superior de la pila a un registro fijo, y así en lo sucesivo.Este método tiene la ventaja de que simplifica el diseño de un generador de código. Sudesventaja es que, si se aplica con demasiado rigor, utiliza los registros en forma ineficiente; talvez no se utilicen ciertos registros en partes considerables del código, mientras que se generaninstrucciones de carga y almacenamiento innecesarias en los demás registros. No obstante, esrazonable en la mayoría de los entornos computacionales reservar unos cuantos registros paralos registros base, apuntadores de pila y demás, y permitir que el generador de código utiliceel resto de los registros según sea apropiado.8.8.1 Repartición global de registrosEl algoritmo de generación de código de la sección 8.6 utiliza registros para guardar valoresdurante el tiempo en que se ejecute un bloque básico individual. Sin embargo, todas las variables vivas se almacenan al final de cada bloque. Para ahorrar varias de estas instrucciones dealmacenamiento y sus instrucciones correspondientes de carga, podríamos hacer que se asignenregistros a las variables que se utilizan con frecuencia y mantener esos registros consistentes entre los límites de los bloques (en forma global ). Debido a que los programas invierten la mayoríade su tiempo en ciclos internos, un método natural para la repartición global de registros estratar de mantener un valor utilizado con frecuencia en un registro fijo, durante todo un ciclo.Mientras tanto, suponga que conocemos la estructura del ciclo de un grafo de flujo, y sabemosqué valores calculados en un bloque básico se utilizan fuera de ese bloque. El siguiente capítulotrata sobre las técnicas para calcular esta información.8.8 Repartición y asignación de registros 553Maq. Cap_8_AHO.indd 553 11/10/07 12:59:01 AM554 Capítulo 8. Generación de códigoUna estrategia para la repartición global de registros es asignar cierto número fijo de registros para guardar los valores más activos en cada ciclo interno. Los valores seleccionados pueden ser distintos en varios ciclos. Los registros que no se hayan repartido ya pueden usarse paracontener valores que sean locales para un bloque, como en la sección 8.6. Este método tiene ladesventaja de que el número fijo de registros no es siempre el número correcto de registros quedebe haber disponibles para la repartición global de los mismos. Aun así, es sencillo implementar el método y se utilizó en Fortran H, el compilador optimizador de Fortran desarrollado porIBM, para las máquinas de la serie 360 a finales de la década de 1960.Con los primeros compiladores de C, un programador podía realizar cierta repartición deregistros en forma explícita, mediante el uso de declaraciones de registros para mantener ciertosvalores en registros durante la ejecución de un procedimiento. El uso juicioso de las declaraciones de registros agilizó muchos programas, pero se alentó a los programadores a perfilarprimero sus programas, para determinar los puntos clave del programa antes de realizar supropia repartición de registros.8.8.2 Conteos de usoEn esta sección vamos a suponer que el ahorro que se obtiene al mantener una variable x en unregistro durante la ejecución de un ciclo L es de una unidad de costo por cada referencia a x, sies que x ya se encuentra en un registro. No obstante, si utilizamos el método de la sección 8.6para generar código para un bloque, hay una buena probabilidad de que una vez que se hayacalculado x en el bloque, permanecerá en un registro si hay usos subsiguientes de x en ese bloque. Por ende, contamos un ahorro de una unidad para cada uso de x en el ciclo L que no vayaprecedido por una asignación a x en el mismo bloque. Por ende, si a x se le asigna un registro,contamos un ahorro de dos unidades por cada bloque en el ciclo L, para el cual x esté viva alsalir y en el que a x se le asigne un valor.En el lado deudor, si x está viva al entrar al encabezado del ciclo, debemos cargar x en suregistro justo antes de entrar al ciclo L. Esta carga cuesta dos unidades. De manera similar,para cada bloque B de salida del ciclo L, en el cual x esté viva al entrar a algún sucesor de Bfuera de L, debemos almacenar x a un costo de dos. No obstante, si suponemos que el ciclose itera muchas veces, podemos ignorar estas deudas ya que sólo ocurren una vez cada queentramos al ciclo. Así, una fórmula aproximada para poder obtener el beneficio de asignar unregistro x dentro del ciclo L es:bloques B en Luso viva (8.1)en donde uso(x, B) es el número de veces que se utiliza x en B antes de cualquier definición de x;viva(x, B) es 1 si x está viva al salir de B y se le asigna un valor en B, y viva(x, B) es 0 en cualquier otro caso. Observe que la ecuación (8.1) es aproximada, ya que no todos los bloques en unciclo se ejecutan con igual frecuencia, y también porque la ecuación (8.1) se basa en la suposiciónde que un ciclo se itera muchas veces. En máquinas específicas, habría que desarrollar una fórmulaanáloga a (8.1), pero tal vez muy distinta de ella.Maq. Cap_8_AHO.indd 554 11/10/07 12:59:01 AMEjemplo 8.17: Considere los bloques básicos en el ciclo interno descrito en la figura 8.17, endonde se han omitido las instrucciones de salto y de salto condicional. Suponga que los registros R0, R1 y R2 se reparten para guardar valores a lo largo del ciclo. Las variables vivas alentrar y al salir de cada bloque se muestran en la figura 8.17 por conveniencia, justo encimay debajo de cada bloque, respectivamente. Hay algunos puntos delicados acerca de las variables vivas que trataremos en el siguiente capítulo. Por ejemplo, observe que tanto e como festán vivas al final de B 1, pero de éstas, sólo e está viva al entrar a B 2 y sólo f al entrar aB 3. En general, las variables vivas al final de un bloque son la unión de las que están vivas alprincipio de cada uno de sus bloques sucesores.vivasvivasFigura 8.17: Grafo de flujo de un ciclo internoPara evaluar la ecuación (8.1) para x = a, observamos que a está viva al salir de B 1 y se leasigna un valor ahí, pero no está viva al salir de B 2, B 3 o B 4. Por ende, ªB en L uso (a, B) = 2.Así, el valor de (8.1) para x = a es 4. Es decir, podemos ahorrar cuatro unidades de costo siseleccionamos a para uno de los registros globales. Los valores de (8.1) para b, c, d, e y f son5, 3, 6, 4 y 4, respectivamente. De este modo, podemos seleccionar a, b y d para los registrosR0, R1 y R2, respectivamente. Usar R0 para e o f en vez de a sería otra elección con el mismo beneficio aparente. La figura 8.18 muestra el código ensamblador generado a partir de lafigura 8.17, suponiendo que se utiliza la estrategia de la sección 8.6 para generar código paracada bloque. No mostramos el código generado para los saltos condicionales o incondicionalesomitidos que terminan cada bloque en la figura 8.17 y, por lo tanto, no mostraremos el códigogenerado como un solo flujo, como aparecería en la práctica. ✷8.8 Repartición y asignación de registros 555Maq. Cap_8_AHO.indd 555 11/10/07 12:59:02 AM556 Capítulo 8. Generación de códigoFigura 8.18: Secuencia de código que utiliza la repartición global de registros8.8.3 Asignación de registros para ciclos externosYa que hemos asignado registros y generado código para los ciclos internos, podemos aplicarla misma idea a ciclos circundantes cada vez más grandes. Si un ciclo externo L 1 contiene unciclo interno L 2, a los nombres que se les asignaron registros en L 2 no se les necesita asignarregistros en L 1 − L 2. De manera similar, si elegimos asignar a x un registro en L 2 pero noen L 1, debemos cargar x al entrar a L 2 y almacenar x al salir de L 2. Dejamos como ejerciciopara el lector la derivación de un criterio para seleccionar los nombres que se van a asignar alos registros en un ciclo externo L, dado que ya se han hecho elecciones para todos los ciclosanidados dentro de L.8.8.4 Asignación de registros mediante la coloración de grafosCuando se requiere un registro para un cálculo pero todos los registros disponibles están en uso,el contenido de uno de los registros debe almacenarse (derramarse) en una ubicación de memoria, para poder liberar un registro. La coloración de grafos es una técnica simple y sistemáticapara asinar registros y administrar los derrames de los mismos.En este método se utilizan dos pasadas. En la primera, se seleccionan instrucciones de la máquina destino como si hubiera un número infinito de registros simbólicos; en efecto, los nombresMaq. Cap_8_AHO.indd 556 11/10/07 12:59:03 AMque se utilizan en el código intermedio se convierten en los nombres de los registros, y las instrucciones de tres direcciones se convierten en instrucciones en lenguaje máquina. Si el acceso a lasvariables requiere instrucciones que utilicen apuntadores de pila, apuntadores de visualización,registros base o demás cantidades que ayuden al acceso, entonces asumimos que estas cantidadesse guardan en los registros reservados para cada fin. Por lo general, su uso puede traducirse enforma directa a un modo de acceso para una dirección mencionada en una instrucción de máquina. Si el acceso es más complejo, debe descomponerse en varias instrucciones de máquina, y talvez haya que crear un registro simbólico temporal (o varios).Una vez seleccionadas las instrucciones, una segunda pasada asigna los registros físicos alos simbólicos. El objetivo es encontrar una asignación que disminuya al mínimo el costo de losderrames.En la segunda pasada, para cada procedimiento se construye un grafo de interferencia deregistros, en el cual los nodos son registros simbólicos y una flecha conecta a dos nodos, si unoestá vivo en un punto en el que el otro está definido. Por ejemplo, un gráfico de interferenciade registros para la figura 8.17 tendría nodos para los nombres a y d. En el bloque B 1, a estáviva en la segunda instrucción, que define a d; por lo tanto, en el gráfico habría un flanco entrelos nodos para a y d.Se hace un intento por colorear el grafo de interferencia de registros usando k colores, endonde k es el número de registros asignables. Se dice que un grafo está coloreado si a cada nodose le asigna un color de tal forma que no haya dos nodos adyacentes con el mismo color. Uncolor representa a un registro, y este color se asegura de que a dos registros simbólicos, quepuedan interferir uno con el otro, no se les asigne el mismo registro físico.Aunque el problema de determinar si un grafo puede colorearse con k colores es NP-completoen general, por lo regular se puede utilizar la siguiente técnica heurística para realizar la coloracióncon rapidez. Suponga que un nodo n en un grafo G tiene menos de k vecinos (nodos conectados an por una flecha). Elimine a n y sus flechas de G para obtener un grafo G. Una coloración con kcolores de G puede extenderse a una coloración con k colores de G, si se asigna a n un color queno se haya asignado a ninguno de sus vecinos.Al eliminar en forma repetida los nodos que tengan menos de k flancos del grafo de interferencia de registros, podemos obtener el grafo vacío, en cuyo caso podemos producir una coloración con k colores para el grafo original, coloreando los nodos en el orden inverso en el que seeliminaron, o podemos obtener un gráfico en el cual cada nodo tenga k o más nodos adyacentes.En este último caso, ya no es posible realizar una coloración con k colores. En este punto, sederrama un nodo introduciendo código para almacenar y recargar el registro. Chaitin ha ideadovarias heurísticas para elegir el nodo a derramar. Una regla general es evitar introducir códigode derrame en los ciclos internos.8.8.5 Ejercicios para la sección 8.8Ejercicio 8.8.1: Construya el grafo de interferencia de registros para el programa en la figura8.17.Ejercicio 8.8.2: Idee una estrategia de asignación de registros, suponiendo que almacenamosde manera automática todos los registros en la pila antes de cada llamada a un procedimiento,y que los restauramos después de su retorno.8.8 Repartición y asignación de registros 557Maq. Cap_8_AHO.indd 557 11/10/07 12:59:05 AM558 Capítulo 8. Generación de código8.9 Selección de instrucciones mediante la rescriturade árbolesLa selección de instrucciones puede ser una extensa tarea combinatoria, en especial en las máquinas que tienen muchos modos de direccionamiento, como las máquinas CISC, o en las máquinascon instrucciones de propósito especial, digamos, para el procesamiento de señales. Aun cuandoasumimos que se proporciona el orden de evaluación y que los registros se asignan mediante unmecanismo separado, la selección de instrucciones (el problema de seleccionar instrucciones en ellenguaje destino para implementar los operadores en la representación intermedia) sigue siendouna extensa tarea combinatoria.En esta sección, manejamos la selección de instrucciones como un problema de rescritura deárboles. Las representaciones tipo árbol de las instrucciones destino se han utilizado de maneraefectiva en los generadores de generadores de código, que en forma automática construyen lafase de selección de instrucciones de un generador de código, a partir de una especificación dealto nivel de la máquina destino. Podría obtenerse mejor código para algunas máquinas mediante el uso de GDAs en lugar de árboles, pero la coincidencia de GDAs es más compleja quela coincidencia de árboles.8.9.1 Esquemas de traducción de árbolesA lo largo de esta sección, la entrada al proceso de generación de código será una secuenciade árboles en el nivel semántico de la máquina de destino. Los árboles son lo que podríamosobtener después de insertar instrucciones en tiempo de ejecución en la representación intermedia, como se describe en la sección 8.3. Además, las hojas de los árboles contienen informaciónacerca de los tipos de almacenamiento de sus etiquetas.Ejemplo 8.18: La figura 8.19 contiene un árbol para la instrucción de asignación a[i] = b + 1,en donde el arreglo a se almacena en la pila en tiempo de ejecución, y la variable b es global enla ubicación de memoria Mb. Las direcciones en tiempo de ejecución de las variables locales a e ise proporcionan como los desplazamientos constantes Ca y Ci de SP, el registro que contiene elapuntador al principio del registro de activación actual.La asignación para a[i] es una asignación indirecta, en la cual el valor r de la ubicaciónpara a[i] se establece al valor r de la expresión b + 1. Las direcciones del arreglo a y la variable i se proporcionan agregando los valores de las constantes Ca y Ci, respectivamente, alcontenido del registro SP. Para simplificar los cálculos de las direcciones del arreglo, asumimosque todos los valores son caracteres de un byte. Algunos conjuntos de instrucciones hacenprovisiones especiales para las multiplicaciones por constantes, como 2, 4 y 8, durante loscálculos de las direcciones.En el árbol, el operador ind trata a su argumento como una dirección de memoria. Como elhijo izquierdo de un operador de asignación, el nodo ind proporciona la ubicación en la que sealmacenará el valor r del lado derecho del operador de asignación. Si un argumento de a + o deloperador ind es una ubicación de memoria o un registro, entonces se toma el contenido de esaubicación de memoria o registro como valor. Las hojas en el árbol se etiquetan con atributos;un subíndice indica el valor del atributo. ✷Maq. Cap_8_AHO.indd 558 11/10/07 12:59:05 AMFigura 8.19: Árbol de código intermedio para a[i] =b+1El código destino se genera aplicando una secuencia de reglas de rescritura de árboles, parareducir el árbol de entrada a un solo nodo. Cada regla de rescritura de árboles tiene la siguienteforma:sustituto ← plantilla { acción }en donde sustituto es un solo nodo, plantilla es un árbol y acción es un fragmento de código,como en un esquema de traducción orientado por la sintaxis.A un conjunto de reglas de rescritura de árboles se le conoce como esquema de traducciónde árboles.Cada regla de rescritura de árboles representa la traducción de una parte del árbol que proporciona la plantilla. La traducción consiste en una secuencia posiblemente vacía de instruccionesde máquina, emitida por la acción asociada con la plantilla. Las hojas de la plantilla son atributos con subíndices, como en el árbol de entrada. Algunas veces se aplican ciertas restricciones alos valores de los subíndices en las plantillas; estas restricciones se especifican como predicadossemánticos que deben cumplirse para poder decir que la plantilla tiene una coincidencia. Porejemplo, un predicado podría especificar que el valor de una constante caiga en un cierto rango.Un esquema de traducción de árboles es una forma conveniente de representar la fase deselección de instrucciones de un generador de código. Como ejemplo de una regla de rescriturade árboles, considere la regla para la instrucción de suma, de registro a registro:Esta regla se utiliza de la siguiente manera. Si el árbol de entrada contiene un subárbol quecoincide con esta plantilla de árbol, es decir, un subárbol cuya raíz esté etiquetada por el operador + y cuyos hijos izquierdo y derecho sean cantidades en los registros i y j, entonces podemossustituir ese subárbol por un solo nodo etiquetado como Ri y generar la instrucción ADD Ri,Ri, Rj como salida. A esta sustitución le llamamos revestimiento (tiling) del subárbol. Más deuna plantilla puede coincidir con un subárbol en un momento dado; en breve describiremos losmecanismos para decidir qué regla aplicar, en casos de conflicto.Ejemplo 8.19: La figura 8.20 contiene reglas de rescritura de árboles para algunas instrucciones de nuestra máquina de destino. A lo largo de esta sección utilizaremos estas reglas en un8.9 Selección de instrucciones mediante la rescritura de árboles 559Maq. Cap_8_AHO.indd 559 11/10/07 12:59:06 AM560 Capítulo 8. Generación de códigoejemplo abierto. Las primeras dos reglas corresponden a instrucciones de carga, las siguientesdos para las instrucciones de almacenamiento, y el resto para las cargas y sumas indexadas.Observe que la regla (8) requiere que el valor de la constante sea 1. Esta condición se especificamediante un predicado semántico. ✷8.9.2 Generación de código mediante el revestimientode un árbol de entradaUn esquema de traducción de árboles funciona de la siguiente manera. Dado un árbol de entrada, se aplican las plantillas en las reglas de rescritura para revestir sus subárboles. Si una plantilla coincide, el subárbol coincidente en el árbol de entrada se sustituye con el nodo de reemplazode la regla y termina la acción asociada con la regla. Si la acción contiene una secuencia deinstrucciones de máquina, se generan las instrucciones. Este proceso se repite hasta que el árbolse reduce a un solo nodo, o hasta que no haya coincidencia entre las plantillas. La secuencia deinstrucciones de máquina generadas a medida que el árbol de entrada se reduce a un solo nodoconstituye la salida del esquema de traducción de árboles en el árbol de entrada dado.El proceso de especificar un generador de código es muy similar al de usar un esquema de traducción orientado por la sintaxis para especificar a un traductor. Escribimos un esquema de traducción de árboles para describir el conjunto de instrucciones de una máquina destino. En la práctica esconveniente encontrar un esquema que genere una secuencia de instrucciones con un costo mínimopara cada árbol de entrada. Existen varias herramientas para ayudar a construir un generador decódigo de manera automática, a partir de un esquema de traducción de árboles.Ejemplo 8.20: Vamos a usar el esquema de traducción de árboles de la figura 8.20 para generar código en el árbol de entrada de la figura 8.19. Suponga que se aplica la primera regla paracargar la constante Ca en el registro R0:La etiqueta de la hoja de más a la izquierda cambia entonces de Ca a R 0 y se genera la instrucción LD R0, #a. Ahora la séptima regla coincide con el subárbol de más a la izquierda, con laraíz etiquetada como +:Mediante esta regla, rescribimos este subárbol como un solo nodo etiquetado como R 0 y generamos la instrucción ADD R0, R0, SP. Ahora el árbol tiene la siguiente apariencia:Maq. Cap_8_AHO.indd 560 11/10/07 12:59:07 AMFigura 8.20: Reglas de rescritura de árboles para algunas instrucciones de la máquina destino8.9 Selección de instrucciones mediante la rescritura de árboles 561Maq. Cap_8_AHO.indd 561 11/10/07 12:59:08 AM562 Capítulo 8. Generación de códigoEn este punto, podríamos aplicar la regla (5) para reducir el subárbol:a un solo nodo etiquetado como R 1, por ejemplo. También podríamos usar la regla (6) parareducir el subárbol más grande:a un solo nodo etiquetado como R 0 y generar la instrucción ADD R0, R0, i(SP). Suponiendo quees más eficiente usar una sola instrucción para calcular el subárbol más grande, en vez del máspequeño, elegimos la regla (6) para obtener lo siguiente:En el subárbol derecho, la regla (2) se aplica a la hoja Mb. Genera una instrucción para cargarb en el registro R1, por ejemplo. Ahora, usando la regla (8) podemos igualar el subárbol:y generar la instrucción de incremento INC R1. En este punto, el árbol de entrada se ha reducido a:Este árbol restante se iguala mediante la regla (4), la cual reduce el árbol a un solo nodo ygenera la instrucción ST *R0, R1. Generamos la siguiente secuencia de código:en el proceso de reducir el árbol a un solo nodo. ✷Maq. Cap_8_AHO.indd 562 11/10/07 12:59:09 AMPara poder implementar el proceso de reducción de árboles en el ejemplo 8.18, debemosconsiderar ciertas cuestiones relacionadas con la coincidencia de patrones de árboles:• ¿Cómo deben coincidir los patrones de árboles? La eficiencia del proceso de generaciónde código (en tiempo de compilación) depende de la eficiencia del algoritmo para igualarárboles.• ¿Qué hacemos si más de una plantilla coincide en un momento dado? La eficiencia del código generado (en tiempo de ejecución) puede depender del orden en el que se igualan lasplantillas, ya que en general, distintas secuencias de coincidencias conducirán a distintassecuencias de código de la máquina destino, algunas más eficientes que otras.Si ninguna plantilla coincide, entonces el proceso de generación de código se bloquea. En elotro extremo, debemos protegernos contra la posibilidad de que se rescriba un solo nodo en formaindefinida, generando una secuencia infinita de instrucciones de movimiento de registros, o unasecuencia infinita de instrucciones de carga y almacenamiento.Para evitar el bloqueo, suponemos que cada operador en el código intermedio puede implementarse mediante una o más instrucciones de la máquina destino. Además, suponemosque hay suficientes registros para calcular cada uno de los nodos del árbol por sí solos. Así,no importa cómo se dé el proceso de igualar el árbol, ya que el árbol restante siempre podrátraducirse en instrucciones de la máquina destino.8.9.3 Coincidencias de los patrones mediante el análisis sintácticoAntes de considerar el proceso general de igualar árboles, consideraremos un método especializado que utiliza un analizador sintáctico LR para buscar las coincidencias de los patrones.El árbol de entrada puede tratarse como una cadena, usando su representación en prefijo. Porejemplo, la representación prefijo para el árbol de la figura 8.19 es:El esquema de traducción de árboles puede convertirse en un esquema de traducción orientado por la sintaxis, para lo cual se sustituyen las reglas de rescritura de árboles con las producciones de una gramática libre de contexto, en la que los lados derechos son representacionesprefijas de las plantillas de instrucciones.Ejemplo 8.21: El esquema de traducción orientado por la sintaxis en la figura 8.21 se basaen el esquema de traducción de árboles de la figura 8.20.Los no terminales de la gramática subyacente son R y M. El terminal m representa unaubicación en memoria específica, como la ubicación para la variable global b en el ejemplo 8.18.La producción M → m en la regla (10) puede considerarse como el proceso de igualar M con m,antes de usar una de las plantillas que involucran a M. De manera similar, introducimos el terminalsp para el registro SP y agregamos la producción R → SP. Por último, la terminal c representa alas constantes.8.9 Selección de instrucciones mediante la rescritura de árboles 563Maq. Cap_8_AHO.indd 563 11/10/07 12:59:10 AM564 Capítulo 8. Generación de códigoFigura 8.21: Esquema de traducción orientado por la sintaxis, construido a partir de la figura8.20Usando estos terminales, la cadena para el árbol de entrada en la figura 8.19 es:✷De las producciones para el esquema de traducción, construimos un analizador sintáctico LRusando una de las técnicas de construcción de analizadores sintácticos LR que vimos en el capítulo 4. Para generar el código destino, se genera la instrucción de máquina correspondiente a cadareducción.Por lo general, una gramática de generación de código es muy ambigua, y hay que tener cuidado en la forma en que se resuelven los conflictos de acción de análisis sintáctico cuando se construye el analizador sintáctico. En la ausencia de información sobre los costos, una regla general es darpreferencia a las reducciones más grandes sobre las más pequeñas. Esto significa que en un conflicto de reducción-reducción, se favorece a la reducción más larga; en un conflicto de desplazamientoreducción, se elige el movimiento por desplazamiento. Este método del “máximo procesado” haceque se realice un gran número de operaciones con una sola instrucción de máquina.Hay algunos beneficios en el uso del análisis sintáctico LR en la generación de código. Enprimer lugar, el método del análisis sintáctico es eficiente y bien comprendido, por lo que puedenproducirse generadores de código confiables y eficientes mediante el uso de los algoritmos descritos en el capítulo 4. En segundo lugar, es muy sencillo redirigir el generador de código resultante;puede construirse un selector de código para una nueva máquina mediante la escritura de unagramática para describir las instrucciones de la nueva máquina. En tercer lugar, la calidad delcódigo generado puede volverse eficiente al agregar producciones de casos especiales para aprovechar características específicas de las máquinas.Sin embargo, existen también algunos retos. El método de análisis sintáctico fija un ordende evaluación de izquierda a derecha. Además, para algunas máquinas con grandes números demodos de direccionamiento, la gramática de descripción de la máquina y el analizador sintáctico resultante pueden aumentar de tamaño en forma desordenada. Como consecuencia, sonnecesarias técnicas especializadas para codificar y procesar las gramáticas de descripción de lamáquina. También debemos tener cuidado en que el analizador sintáctico resultante no se bloquee (que no pueda hacer más movimientos) mientras analiza un árbol de expresiones, ya seadebido a que la gramática no maneja algunos patrones de operadores, o porque el analizadorMaq. Cap_8_AHO.indd 564 11/10/07 12:59:11 AMsintáctico ha tomado la resolución incorrecta de algún conflicto de acción del análisis sintáctico.También debemos asegurarnos de que el analizador sintáctico no entre en un ciclo infinito dereducciones de producciones con símbolos individuales en el lado derecho. El problema de losciclos puede resolverse mediante el uso de una técnica de división de estados, al mismo tiempoen el que se generan las tablas del analizador sintáctico.8.9.4 Rutinas para la comprobación semánticaEn un esquema de traducción para la generación de código, aparecen los mismos atributos que enun árbol de entrada, pero a menudo con restricciones en los valores que pueden tener los subíndices.Por ejemplo, una instrucción de máquina puede requerir que el valor de un atributo se encuentredentro de cierto rango, o que los valores de dos atributos estén relacionados.Estas restricciones sobre los valores de los atributos pueden especificarse como predicadosque se invocan antes de realizar una reducción. De hecho, el uso general de acciones semánticasy predicados puede proporcionar una mayor flexibilidad y facilidad de descripción que unaespecificación sólo gramatical de un generador de código. Pueden utilizarse plantillas genéricaspara representar clases de instrucciones y después pueden usarse las acciones semánticas paraelegir instrucciones en los casos específicos. Por ejemplo, dos formas de la instrucción de sumapueden representarse con una plantilla:Los conflictos de acción del análisis sintáctico pueden resolverse mediante predicados paraeliminar ambigüedades, los cuales pueden permitir el uso de distintas estrategias de selecciónen varios contextos. Es posible una descripción más pequeña de una máquina destino, ya queciertos aspectos de la arquitectura de la máquina, como los modos de direccionamiento, puedenfactorizarse en los atributos. La complicación en este método es que puede ser difícil verificar laprecisión del esquema de traducción como una descripción fiel de la máquina destino, aunqueeste problema se comparte hasta cierto grado por todos los generadores de código.8.9.5 Proceso general para igualar árbolesEl método de análisis sintáctico LR para la coincidencia de patrones basada en las representaciones prefijas favorece al operando izquierdo de un operador binario. En una representación prefijaop E 1 E 2, las decisiones del análisis sintáctico LR de lectura anticipada limitada deben hacerseen base a cierto prefijo de E 1, ya que E 1 puede tener una longitud arbitraria. Por ende, la coincidencia de patrones puede omitir matices del conjunto de instrucciones destino que se deben alos operandos derechos.En vez de la representación prefija, podríamos usar una representación postfija. Pero, entonces un método de análisis sintáctico LR para la coincidencia de patrones favorecería al operandoderecho.Para un generador de código escrito a mano podemos usar plantillas de árboles, como en lafigura 8.20, como una guía y escribir un igualador ad hoc. Por ejemplo, si la raíz del árbol deentrada se etiqueta como ind, entonces el único patrón que podría coincidir es para la regla (5);8.9 Selección de instrucciones mediante la rescritura de árboles 565Maq. Cap_8_AHO.indd 565 11/10/07 12:59:12 AM566 Capítulo 8. Generación de códigoen cualquier otro caso, si la raíz está etiquetada como +, entonces los patrones que podría coincidir son para las reglas (6-8).Para un generador de generadores de código, necesitamos un algoritmo general de coincidencia de árboles. Podemos desarrollar un algoritmo descendente eficiente, extendiendo las técnicasde coincidencia de patrones de cadenas del capítulo 3. La idea es representar cada plantilla comoun conjunto de cadenas, en donde una cadena corresponde a una ruta de la raíz hacia una hojaen la plantilla. Tratamos a todos los operandos de igual forma, incluyendo el número de posiciónde un hijo, de izquierda a derecha, en las cadenas.Ejemplo 8.22: Para construir el conjunto de cadenas para un conjunto de instrucciones, vamos a omitir los subíndices, ya que la coincidencia de patrones se basa sólo en los atributos,no en sus valores.Las plantillas de la figura 8.22 tienen el siguiente conjunto de cadenas, desde la raíz hastauna hoja:La cadena C representa a la plantilla que tiene a C en la raíz. La cadena + 1 R representa al +y su operando izquierdo R en las dos plantillas que tienen a + en la raíz. ✷Figura 8.22: Un conjunto de instrucciones para la coincidencia de árbolesSi utilizamos conjuntos de cadenas como en el ejemplo 8.22, podemos construir un igualador de patrones de árboles mediante el uso de las técnicas para igualar con eficiencia variascadenas en paralelo.En la práctica, el proceso de rescritura de árboles puede implementarse mediante la ejecucióndel igualador de patrones de árboles durante un recorrido en profundidad (depth-first) del árbolde entrada, y realizando las reducciones a medida que se visitan los nodos por última vez.Podemos tomar en cuenta los costos de las instrucciones, asociando con cada regla de rescritura de árboles el costo de la secuencia de instrucciones de máquina que se generan, si seaplica esa regla. En la sección 8.11 veremos un algoritmo de programación dinámica que puedeusarse en conjunto con la coincidencia de patrones de árboles.Al ejecutar el algoritmo de programación dinámica en forma concurrente, podemos seleccionar una secuencia óptima de coincidencias, usando la información de costos asociada conMaq. Cap_8_AHO.indd 566 11/10/07 12:59:13 AMcada regla. Tal vez haya que diferir la decisión sobre una coincidencia hasta que se conozca elcosto de todas las alternativas. Mediante el uso de este método, podemos construir con rapidezun generador de código eficiente y pequeño, a partir de un esquema de rescritura de árboles.Además, el algoritmo de programación dinámica libera al diseñador del generador de código detener que resolver las coincidencias conflictivas, o decidir sobre un orden para la evaluación.8.9.6 Ejercicios para la sección 8.9Ejercicio 8.9.1: Construya árboles sintácticos para cada una de las siguientes instrucciones,suponiendo que todos los operandos no constantes se encuentran en ubicaciones de memoria:a) x = a * b + c * d;b) x[i] = y[j] * z[k];c) x = x + 1;Use el esquema de rescritura de árboles de la figura 8.20 para generar el código de cada instrucción.Ejercicio 8.9.2: Repita el ejercicio 8.9.1 anterior, mediante el esquema de traducción orientado por la sintaxis de la figura 8.21 en vez del esquema de rescritura de árboles.Ejercicio 8.9.3: Extienda el esquema de rescritura de árboles de la figura 8.20 para que seaplique a las instrucciones while.Ejercicio 8.9.4: ¿Cómo extendería la rescritura de árboles para aplicarla a los GDAs?8.10 Generación de código óptimo para las expresionesPodemos elegir registros de manera óptima cuando un bloque básico consiste en una sola evaluación de una expresión, o si aceptamos que basta con generar código para un bloque, unaexpresión a la vez. En el siguiente algoritmo, presentamos un esquema de numeración para losnodos de un árbol de expresión (un árbol sintáctico para una expresión) que nos permite generarel código óptimo para un árbol de expresión, cuando hay un número fijo de registros con los quese puede evaluar la expresión.8.10.1 Números de ErshovEmpezaremos por asignar a los nodos de un árbol de expresión un número que indique cuántosregistros se necesitan para evaluar ese nodo, sin almacenar valores temporales. A estos números seles llama algunas veces números de Ershov, por A. Ershov, quien utilizó un esquema similar paralas máquinas con un solo registro aritmético. Para nuestro modelo de máquina, las reglas son:1. Etiquetar cualquier hoja con 1.2. La etiqueta de un nodo interior con un hijo es la etiqueta de su hijo.8.10 Generación de código óptimo para las expresiones 567!!Maq. Cap_8_AHO.indd 567 11/10/07 12:59:14 AM568 Capítulo 8. Generación de código3. La etiqueta de un nodo interior con dos hijos es: (a) La más grande de las etiquetas de sus hijos, si esas etiquetas son distintas. (b) Uno más la etiqueta de sus hijos, si las etiquetas son iguales.Ejemplo 8.23: En la figura 8.23 vemos un árbol de expresión (se omitieron los operadores)que podría ser el árbol para la expresión (a − b ) + e × (c + d ) o el siguiente código de tresdirecciones: t1 = a − bt2 = c + dt3 = e * t2t4 = t1 + t3Cada una de las cinco hojas se etiqueta como 1 mediante la regla (1). Así, podemos etiquetarel nodo interior para t1 = a − b, ya que ambos de sus hijos están etiquetados. La regla (3b) seaplica, por lo que obtiene una etiqueta más que las etiquetas de sus hijos, es decir, 2. Lo mismose aplica para el nodo interior para t2 = c + d.Figura 8.23: Un árbol etiquetado con números de ErshovAhora podemos trabajar en el nodo para t3 = e * t2. Sus hijos tienen las etiquetas 1 y 2,por lo que la etiqueta del nodo para t3 es la máxima, 2, según la regla (3a). Por último, laraíz, el nodo para t4 = t1 + t3, tiene dos hijos con la etiqueta 2 y, por lo tanto, obtienela etiqueta 3. ✷8.10.2 Generación de código a partir de árbolesde expresión etiquetadosPuede demostrarse que, en nuestro modelo de máquina, en donde todos los operandos debenestar en registros, y los registros pueden utilizarse tanto por un operando como por el resultadode una operación, la etiqueta de un nodo es la menor cantidad de registros con los que se puedeevaluar la expresión, sin utilizar almacenamientos de resultados temporales. Como en este modelo estamos obligados a cargar cada operando y a calcular el resultado correspondiente a cadanodo interior, lo único que puede hacer que el código generado sea inferior al código óptimo essi hay almacenamientos innecesarios de valores temporales. El argumento para esta declaraciónestá incrustado en el siguiente algoritmo para generar código sin almacenamientos de valorestemporales, utilizando un número de registros igual a la etiqueta de la raíz.Maq. Cap_8_AHO.indd 568 11/10/07 12:59:14 AMAlgoritmo 8.24: Generación de código a partir de un árbol de expresión etiquetado.ENTRADA: Un árbol etiquetado con cada operando que aparece una vez (es decir, sin subexpresiones comunes).SALIDA: Una secuencia óptima de instrucciones de máquina para evaluar la raíz y colocarlaen un registro.MÉTODO: A continuación se muestra un algoritmo recursivo para generar el código máquina. Los siguientes pasos se aplican, empezando en la raíz del árbol. Si el algoritmo se aplica aun nodo con la etiqueta k, entonces sólo se utilizarán k registros. No obstante, hay una “base”b ¦ 1 para los registros utilizados, de manera que los registros actuales utilizados son Rb , Rb + 1,…, Rb + k − 1. El resultado siempre aparece en Rb + k − 1.1. Para generar código máquina para un nodo interior con la etiqueta k y dos hijos conetiquetas iguales (que deben ser k − 1), haga lo siguiente: (a) Genere código en forma recursiva para el hijo derecho, usando la base b + 1. Elresultado del hijo derecho aparece en el registro Rb + k . (b) Genere código en forma recursiva para el hijo izquierdo, usando la base b; el resultado aparece en Rb + k − 1. (c) Genere la instrucción OP Rb + k , Rb + k − 1, Rb + k , en donde OP es la operación apropiada para el nodo interior en cuestión.2. Suponga que tenemos un nodo interior con la etiqueta k e hijos con etiquetas desiguales.Entonces uno de los hijos, al que llamaremos el hijo “grande”, tiene la etiqueta k, y elotro hijo, el hijo “pequeño”, tiene cierta etiqueta m < k. Haga lo siguiente para generarcódigo para este nodo interior, usando la base b : (a) Genere código en forma recursiva para el hijo grande, usando la base b; el resultadoaparece en el registro Rb +k − 1. (b) Genere código en forma recursiva para el hijo pequeño, usando la base b; el resultado aparece en el registro Rb + m − 1. Observe que como m < k, no se utiliza Rb + k − 1ni cualquier otro registro con numeración más alta. (c) Genere la instrucción OP Rb + k − 1, Rb + m − 1, Rb + k − 1 o la instrucción OP Rb + k − 1,Rb + k − 1, Rb + m − 1, dependiendo de si el hijo grande es el derecho o el izquierdo,respectivamente.3. Para una hoja que represente al operando x, si la base es b genere la instrucción LD Rb, x.✷Ejemplo 8.25: Vamos a aplicar el Algoritmo 8.24 al árbol de la figura 8.23. Como la etiquetade la raíz es 3, el resultado aparecerá en R 3 y sólo se utilizarán R 1, R 2 y R 3. La base para laraíz es b = 1. Como la raíz tiene hijos con etiquetas iguales, generamos código para el hijoderecho primero, con la base 2.8.10 Generación de código óptimo para las expresiones 569Maq. Cap_8_AHO.indd 569 11/10/07 12:59:15 AM570 Capítulo 8. Generación de códigoAl generar código para el hijo derecho de la raíz, etiquetado como t3, encontramos que elhijo grande es el hijo derecho y que el hijo pequeño es el hijo izquierdo. Por ende, generamoscódigo para el hijo derecho primero, con b = 2. Si aplicamos las reglas para hijos y hojas conetiquetas iguales, generamos el siguiente código para el nodo etiquetado como t2:LD R3, dLD R2, cADD R3, R2, R3A continuación, generamos código para el hijo izquierdo del hijo derecho de la raíz; este nodoes la hoja etiquetada como e. Debido a que b = 2, la instrucción apropiada es:LD R2, eAhora podemos completar el código para el hijo derecho de la raíz, agregando la siguienteinstrucción:MUL R3, R2, R3El algoritmo procede a generar código para el hijo izquierdo de la raíz, dejando el resultado enR 2 y con la base 1. La secuencia completa de instrucciones se muestra en la figura 8.24. ✷ LD R3, dLD R2, cADD R3, R2, R3LD R2, eMUL R3, R2, R3LD R2, bLD R1, aSUB R2, R1, R2ADD R3, R2, R3Figura 8.24: Código óptimo de tres registros para el árbol de la figura 8.238.10.3 Evaluación de expresiones con un suministro insuficientede registrosCuando hay un número de registros disponibles menor a la etiqueta de la raíz del árbol, nopodemos aplicar el Algoritmo 8.24 en forma directa. Debemos introducir ciertas instruccionesde almacenamiento que derramen los valores de los subárboles en la memoria, y después tenemos que cargar esos valores de vuelta en los registros, según se vayan requiriendo. He aquí elalgoritmo modificado que toma en cuenta una limitación sobre el número de registros.Algoritmo 8.26: Generación de código a partir de un árbol de expresión etiquetado.ENTRADA: Un árbol etiquetado, en el que cada operando aparece una vez (es decir, no haysubexpresiones comunes) y un número de registros r ¦ 2.Maq. Cap_8_AHO.indd 570 11/10/07 12:59:16 AMSALIDA: Una secuencia óptima de instrucciones de máquina para evaluar la raíz y colocarlaen un registro, usando no más de r registros, que suponemos son R 1, R 2,…, Rr .MÉTODO: Aplique el siguiente algoritmo recursivo, empezando en la raíz del árbol, con labase b = 1. Para un nodo N con la etiqueta r o menor, el algoritmo es exactamente el mismoque el Algoritmo 8.24, por lo que no repetiremos aquí esos pasos. Sin embargo, para los nodosinteriores con una etiqueta k > r, debemos trabajar en cada lado del árbol por separado yalmacenar el resultado del subárbol más grande. Ese resultado se regresa a la memoria justoantes de evaluar el nodo N, y el paso final se llevará a cabo en los registros Rr − 1 y Rr . Lasmodificaciones al algoritmo básico son las siguientes:1. El nodo N tiene por lo menos un hijo con la etiqueta r o mayor. Elija el hijo más grande(o cualquiera si sus etiquetas son iguales) para que sea el hijo “grande” y que el otro hijosea el “pequeño”.2. Genere código en forma recursiva para el hijo grande, usando la base b = 1. El resultadode esta evaluación aparecerá en el registro Rr .3. Genere la instrucción de máquina ST t k, Rr, en donde t k es una variable temporal que seutiliza para los resultados temporales que ayudan a evaluar los nodos con la etiqueta k.4. Genere código para el hijo pequeño de la siguiente forma. Si el hijo pequeño tiene laetiqueta r o mayor, elija la base b = 1. Si la etiqueta del hijo pequeño es j < r, entonceselija b = r − j. Después aplique este algoritmo en forma recursiva al hijo pequeño; elresultado aparece en Rr .5. Genere la instrucción LD Rr − 1, tk .6. Si el hijo grande es el hijo derecho de N, entonces genere la instrucción OP Rr, Rr, Rr − 1. Siel hijo grande es el izquierdo, genere OP Rr, Rr − 1, Rr .✷Ejemplo 8.27: Vamos a repasar la expresión representada por la figura 8.23, pero ahorasuponga que r = 2; es decir, sólo están disponibles los registros R1 y R2 para guardar lostemporales usados en la evaluación de las expresiones. Al aplicar el Algoritmo 8.26 a la figura8.23, podemos ver que la raíz, con la etiqueta 3, tiene una etiqueta más grande que r = 2. Porende, tenemos que identificar a uno de los hijos como el hijo “grande”. Como tienen etiquetasiguales, cualquiera puede serlo. Suponga que elegimos el hijo derecho como el hijo grande.Como la etiqueta del hijo grande de la raíz es 2, hay suficientes registros. Por ende, aplicamos el Algoritmo 8.24 a este subárbol, con b = 1 y dos registros. El resultado es muy parecidoal código que generamos en la figura 8.24, pero con los registros R1 y R2 en vez de R2 y R3.Este código es:LD R2, dLD R1, cADD R2, R1, R2LD R1, eMUL R2, R1, R28.10 Generación de código óptimo para las expresiones 571Maq. Cap_8_AHO.indd 571 11/10/07 12:59:16 AM572 Capítulo 8. Generación de códigoAhora, como necesitamos ambos registros para el hijo izquierdo de la raíz, debemos generar lasiguiente instrucción:ST t3, R2A continuación, se maneja el hijo izquierdo de la raíz. De nuevo, el número de registros es suficiente para este hijo, y el código es:LD R2, bLD R1, aSUB R2, R1, R2Por último, recargamos la variable temporal que contiene el hijo derecho de la raíz con la siguiente instrucción:LD R1, t3y ejecutamos la operación en la raíz del árbol con la siguiente instrucción:ADD R2, R2, R1La secuencia completa de instrucciones se muestra en la figura 8.25. ✷ LD R2, dLD R1, cADD R2, R1, R2LD R1, eMUL R2, R1, R2ST t3, R2LD R2, bLD R1, aSUB R2, R1, R2LD R1, t3ADD R2, R2, R1Figura 8.25: Código óptimo de tres registros para el árbol de la figura 8.23, usando sólo dosregistros8.10.4 Ejercicios para la sección 8.10Ejercicio 8.10.1: Calcule los números de Ershov para las siguientes expresiones:a) a/(b + c) − d ∗ (e + f ).b) a + b ∗ (c ∗ (d + e)).Maq. Cap_8_AHO.indd 572 11/10/07 12:59:17 AMc) (−a + ∗p) ∗ ((b − ∗q)/(−c + ∗r)).Ejercicio 8.10.2: Genere código óptimo, usando dos registros para cada una de las expresiones del ejercicio 8.10.1.Ejercicio 8.10.3: Genere código óptimo, usando tres registros para cada una de las expresiones del ejercicio 8.10.1.Ejercicio 8.10.4: Generalice el cálculo de los números de Ershov para los árboles de expresióncon nodos interiores que tengan tres o más hijos.Ejercicio 8.10.5: Una asignación a un elemento de un arreglo, como a[i] = x, parece ser unoperador con tres operandos: a, i y x. ¿Cómo modificaría el esquema de etiquetado de árbolespara generar código óptimo para este modelo de máquina?Ejercicio 8.10.6: Los números de Ershov originales se utilizaron para una máquina que permitía que el operando derecho de una expresión estuviera en la memoria, en vez de estar en unregistro. ¿Cómo modificaría el esquema de etiquetado de árboles para generar código óptimopara este modelo de máquina?Ejercicio 8.10.7: Algunas máquinas requieren dos registros para ciertos valores de precisiónsimple. Suponga que el resultado de una multiplicación de cantidades de un solo registro requiere dos registros consecutivos, y que cuando dividimos a/b, el valor de a debe guardarseen dos registros consecutivos. ¿Cómo modificaría el esquema de etiquetado de árboles paragenerar código óptimo en este modelo de máquina?8.11 Generación de código de programación dinámicaEl Algoritmo 8.26 en la sección 8.10 produce código óptimo a partir de un árbol de expresión,utilizando una cantidad de tiempo que es una función lineal del tamaño del árbol. Este procedimiento funciona para las máquinas en las que todos los cálculos se realizan en registros, y enlas que las instrucciones consisten en un operador que se aplica a dos registros, o a un registroy a una ubicación de memoria.Podemos usar un algoritmo basado en el principio de la programación dinámica con el finde extender la clase de máquinas para las cuales se puede generar un código óptimo, a partir deárboles de expresión en tiempo lineal. El algoritmo de programación dinámica se aplica a unaamplia clase de máquinas de registro con conjuntos complejos de instrucciones.El algoritmo de programación dinámica se puede utilizar en la generación de código paracualquier máquina con r registros intercambiables R0, R1,…, Rr − 1 e instrucciones de carga,almacenamiento y suma. Por cuestión de simplicidad, suponemos que cada instrucción tieneun costo de una unidad, aunque el algoritmo de programación dinámica puede modificarse confacilidad para trabajar aun cuando cada instrucción tiene su propio costo.8.11 Generación de código de programación dinámica 573!!!!Maq. Cap_8_AHO.indd 573 11/10/07 12:59:17 AM574 Capítulo 8. Generación de código8.11.1 Evaluación contiguaEl algoritmo de programación dinámica particiona el problema de generar código óptimo parauna expresión en los subproblemas de generar código óptimo para las subexpresiones de laexpresión dada. Como un ejemplo simple, considere una expresión E de la forma E 1 + E 2. Unprograma óptimo para E se forma mediante la combinación de programas óptimos para E 1 yE 2, en un orden o en el otro, seguidos de código para evaluar el operador +. Los subproblemasde generar código óptimo para E 1 y E 2 se resuelven de manera similar.Un programa óptimo producido por el algoritmo de programación dinámica tiene unaimportante propiedad. Evalúa una expresión E = E 1 op E 2 en forma “contigua”. Podemosapreciar lo que esto significa si analizamos el árbol sintáctico T para E:Aquí, T 1 y T 2 son árboles para E 1 y E 2, respectivamente.Decimos que un programa P evalúa a un árbol T en forma contigua si primero evalúa lossubárboles de T que deben calcularse en la memoria. Después, evalúa el resto de T, ya sea enel orden T 1, T 2, y después la raíz, o en el orden T 2, T 1, y después la raíz, en cualquier casousando los valores antes calculados de la memoria, siempre que sea necesario. Como ejemplo deevaluación no contigua, P primero podría evaluar parte de T 1, dejando el valor en un registro (en vez de dejarlo en la memoria), después evaluar T 2 y luego regresar a evaluar el restode T 1.Para la máquina de registro en esta sección, podemos demostrar que dado cualquier programa P en lenguaje máquina para evaluar un árbol de expresión T, podemos encontrar un programaequivalente P de forma que:1. P no tenga un mayor costo que P.2. P no utilice más registros que P.3. P evalúe el árbol en forma contigua.Este resultado implica que cada árbol de expresión se puede evaluar de manera óptimamediante un programa contiguo.En contraste, las máquinas con pares de registros impar-par no siempre tienen evaluaciones contiguas óptimas; la arquitectura x86 utiliza pares de registros para la multiplicación yla división. Para tales máquinas, podemos dar ejemplos de árboles de expresión en los que unprograma óptimo en lenguaje máquina primero debe evaluar en un registro una porción delsubárbol izquierdo de la raíz, después una porción del subárbol derecho, sucesivamente otraparte del subárbol izquierdo, después otra parte del subárbol derecho, y así en lo sucesivo. Estetipo de oscilación es innecesaria para una evaluación óptima de cualquier árbol de expresión,si usamos la máquina de esta sección.Maq. Cap_8_AHO.indd 574 11/10/07 12:59:18 AMLa propiedad de evaluación contigua antes definida asegura que, para cualquier árbol deexpresión T, siempre exista un programa óptimo que consista de programas óptimos para lossubárboles de la raíz, seguidos de una instrucción para evaluar la raíz. Esta propiedad nospermite usar un algoritmo de programación dinámica en la generación de un programa óptimopara T.8.11.2 El algoritmo de programación dinámicaEl algoritmo de programación dinámica procede en tres fases (suponga que la máquina destinotiene r registros):1. Calcula, de abajo hacia arriba, para cada nodo n del árbol de expresión T, un arreglo Cde costos, en el cual el i-ésimo componente C[i] es el costo óptimo de calcular el subárbolS con raíz en n y colocarlo en un registro, suponiendo que hay i registros disponibles parael cálculo, para 1 ¥ i ¥ r.2. Recorre T, usando los vectores de costo para determinar cuáles subárboles de T debencalcularse y colocarse en la memoria.3. Recorre cada árbol usando los vectores de costo y las instrucciones asociadas para generar el código de destino final. El código para los árboles que se calculan en las ubicacionesde memoria se genera primero.Cada una de estas fases puede implementarse para ejecutarse en el tiempo, en forma linealmente proporcional al tamaño del árbol de expresión.El costo de calcular un nodo n incluye todas las instrucciones de carga y almacenamientonecesarias para evaluar S en el número dado de registros. También incluye el costo de calcular eloperador en la raíz de S. El componente cero del vector de costos es el costo óptimo de calcularel subárbol S y colocar el resultado en la memoria. La propiedad de evaluación contigua aseguraque pueda generarse un programa óptimo para S, al considerar las combinaciones de los programas óptimos sólo para los subárboles de la raíz de S. Esta restricción reduce el número de casosque hay que considerar.Para poder calcular los costos C[i] en el nodo n, vemos las instrucciones como reglas de rescritura de árboles, como en la sección 8.9. Considere cada plantilla E que coincida con el árbol deentrada en el nodo n. Al examinar los vectores de costo en los descendientes correspondientesde n, determine los costos de evaluar los operandos en las hojas de E. Para aquellos operandos de Eque sean registros, considere todos los órdenes posibles en los que pueden evaluarse los correspondientes subárboles de T y colocarse en registros. En cada ordenamiento, el primer subárbol correspondiente a un operando de registro puede evaluarse mediante el uso de i registros disponibles, elsegundo usando i − 1 registros, y así en lo sucesivo. Para tomar en cuenta el nodo n, agregueel costo de la instrucción asociada con la plantilla E. El valor C[i] es entonces el costo mínimosobre todos los posibles órdenes.Los vectores de costos para todo el árbol T pueden calcularse de abajo hacia arriba en el tiempo, de una forma linealmente proporcional al número de nodos en T. Es conveniente almacenaren cada nodo la instrucción utilizada para lograr el mejor costo para C[i], por cada valor de i. Elmenor costo en el vector para la raíz de T proporciona el mínimo costo de evaluar a T.8.11 Generación de código de programación dinámica 575Maq. Cap_8_AHO.indd 575 11/10/07 12:59:18 AM576 Capítulo 8. Generación de códigoEjemplo 8.28: Considere una máquina que tiene dos registros R0 y R1, y las siguientes instrucciones, cada una de un costo unitario:En estas instrucciones, Ri es R0 o R1, y Mj es una ubicación de memoria. El operador op corresponde a los operadores aritméticos.Vamos a aplicar el algoritmo de programación dinámica para generar código óptimo parael árbol sintáctico de la figura 8.26. En la primera fase, calculamos los vectores de costos quese muestran en cada nodo. Para ilustrar este cálculo de costos, considere el vector de costosen la hoja a. C[0], el costo de calcular a en la memoria, es 0 debido a que ya se encuentra ahí.C[1], el costo de calcular a en un registro es 1, ya que podemos cargarla en un registro con lainstrucción LD R0, a. C[2], el costo de cargar a en un registro con dos registros disponibles es elmismo que con un registro disponible. Por lo tanto, el vector de costos en la hoja a es (0, 1, 1).Figura 8.26: Árbol sintáctico para (a−b)+c*(d/e), con un vector de costos en cada nodoConsidere el vector de costos en la raíz. Primero determinamos el costo mínimo de calcularla raíz con uno y dos registros disponibles. La instrucción de máquina ADD R0, R0, M coincidecon la raíz, ya que está etiquetada con el operador +. Utilizando esta instrucción, el costo mínimo de evaluar la raíz con un registro disponible es el costo mínimo de calcular su subárbolderecho en la memoria, más el costo mínimo de calcular su subárbol izquierdo en el registro,más 1 por la instrucción; no existe otra manera. Los vectores de costos en los hijos derecho eizquierdo de la raíz muestran que el costo mínimo de calcular la raíz con un registro disponiblees 5 + 2 + 1 = 8.Ahora considere el costo mínimo de evaluar la raíz con dos registros disponibles. Surgen trescasos, dependiendo de qué instrucción se utilice para calcular la raíz y en qué orden se evalúenlos subárboles izquierdo y derecho de la raíz.Maq. Cap_8_AHO.indd 576 11/10/07 12:59:19 AM1. Calcule el subárbol izquierdo con dos registros disponibles en el registro R0, calcule elsubárbol derecho con un registro disponible en el registro R1 y utilice la instrucción ADDR0, R0, R1 para calcular la raíz. Esta secuencia tiene un costo de 2 + 5 + 1 = 8.2. Calcule el subárbol derecho con dos registros disponibles en R1, calcule el subárbol izquierdo con un registro disponible en R0, y utilice la instrucción ADD R0, R0, R1. Estasecuencia tiene un costo de 4 + 2 + 1 = 7.3. Calcule el subárbol derecho en la ubicación de memoria M, calcule el subárbol izquierdocon dos registros disponibles en el registro R0, y utilice la instrucción ADD R0, R0, M. Estasecuencia tiene un costo de 5 + 2 + 1 = 8.La segunda elección produce el costo mínimo de 7.El costo mínimo de calcular la raíz en la memoria se determina sumando uno al costomínimo de calcular la raíz con todos los registros disponibles; es decir, calculamos la raíz enun registro y después almacenamos el resultado. Por lo tanto, el vector de costos en la raíz es(8, 8, 7).A partir de los vectores de costos podemos construir con facilidad la secuencia de código,mediante un recorrido del árbol. Del árbol en la figura 8.26, suponiendo que hay dos registrosdisponibles, una secuencia de código óptimo sería:✷Las técnicas de programación dinámica se han utilizado en una variedad de compiladores,incluyendo la segunda versión del compilador C portable, PCC2. La técnica facilita la redestinación, debido a la capacidad de aplicación de la técnica de programación dinámica a unaamplia clase de máquinas.8.11.3 Ejercicios para la sección 8.11Ejercicio 8.11.1: Aumente el esquema de rescritura de árboles en la figura 8.20 con costos, yutilice la programación dinámica y la coincidencia de árboles en la generación de código paralas instrucciones del ejercicio 8.9.1.Ejercicio 8.11.2: ¿Cómo extendería la programación dinámica para realizar la generación decódigo óptimo en GDAs?8.11 Generación de código de programación dinámica 577!!Maq. Cap_8_AHO.indd 577 11/10/07 12:59:20 AM578 Capítulo 8. Generación de código8.12 Resumen del capítulo 8♦ La generación de código es la fase final de un compilador. El generador de código asigna larepresentación intermedia producida por el front-end, o si hay una fase de optimizaciónde código correspondiente al optimizador de código, al programa de destino.♦ La selección de instrucciones es el proceso de elegir instrucciones del lenguaje de destinopara cada instrucción representación intermedia.♦ La repartición de registros es el proceso de decidir qué valores de representación intermedia mantener en los registros. La coloración de grafos es una técnica efectiva para realizarla repartición de registros en los compiladores.♦ La asignación de registros es el proceso de decidir qué registro debe contener un valor derepresentación intermedia dado.♦ Un compilador redestinable es uno que puede generar código para varios conjuntos deinstrucciones.♦ Una máquina virtual es un intérprete para un lenguaje intermedio de bytecodes, producido por lenguajes como Java y C#.♦ Una máquina CISC es, por lo general, una máquina de dos direcciones, con pocos registros, varias clases de registros e instrucciones de longitud variable con modos de direccionamiento complejos.♦ Una máquina RISC es, por lo general, una máquina de tres direcciones con muchos registros, en donde las operaciones se realizan en los registros.♦ Un bloque básico es una secuencia máxima de instrucciones consecutivas de tres direcciones, en donde el flujo de control sólo puede entrar en la primera instrucción del bloquey salir en la última instrucción, sin detenerse o bifurcarse, tal vez excepto en la últimainstrucción del bloque básico.♦ Un grafo de flujo es una representación gráfica de un programa, en el cual los nodos delgráfico son bloques básicos y las flechas del grafo muestran cómo puede fluir el controlentre los bloques.♦ Un ciclo en un grafo de flujo es una región con conexión sólida, con un solo punto deentrada conocido como el encabezado del ciclo.♦ Una representación GDA de un bloque básico es un grafo dirigido acíclico, en el cual losnodos del GDA representan a las instrucciones dentro del bloque y cada hijo de un nodocorresponde a la instrucción, que es la última definición de un operando utilizado en lainstrucción.♦ Las optimizaciones de mirilla (peephole) son transformaciones para mejorar el códigolocal que pueden aplicarse a un programa, por lo general, a través de una ventana deslizable.Maq. Cap_8_AHO.indd 578 11/10/07 12:59:21 AM♦ La selección de instrucciones puede realizarse mediante un proceso de rescritura de árboles, en el cual los patrones tipo árbol que corresponden a las instrucciones de máquinase utilizan para revestir un árbol sintáctico. Podemos asociar los costos con las reglas derescritura de árboles y aplicar la programación dinámica para obtener un revestimientoóptimo para clases útiles de máquinas y expresiones.♦ Un número de Ershov indica cuántos registros se necesitan para evaluar una expresión sinalmacenar valores temporales.♦ El código de derrame es una secuencia de instrucciones que almacena un valor en un registro en la memoria, con el fin de hacer espacio para guardar otro valor en ese registro.8.13 Referencias para el capítulo 8Muchas de las técnicas que se vieron en este capítulo tienen sus orígenes en los primeros compiladores. El algoritmo de etiquetado de Ershov apareció en 1958 [7]. Sethi y Ullman [16] utilizaron este etiquetado en un algoritmo que demostraron que generaba código óptimo para lasexpresiones aritméticas. Aho y Jonson [1] utilizaron la programación dinámica en la generaciónde código óptimo para los árboles de expresiones en máquinas CISC. Hennessy y Patterson [12]tiene una buena exposición acerca de la evolución de las arquitecturas de las máquinas CISC yRISC, y de las concesiones implicadas en el diseño de un buen conjunto de instrucciones.Las arquitecturas RISC se hicieron populares después de 1990, aunque sus orígenes seremontan a las computadoras como la CDC 6600, que se produjo por primera vez en 1964.Muchas de las computadoras diseñadas antes de 1990 eran máquinas CISC, pero la mayoríade las computadoras de propósito general instaladas después de 1990 siguen siendo máquinasCISC, debido a que se basan en la arquitectura Intel 80x86 y sus descendientes, como el Pentium. La computadora Burroughs B5000 producida en 1963 fue una de las primeras máquinasbasadas en pila.Muchas de las heurísticas para la generación de código propuestas en este capítulo se hanutilizado en varios compiladores. Nuestra estrategia de repartir un número fijo de registrospara guardar variables durante la ejecución de un ciclo se utilizó en la implementación de Fortran H, por Lowry y Medlock [13].También se han estudiado las técnicas de asignación y repartición eficiente de registrosdesde la aparición de las primeras computadoras. Cocke, Ershov [8] y Schwartz [15] propusieron la coloración de grafos como técnica de repartición de registros. Muchas variantes de losalgoritmos de la coloración de grafos se han propuesto para la repartición de registros. Nuestrotratamiento de la coloración de gráficos se basa en Chaitin [3] [4]. Chow y Hennessy describensu algoritmo de la coloración basado en prioridades para la repartición de registros en [5]. En[6] podrá ver una explicación de las técnicas más recientes de la división de grafos y reescritura,para la asignación de registros.Los generadores de analizadores léxicos y sintácticos incitaron el desarrollo de la selección deinstrucciones dirigida por patrones. Glanville y Graham [11] utilizaron las técnicas de generaciónde analizadores sintácticos LR para la selección de instrucciones automatizada. Los generadores de código controlados por tablas evolucionaron en una variedad de herramientas de generación de código para la coincidencia de patrones tipo árbol [14]. Aho, Ganapathi y Tjiang [2]8.13 Referencias para el capítulo 8 579Maq. Cap_8_AHO.indd 579 11/10/07 12:59:21 AM580 Capítulo 8. Generación de códigocombinaron técnicas eficientes para la coincidencia de patrones de árboles con la programacióndinámica en la herramienta de generación de código twig. Fraser, Hanson y Proebsting [10]refinaron aún más estas ideas en su generador de generadores de código simple y eficiente. 1. Aho, A. V. y S. C. Johnson, “Optimal code generation for expression trees”, J. ACM23:3, pp. 488-501. 2. Aho, A. V., M. Ganapathi y S. W. K. Tjiang, “Code generation using tree matchingand dynamic programming”, ACM Trans. Programming Languages and Systems 11:4(1989), pp. 491-516. 3. Chaitin, G. J., M. A. Auslander, A. K. Chandra, J. Cocke, M. E. Hopkins y P. W. Markstein, “Register allocation via coloring”, Computer Languages 6:1 (1981), pp. 47-57. 4. Chaitin, G. J., “Register allocation and spilling via graph coloring”, ACM SIGPLANNotices 17:6 (1982), pp. 201-207. 5. Chow, F. y J. L. Hennessy, “The priority-based coloring approach to register allocation”, ACM Trans. Programming Languages and Systems 12:4 (1990), pp. 501-536. 6. Cooper, K. D. y L. Torczon, Engineering a Compiler, Morgan Kaufmann, San FranciscoCA, 2004. 7. Ershov, A. P., “On programming of arithmetic operations”, Comm. ACM 1:8 (1958),pp. 3-6. Además, Comm. ACM 1:9 (1958), p. 16. 8. Ershov, A. P., The Alpha Automatic Programming System, Academic Press, NuevaYork, 1971. 9. Fischer, C. N. y R. J. LeBlanc, Crafting a Compiler with C, Benjamin-Cummings, Redwood City, CA, 1991.10. Fraser, C. W., D. R. Hanson y T. A. Proebsting, “Engineering a simple, efficientcode generator generator”, ACM Letters on Programming Languages and Systems 1:3(1992), pp. 213-226.11. Glanville, R. S. y S. L. Graham, “A new method for compiler code generation”, Conf.Rec. Fifth ACM Symposium on Principles of Programming Languages (1978), pp. 231-240.12. Hennessy, J. L. y D. A. Patterson, Computer Architecture: A Quantitative Approach,Tercera Edición, Morgan Kaufman, San Francisco, 2003.13. Lowry, E. S. y C. W. Medlock, “Object code optimization”, Comm. ACM 12:1 (1969),pp. 13-22.Maq. Cap_8_AHO.indd 580 11/10/07 12:59:22 AM14. Pelegri-Llopart, E. y S. L. Graham, “Optimal code generation for expressions trees: anapplication of BURS theory”, Conf. Rec. Fifteenth Annual ACM Symposium on Principles of Programming Languages (1988), pp. 294-308.15. Schwartz, J. T., On Programming: An Interim Report on the SETL Project, Informetécnico, Courant Institute of Mathematical Sciences, Nueva York, 1973.16. Sethi, R. y J. D. Ullman, “The generation of optimal code for arithmetic expressions”,J. ACM 17:4 (1970), pp. 715-728.8.13 Referencias para el capítulo 8 581Maq. Cap_8_AHO.indd 581 11/10/07 12:59:22 AMMaq. Cap_8_AHO.indd 582 11/10/07 12:59:23 AMLas construcciones de lenguajes de alto nivel pueden introducir una sobrecarga considerable en eltiempo de ejecución si traducimos directamente cada construcción de manera independiente en código máquina. Este capítulo habla de cómo eliminar muchas de estas ineficiencias. A la eliminaciónde instrucciones innecesarias en el código objeto, o la sustitución de una secuencia de instruccionespor una secuencia más rápida de instrucciones que haga lo mismo, se le conoce como “mejora decódigo” u “optimización de código”.La optimización de código local (mejora de código dentro de un bloque básico) se presentóen la sección 8.5. Este capítulo maneja la optimización de código global, en donde las mejorastoman en cuenta lo que ocurre a través de los bloques básicos. En la sección 9.1 empezamos conuna explicación sobre las principales oportunidades para la mejora de código.La mayoría de las optimizaciones globales se basan en el análisis del flujo de datos, queson algoritmos para recopilar información acerca de un programa. Todos los resultados de losanálisis del flujo de datos tienen la misma forma: para cada instrucción en el mismo, especifican cierta propiedad que debe aplicarse cada vez que se ejecute esa instrucción. Los análisisdifieren en las propiedades que calculan. Por ejemplo, un análisis de propagación de constantes calcula, para cada punto en el programa, y para cada variable utilizada por el programa,si esa variable tiene un valor constante único en ese punto. Por ejemplo, esta informaciónpuede utilizarse para sustituir las referencias a las variables por valores constantes. Comootro ejemplo, un análisis del estado de vida determina, para cada punto en el programa, si esseguro que el valor que contiene una variable específica en ese punto se sobrescriba antes deleerlo. De ser así, no necesitamos preservar ese valor, ya sea en un registro o en una ubicaciónen memoria.En la sección 9.2 presentamos el análisis del flujo de datos, incluyendo varios ejemplos importantes del tipo de información que recopilamos en forma global y después utilizamos paramejorar el código. La sección 9.3 muestra la idea general de un marco de trabajo del flujo dedatos, del cual los análisis del flujo de datos en la sección 9.2 son casos especiales. En esenciapodemos usar los mismos algoritmos para todas estas instancias del análisis del flujo de datos,Capítulo 9Optimizaciones independientesde la máquina583Maq. Cap_9_Aok.indd 583 11/10/07 1:01:31 AM584 Capítulo 9. Optimizaciones independientes de la máquinay podemos medir el rendimiento de estos algoritmos, incluso demostrar que sean correctos entodas las instancias. La sección 9.4 es un ejemplo del framework general que realiza un análisismás poderoso que los primeros ejemplos. Después, en la sección 9.5 consideraremos una técnicapoderosa, conocida como “eliminación parcial de redundancia”, para optimizar la colocaciónde cada evaluación de una expresión en el programa. La solución a este problema requiere lasolución de una variedad de distintos problemas del flujo de datos.En la sección 9.6 nos encargaremos del descubrimiento y análisis de los ciclos en los programas. La identificación de ciclos conduce a otra familia de algoritmos para resolver los problemasdel flujo de datos, los cuales se basan en la estructura jerárquica de los ciclos de un programabien formado (“reducible”). Este método para el análisis del flujo de datos se cubre en la sección 9.7. Por último, la sección 9.8 utiliza el análisis jerárquico para eliminar las variables deinducción (en esencia, variables que cuentan el número de iteraciones alrededor de un ciclo).Esta mejora en el código es una de las más importantes que podemos hacer para los programasescritos en lenguajes de programación de uso común.9.1 Las fuentes principales de optimizaciónLa optimización de un compilador debe preservar la semántica del programa original. Exceptoen circunstancias muy especiales, una vez que un programador elige e implementa un algoritmo específico, el compilador no puede comprender lo suficiente acerca del programa comopara sustituirlo con un algoritmo considerablemente diferente y más eficiente. Un compiladorsólo sabe cómo aplicar transformaciones semánticas de un nivel relativamente bajo, mediantehechos generales como identidades algebraicas del tipo i + 0 = i, o semántica de programascomo el hecho de que al realizar la misma operación sobre los mismos valores se produce elmismo resultado.9.1.1 Causas de redundanciaExisten muchas operaciones redundantes en un programa típico. Algunas veces, la redundanciaestá disponible en el nivel de origen. Por ejemplo, un programador puede encontrar que es másdirecto y conveniente volver a calcular cierto resultado, dejando al compilador la opción dereconocer que sólo es necesario un cálculo de ese tipo. Pero con más frecuencia, la redundanciaes un efecto adicional de haber escrito un programa en lenguajes de alto nivel. En la mayoríade los lenguajes (distintos de C o C++, en donde se permite la aritmética de apuntadores), losprogramadores no tienen otra opción de referirse a los elementos de un arreglo o a los camposen un estructura a través de accesos como A[i ][j ] o X → f 1.A medida que se compila un programa, cada uno de estos accesos a estructuras de datos dealto nivel se expande en un número de operaciones aritméticas de bajo nivel, como el cálculode la ubicación del (i, j )-ésimo elemento de una matriz A. A menudo, los accesos a la mismaestructura de datos comparten muchas operaciones comunes de bajo nivel. Los programadoresno están conscientes de estas operaciones y no pueden eliminar las redundancias por sí mismos.De hecho, es preferible desde una perspectiva de ingeniería de software que los programadoressólo accedan a los elementos de datos mediante sus nombres de alto nivel; los programas sonmás fáciles de escribir y, lo que es más importante, más fáciles de comprender y de desarrollar.Maq. Cap_9_Aok.indd 584 11/10/07 1:01:35 AMAl hacer que un compilador elimine las redundancias, obtenemos lo mejor de ambos mundos:los programas son tanto eficientes como fáciles de mantener.9.1.2 Un ejemplo abierto: QuicksortA continuación, vamos a usar el fragmento de un programa para ordenar datos, llamado quicksort, para ilustrar varias transformaciones importantes relacionadas con la mejora de código. Elprograma en C de la figura 9.1 se deriva de Sedgewick,1 quien describió la optimización manualde un programa así. No vamos a hablar sobre todos los aspectos algorítmicos sutiles de este programa aquí y, por ejemplo, el hecho de que a[0] debe contener el más pequeño de los elementosordenados, y a[max] el más grande.void quicksort(int m, int n) /* ordena desde a[m] hasta a[n] en forma recursiva */{ int i, j; int v, x; if (n <= m) return; /* aquı´ empieza el fragmento */ i = m−1; j = n; v = a[n]; while (1) { do i = i+1; while (a[i] < v); do j = j−1; while (a[j] > v); if (i >= j) break; x = a[i]; a[i]=a[j]; a[j] = x; /* intercambia a[i], a[j] */ } x = a[i]; a[i] = a[n]; a[n] = x; /* intercambia a[i], a[n] */ /* aquı´ termina el fragmento */ quicksort(m,j); quicksort(i+1,n);}Figura 9.1: Código en C para el algoritmo Quick SortAntes de poder optimizar las redundancias en los cálculos de direcciones, primero debemosdescomponer las operaciones con direcciones en un programa en operaciones aritméticas debajo nivel, para exponer las redundancias. Durante el resto de este capítulo, vamos a suponerque la representación intermedia consiste en instrucciones de tres direcciones, en donde seutilizan variables temporales para guardar todos los resultados de las expresiones intermedias.El código intermedio para el fragmento marcado del programa de la figura 9.1 se muestra enla figura 9.2.En este ejemplo suponemos que los enteros ocupan cuatro bytes. La asignación x = a[i] setraduce como en la sección 6.4.4 en las siguientes dos instrucciones de tres direcciones:9.1 Las fuentes principales de optimización 5851R. Sedgewick, “Implementing Quicksort Programs”, Comm. ACM, 21, 1978, pp. 847-857.Maq. Cap_9_Aok.indd 585 11/10/07 1:01:36 AM586 Capítulo 9. Optimizaciones independientes de la máquinaFigura 9.2: Código de tres direcciones para el fragmento de código de la figura 9.1t6 = 4*ix = a[t6]como se muestra en los pasos (14) y (15) de la figura 9.2. De manera similar, a[j] = x se convierte en:t10 = 4*ja[t10] = xen los pasos (20) y (21). Tenga en cuenta que todo acceso a un arreglo en el programa original se traduce en un par de pasos, que consisten en una multiplicación y una operación consubíndices de arreglos. Como resultado, este fragmento de programa corto se traduce en una secuencia bastante extensa de operaciones de tres direcciones.La figura 9.3 es el grafo de flujo para el programa de la figura 9.2. El bloque B1 es el nodode entrada. Todos los saltos condicionales e incondicionales a las instrucciones en la figura 9.2se han sustituido en la figura 9.3 por saltos hacia el bloque del cual las instrucciones son líderes,como en la sección 8.4. En la figura 9.3, hay tres ciclos. Los bloques B 2 y B 3 son bloques porsí mismos. Los bloques B 2, B 3, B 4 y B 5 forman en conjunto un ciclo, en donde B 2 es el únicopunto de entrada.9.1.3 Transformaciones que preservan la semánticaHay varias formas en las que un compilador puede mejorar un programa, sin cambiar la funciónque calcula. La eliminación de subexpresiones comunes, la propagación de copia, la eliminación de código muerto y el cálculo previo de constantes son ejemplos comunes de dichas transformaciones que preservan las funciones (o que preservan la semántica); consideraremos cadauno de estos ejemplos, uno a la vez.Maq. Cap_9_Aok.indd 586 11/10/07 1:01:36 AMB 6B 2B 3B 1B 4t6 = 4*ix = a[t6]t7 = 4*it8 = 4*jt9 = a[t8]a[t7] = t9t10 = 4*ja[t10] = xgotoB 5B t11 = 4*i 6x = a[t11]t12 = 4*it13 = 4*nt14 = a[t13]a[t12] = t14t15 = 4*na[t15] = xB 2B 3B 2i = mï1j = nt1 = 4*nv = a[t1]i = i+1t2 = 4*it3 = a[t2]if t3<v gotoj = jï1t4 = 4*jt5 = a[t4]if t5>v gotoif i>=j gotoFigura 9.3: Grafo de flujo para el fragmento quicksort9.1 Las fuentes principales de optimización 587Maq. Cap_9_Aok.indd 587 11/10/07 1:01:38 AM588 Capítulo 9. Optimizaciones independientes de la máquinaEs común que un programa incluya varios cálculos del mismo valor, como un desplazamiento en un arreglo. Como dijimos en la sección 9.1.2, el programador no puede evitar algunos deestos cálculos duplicados, ya que se encuentran por debajo del nivel de detalle accesible dentrodel lenguaje fuente. Por ejemplo, el bloque B 5 que se muestra la figura 9.4(a) vuelve a calcular 4 ∗ i y 4 ∗ j, aunque el programador no solicitará de manera explícita ninguno de estoscálculos.B 5B 2t6 = 4*ix = a[t6]t7 = 4*it8 = 4*jt9 = a[t8]a[t7] = t9t10 = 4*ja[t10] = xgotot6 = 4*ix = a[t6]t8 = 4*jt9 = a[t8]a[t6] = t9a[t8] = xgotoBB52(a) Antes. (b) Después.Figura 9.4: Eliminación de subexpresiones comunes locales9.1.4 Subexpresiones comunes globalesA una ocurrencia de una expresión E se le conoce como subexpresión común si E se calculópreviamente y los valores de las variables en E no han cambiado desde ese cálculo previo. Evitamos volver a calcular E si podemos usar su valor calculado previamente; es decir, la variablex a la cual se asignó el cálculo anterior de E no ha cambiado en ese lapso.2Ejemplo 9.1: Las asignaciones a t7 y t10 en la figura 9.4(a) calculan las subexpresiones comunes 4 ∗ i y 4 ∗ j, respectivamente. Estos pasos se han eliminado en la figura 9.4(b), la cualutiliza a t6 en vez de t7, y t8 en vez de t10. ✷Ejemplo 9.2: La figura 9.5 muestra el resultado de la eliminación de subexpresiones comunestanto globales como locales de los bloques B 5 y B 6 en el grafo de flujo de la figura 9.3. Primerohablaremos sobre la transformación de B 5 y después mencionaremos algunas sutilezas relacionadas con los arreglos.Después de eliminar las subexpresiones comunes locales, B 5 sigue evaluando 4 ∗ i y 4 ∗ j,como se muestra la figura 9.4(b). Ambas son subexpresiones comunes; en especial, las tresinstrucciones2Si x ha cambiado, aún puede ser posible reutilizar el cálculo de E si asignamos su valor a una nueva variable y,así como a x, y utilizamos el valor de y en lugar de volver a calcular a E.Maq. Cap_9_Aok.indd 588 11/10/07 1:01:39 AMB 6B 2B 3B 1B 4B B 6 5B 2B 3B 2i = mï1j = nt1 = 4*nv = a[t1]i = i+1t2 = 4*it3 = a[t2]if t3<v gotoj = jï1t4 = 4*jt5 = a[t4]if t5>v gotoif i>=j gotox = t3a[t2] = t5x = t3t14 = a[t1]a[t2] = t14goto a[t1] = xa[t4] = xFigura 9.5: B 5 y B 6 después de la eliminación de subexpresiones comunest8 = 4*jt9 = a[t8]a[t8] = xen B 5 pueden sustituirse port9 = a[t4]a[t4] = xutilizando el valor de t 4 que se calculó en el bloque B 3. En la figura 9.5, observe que a medidaque el control pasa de la evaluación de 4 ∗ j en B 3 a B 5, no se modifican j ni t 4, por lo que sepuede usar t 4 si se necesita 4 ∗ j.Otra subexpresión común surge en B 5, cuando t 4 sustituye a t 8. La nueva expresión a[t 4]corresponde al valor de a[j ] en el nivel de origen. No sólo j retiene su valor cuando el control salede B 3 y después entra a B 5, sino que a[j ], un valor que se calcula en una variable temporal t 5,9.1 Las fuentes principales de optimización 589Maq. Cap_9_Aok.indd 589 11/10/07 1:01:40 AM590 Capítulo 9. Optimizaciones independientes de la máquinatambién retiene su valor, ya que no hay asignaciones a los elementos del arreglo a en ese lapso.Por lo tanto, las instruccionest9 = a[t4]a[t6] = t9en B 5 pueden sustituirse pora[t6] = t5De manera análoga, el valor asignado a x en el bloque B 5 de la figura 9.4(b) se ve comoel mismo valor asignado a t 3 en el bloque B 2. El bloque B 5 en la figura 9.5 es el resultadode eliminar subexpresiones comunes que corresponden a los valores de las expresiones del nivel deorigen a[i ] y a[j ] de B 5 en la figura 9.4(b). En la figura 9.5 se ha realizado una serie similarde transformaciones a B 6.La expresión a[t 1] en los bloques B1 y B 6 de la figura 9.5 no se consideran una subexpresióncomún, aunque t 1 puede usarse en ambos lugares. Una vez que el control sale de B1 y antes deque llegue a B6, puede pasar a través de B 5, en donde hay asignaciones para a. Por ende, a[t 1]tal vez no tenga el mismo valor al llegar a B 6 que tenía al salir de B1, y no es seguro tratar aa[t 1] como una subexpresión común. ✷9.1.5 Propagación de copiasEl bloque B 5 en la figura 9.5 puede mejorarse aún más mediante la eliminación de x, mediantedos nuevas transformaciones. Una se relaciona con las asignaciones de la forma u = v, conocidascomo instrucciones de copia, o simplemente copias. Si hubiéramos entrado en más detalles enel ejemplo 9.2, las copias hubieran surgido mucho antes, debido a que el algoritmo normal paraeliminar subexpresiones comunes las introduce, al igual que varios otros algoritmos.a = d+e b = d+ec = d+et = d+ea = tt = d+eb = tc = t(a) (b)Figura 9.6: Copias introducidas durante la eliminación de subexpresiones comunesEjemplo 9.3: Para poder eliminar la subexpresión común de la instrucción c = d+e en lafigura 9.6(a), debemos utilizar una nueva variable t para guardar el valor de d + e. En la figura9.6(b) se asigna a c el valor de la variable t, en vez del valor de la expresión d + e. Como elcontrol puede llegar a c = d+e ya sea después de la asignación a a, o después de la asignacióna b, sería incorrecto sustituir c = d+e por c = a o por c = b. ✷Maq. Cap_9_Aok.indd 590 11/10/07 1:01:41 AMLa idea de la transformación por propagación de copias es utilizar v para u, siempre quesea posible después de la instrucción de copia u = v. Por ejemplo, la asignación x = t3 en elbloque B 5 de la figura 9.5 es una copia. La propagación de copia que se aplica a B 5 produceel código de la figura 9.7. Este cambio tal vez no parezca ser una mejora pero, como veremos enla sección 9.1.6, nos da la oportunidad de eliminar la asignación a x. x = t3a[t2] = t5a[t4] = t3goto B 2Figura 9.7: Bloque básico B 5 después de la propagación de copias9.1.6 Eliminación de código muertoUna variable está viva en un punto en el programa, si su valor puede utilizarse más adelante; encaso contrario, está muerta en ese punto. Una idea relacionada es el código muerto (o inútil):instrucciones que calculan valores que nunca se utilizan. Aunque es poco probable que el programador introduzca código muerto de manera intencional, puede aparecer como resultado delas transformaciones anteriores.Ejemplo 9.4: Suponga que debug se establece a TRUE o FALSE en varios puntos en el programa, y que se utiliza en instrucciones como:if (debug) print ...Puede ser posible para el compilador deducir que cada vez que el programa llega a esta instrucción, el valor de debug es FALSE. Por lo general, se debe a que hay una instrucción enparticular:debug = FALSEque debe ser la última asignación a debug, antes de cualquier evaluación del valor de debug, sinimportar que secuencia de bifurcaciones tome realmente el programa. Si la propagación de copiasustituye a debug por FALSE, entonces la instrucción de impresión está muerta, ya que no sepuede llegar a ella. Podemos eliminar del código objeto tanto la operación de evaluación comola de impresión. En forma más general, al proceso de deducir en tiempo de compilación que elvalor de una expresión es una constante, y utilizar mejor esa constante se le conoce como cálculoprevio de constantes. ✷Una ventaja de la propagación de copias es que a menudo convierte la instrucción de copiaen código muerto. Por ejemplo, la propagación de copias seguida de la eliminación de códigomuerto elimina la asignación a x y transforma el código de la figura 9.7 en:9.1 Las fuentes principales de optimización 591Maq. Cap_9_Aok.indd 591 11/10/07 1:01:42 AM592 Capítulo 9. Optimizaciones independientes de la máquina a[t2] = t5a[t4] = t3goto B 2Este código es una mejora más al bloque B 5 de la figura 9.5.9.1.7 Movimiento de códigoLos ciclos son un lugar muy importante para las optimizaciones, en especial los ciclos internosen donde los programas tienden a invertir la mayor parte su tiempo. El tiempo de ejecuciónde un programa puede mejorarse si reducimos el número de instrucciones en un ciclo interno,incluso si incrementamos la cantidad de código fuera de ese ciclo.Una modificación importante que reduce la cantidad de código en un ciclo es el movimientode código. Esta transformación toma una expresión que produce el mismo resultado sin importar el número de veces que se ejecute un ciclo (el cálculo de una invariante de ciclo) y evalúala expresión antes del ciclo. Observe que la noción “antes del ciclo” asume la existencia de unaentrada para el ciclo, es decir, un bloque básico al cual se dirigen todos los saltos desde el exterior del ciclo (de la sección 8.4.5).Ejemplo 9.5: La evaluación de limite − 2 es un cálculo de una invariante de ciclo en la siguiente instrucción while:while (i <= limite−2) /* instruccio´n que no cambia el lı´mite */El movimiento de código producirá el siguiente código equivalente: t = limite−2while (i <= t) /* instruccio´n que no cambia lı´mite ni t */Ahora, el cálculo de limite − 2 se realizó una vez, antes de entrar al ciclo. Anteriormente, habría n + 1 cálculos de limite − 2 si iteráramos el cuerpo del ciclo n veces. ✷9.1.8 Variables de inducción y reducción en fuerzaOtra optimización importante es la de buscar variables de inducción en ciclos y optimizar sucálculo. Se dice que una variable x es una “variable de inducción” si hay una constante c positiva o negativa tal que cada vez que se asigne x, su valor aumente en base a c. Por ejemplo,i y t 2 son las variables de inducción en el ciclo que contiene a B 2 de la figura 9.5. Las variablesde inducción pueden calcularse con un solo incremento (suma o resta) por cada iteración de unciclo. La trasformación de sustituir una operación costosa, como la multiplicación, por una máseconómica, como la suma, se conoce como reducción de fuerza. Pero las variables de inducciónno sólo nos permiten algunas veces realizar una reducción de fuerza; a menudo es posible eliminar todos excepto un grupo de variables de inducción, cuyos valores permanezcan en un pasobloqueado a medida que recorremos el ciclo.Maq. Cap_9_Aok.indd 592 11/10/07 1:01:43 AMB 6B 2B 3B 4B B 6 5B 1 i = mï1j = nt1 = 4*nv = a[t1]B 2B 3B 2i = i+1t2 = 4*it3 = a[t2]if t3<v gotoj = jï1t5 = a[t4]if t5>v gotoif i>=j gotox = t3a[t2] = t5x = t3t14 = a[t1]a[t2] = t14a[t1] = xt4 = t4ï4t4 = 4*jgotoa[t4] = xFigura 9.8: Reducción de fuerza aplicada a 4 ∗ j en el bloque B 3Al procesar los ciclos, es útil trabajar de “adentro hacia fuera”; es decir, empezamos conlos ciclos internos y pasamos a los ciclos circundantes, que cada vez se hacen más grandes. Porende, vamos a ver cómo se aplica esta optimización a nuestro ejemplo de ordenamiento rápido(quicksort), empezando con uno de los ciclos más internos: B 3 por sí solo. Observe que los valores de j y t 4 permanecen en paso bloqueado; cada vez que el valor de j se decrementa en 1, elvalor de t 4 se decrementa en 4, ya que 4 ∗ j se asigna a t 4. Estas variables, j y t 4, forman, porlo tanto, un buen ejemplo de un par de variables de inducción.Cuando hay dos o más variables de inducción en un ciclo, puede ser posible deshacerse detodas menos de una. Para el ciclo interno de B 3 en la figura 9.5, no podemos deshacernos de j o t 4por completo; t 4 se utiliza en B 3 y j se utiliza en B 4. Sin embargo, podemos ilustrar la reducciónen fuerza y una parte del proceso de eliminación de variables de inducción. En un momento dado,j se eliminará cuando lleguemos al ciclo externo que consiste en los bloques B 2, B 3, B 4 y B 5.9.1 Las fuentes principales de optimización 593Maq. Cap_9_Aok.indd 593 11/10/07 1:01:43 AM594 Capítulo 9. Optimizaciones independientes de la máquinaEjemplo 9.6: Como es seguro que la relación t 4 = 4 ∗ j sea válida después de la asignacióna t 4 en la figura 9.5, y que t 4 no se modifica en ningún otro lado del ciclo interno alrededorde B 3, resulta ser que justo después de la instrucción j = j−1, la relación t 4 = 4 ∗ j + 4 debeser válida. Por lo tanto, podemos sustituir la asignación t4 = 4*j por t4 = t4−4. El únicoproblema es que t 4 no tiene un valor cuando entramos al bloque B 3 por primera vez.Como debemos mantener la relación t 4 = 4 ∗ j al entrar al bloque B 3, colocamos una inicialización de t 4 al final del bloque en donde se inicializa la misma j, lo cual se muestra mediantela parte adicional en líneas punteadas en el bloque B1 de la figura 9.8. Aunque hemos agregadouna instrucción más, la cual se ejecuta una vez en el bloque B1, la sustitución de una multiplicación por una resta aumentará la velocidad del código objeto, si la multiplicación ocupa mástiempo que la suma o una resta, como es el caso en muchas máquinas. ✷B 2B B 6 5B 1B 3t14 = a[t1]a[t2] = t14B 2 a[t1] = t3B 4 B 6B 2B 3t4 = 4*jt2 = 4*iv = a[t1]t1 = 4*nj = ni = mï1t2 = t2+4t3 = a[t2]a[t7] = t5a[t10] = t3gotoif t2>t4 gotot5 = a[t4]t4 = t4ï4if t5>v gotoif t3<v gotoFigura 9.9: Grafo de flujo después de eliminar las variables de inducciónConcluimos esta sección con una instancia más de eliminación de variables de inducción. Esteejemplo trata a i y a j en el contexto del ciclo externo que contiene a B 2, B 3, B 4 y B 5.Maq. Cap_9_Aok.indd 594 11/10/07 1:01:44 AMEjemplo 9.7: Una vez que se aplica la reducción en fuerza a los ciclos internos alrededor deB 2 y B 3, el único uso de i y j es determinar el resultado de la evaluación en el bloque B 4. Sabemos que los valores de i y t 2 satisfacen la relación t 2 = 4 ∗ i, mientras que los valores de j y t 4satisfacen la relación t 4 = 4 ∗ j. Por ende, la prueba t 2 ¦ t 4 puede sustituir a i ¦ j. Una vezque se realiza esta sustitución, las variables i en el bloque B 2 y j en el bloque B 3 se conviertenen variables muertas, y las asignaciones a ellas en estos bloques se convierten en código muertoque puede eliminarse. El grafo de flujo resultante se muestra en la figura 9.9. ✷(1)(2)(3)(4)(5)(6)(7)(8)(9)(10)(11)a = 1b = 2c = a+bd = cïad = b+dd = a+be = e+1b = a+be = cïaa = b*db = aïdBBBBBB125634ENTRADASALIDAFigura 9.10: Grafo de flujo para el ejercicio 9.1.1Las transformaciones para mejorar el código que hemos visto han sido efectivas. En la figura9.9, los números de las instrucciones en los bloques B 2 y B 3 se han reducido de 4 a 3, en comparacióncon el grafo de flujo original la figura 9.3. En B 5, el número se ha reducido de 9 a 3, y en B 6 de 8 a 3.9.1 Las fuentes principales de optimización 595Maq. Cap_9_Aok.indd 595 11/10/07 1:01:45 AM596 Capítulo 9. Optimizaciones independientes de la máquinaEs cierto que B1 ha aumentado de cuatro instrucciones a seis, pero B1 se ejecuta sólo una vez enel fragmento, por lo que el tiempo total de ejecución casi no se ve afectado por el tamaño de B1.9.1.9 Ejercicios para la sección 9.1Ejercicio 9.1.1: Para el grafo de flujo de la figura 9.10:a) Identifique los ciclos del grafo de flujo.b) Las instrucciones (1) y (2) en B1 son instrucciones de copia, en las cuales a y b sonvalores constantes dados. ¿Para qué usos de a y b podemos realizar la propagación decopias y sustituir estos usos de variables por usos de constantes? Haga esto cada vez quesea posible.c) Identifique cualquier subexpresión común global para cada ciclo.d) Identifique cualquier variable de inducción para cada ciclo. Asegúrese de tomar en cuentatodas las constantes introducidas en (b).e) Identifique cualquier cálculo de una invariante de ciclo para cada uno de ellos.Ejercicio 9.1.2: Aplique las transformaciones de esta sección al grafo de flujo de la figura8.9.Ejercicio 9.1.3: Aplique las transformaciones de esta sección a sus grafos de flujo del (a) ejercicio 8.4.1; (b) ejercicio 8.4.2.Ejercicio 9.1.4: En la figura 9.11 hay código intermedio para calcular el producto punto dedos vectores A y B. Optimice este código, eliminando las subexpresiones comunes, realizando lareducción en fuerza sobre las variables de inducción, y eliminando todas las variables de inducción que pueda.Figura 9.11: Código intermedio para calcular el producto puntoMaq. Cap_9_Aok.indd 596 11/10/07 1:01:46 AM9.2 Introducción al análisis del flujo de datosTodas las optimizaciones que se presentan en la sección 9.1 dependen del análisis del flujo dedatos. El “análisis del flujo de datos” se refiere a un cuerpo de técnicas que derivan informaciónacerca del flujo de datos a lo largo de los caminos de ejecución de los programas. Por ejemplo,una manera de implementar la eliminación de subexpresiones comunes globales requiere quedeterminemos si dos expresiones textualmente idénticas se evalúan con el mismo valor, a lolargo de cualquier camino de ejecución posible del programa. Como otro ejemplo, si el resultadode una asignación no se utiliza a lo largo de cualquier camino de ejecución subsiguiente, entonces podemos eliminar esa asignación como si fuera código muerto. El análisis del flujo de datospuede responder a ésta y muchas otras preguntas importantes.9.2.1 La abstracción del flujo de datosDe acuerdo con la sección 1.6.2, la ejecución de un programa puede verse como una serie detransformaciones del estado del programa, qué consiste en los valores de todas las variables enel programa, incluyendo aquellas que están asociadas con los marcos de pila debajo del topede la pila en tiempo de ejecución. Cada ejecución de una instrucción de código intermediotransforma un estado de entrada en un nuevo estado de salida. El estado de entrada se asociacon el punto del programa antes de la instrucción y el de salida se asocia con el punto delprograma después de la instrucción.Al analizar el comportamiento de un programa, debemos considerar todas las posiblessecuencias de punto del programa (“caminos”) a través de un grafo de flujo que la ejecucióndel programa puede tomar. Después extraemos, de los posibles estados del programa en cadapunto, la información que necesitamos para el problema específico de análisis del flujo de datosque deseamos resolver. En análisis más complejos, debemos considerar caminos que saltanentre los grafos de flujo para varios procedimientos, a medida que se ejecutan las llamadas ylos retornos. Sin embargo, para comenzar nuestro estudio, vamos a concentrados en los caminosa través de un solo grafo de flujo para un solo procedimiento.Vamos a ver lo que nos indica el grafo de flujo acerca de los posibles caminos de ejecución.• Dentro de un bloque básico, el punto del programa después de la instrucción es el mismoque el punto del programa antes de la siguiente instrucción.• Si hay una flecha del bloque B1 al bloque B 2, entonces el punto del programa despuésde la última instrucción de B1 puede ir seguido inmediatamente del punto del programaantes de la primera instrucción de B 2.Por ende, podemos definir un camino de ejecución (o sólo camino) del punto p 1 al punto p ncomo una secuencia de puntos p 1, p 2,…, p n tal que para cada i = 1, 2,…, n − 1, se cumplauno de los siguientes puntos:1. p i es el punto que va justo antes de una instrucción y p i +1 es el punto que va justo después de esa misma instrucción.2. p i es el final de un bloque y p i 1 es el inicio de un bloque sucesor.9.2 Introducción al análisis del fl ujo de datos 597Maq. Cap_9_Aok.indd 597 11/10/07 1:01:47 AM598 Capítulo 9. Optimizaciones independientes de la máquinaEn general, hay un número infinito de caminos posibles de ejecución a través de un programa, y no hay un límite superior finito en cuanto a la longitud de un camino de ejecución.Los análisis de los programas sintetizan todos los estados posibles de un programa que puedenocurrir en un punto en el programa con un conjunto finito de hechos. Los distintos análisispueden elegir abstraer información distinta y, en general, ningún análisis es necesariamente unarepresentación perfecta del estado.Ejemplo 9.8: Incluso hasta el programa simple de la figura 9.12 describe un número ilimitadode caminos de ejecución. Sin entrar al ciclo en absoluto, el camino de ejecución completo máscorto consiste en los puntos (1, 2, 3, 4, 9) del programa. El siguiente camino más corto ejecutó una iteración del ciclo y consiste en los puntos (1, 2, 3, 4, 5, 6, 7, 8, 3, 4, 9). Por ejemplo,sabemos que la primera vez que se ejecuta el punto (5) del programa, el valor de a se debe ala definición d 1. Decimos que d 1 llega al punto (5) en la primera iteración. En las iteracionessiguientes, d 3 llega al punto (5) y el valor de a es 243.d a = 1 1 :B 4B 3B 2B 1B 4B 2(1)(2)(3)(4)(9)(5)(8)(7)(6) b = aa = 243gotodd23::if read()<=0 gotoFigura 9.12: Programa de ejemplo que ilustra la abstracción del flujo de datosEn general, no es posible llevar el registro de todos los estados del programa para todas los caminos posibles. En el análisis del flujo de datos, no diferenciamos entre los caminos que se tomanpara llegar a un punto del programa. Tampoco llevamos el registro de los estados completos; en vez de ello, abstraemos ciertos detalles, y mantenemos sólo los datos que necesitamos parael propósito del análisis. Dos ejemplos ilustrarán cómo los estados del mismo programa puedenllevar a que se abstraiga información distinta en un punto.1. Para ayudar a los usuarios a depurar sus programas, tal vez sea conveniente averiguarcuáles son todos los valores que una variable puede tener en un punto del programa, endonde pueden definirse esos valores. Por ejemplo, podemos sintetizar todos los estados delprograma en el punto (5) al decir que el valor de a es uno de {1, 243}, y que puede definirsemediante uno de {d 1, d 3}. Las definiciones que pueden llegar a un punto del programa alo largo de cierto camino se conocen como definiciones de alcance.Maq. Cap_9_Aok.indd 598 11/10/07 1:01:48 AM2. Suponga, por el contrario, que estamos interesados en implementar el cálculo previo deconstantes. Si podemos llegar a un uso de la variable x sólo mediante una definición,y esa definición asigna una constante a x, entonces podemos sólo sustituir x por esaconstante. Si, por otra parte, varias definiciones de x pueden llegar a un solo punto delprograma, entonces no podemos realizar el cálculo previo de constantes sobre x. En consecuencia, para el cálculo previo de constantes queremos encontrar las definiciones quesean la única definición de su variable para llegar a un punto dado del programa, sinimportar qué camino de ejecución se tome. Para el punto (5) de la figura 9.12, no hayuna definición que deba ser la definición de a en ese punto, por lo que este conjunto estávacío para a en el punto (5). Incluso si una variable tiene una definición única en unpunto, esa definición debe asignar una constante a la variable. Por ende, sólo podemosdescribir ciertas variables como “no constantes”, en vez de recolectar todos sus valoreso definiciones posibles.Así, podemos ver que la misma información puede resumirse en forma distinta, dependiendo delpropósito del análisis. ✷9.2.2 El esquema del análisis del flujo de datosEn cada aplicación del análisis del flujo de datos, asociamos con cada punto del programa unvalor del flujo de datos que representa una abstracción del conjunto de todos los posibles estados del programa que pueden observarse para ese punto. El conjunto de posibles valores delflujo de datos es el dominio para esta aplicación. Por ejemplo, el dominio de valores del flujo dedatos para llegar a las definiciones es el conjunto de todos los subconjuntos de definiciones enel programa. Un valor de flujo de datos específico es un conjunto de definiciones, y deseamosasociar con cada punto del programa el conjunto exacto de definiciones que pueden llegar a esepunto. Como dijimos antes, la elección de la abstracción depende del objetivo del análisis; paraque sea eficiente, sólo llevamos el registro de la información relevante.Denotamos los valores del flujo de datos antes y después de cada instrucción s medianteENT[s ] y SAL[s ], respectivamente. El problema del flujo de datos es encontrar una solución paraun conjunto de restricciones sobre los valores ENT[s ] y SAL[s ], para todas las instrucciones s.Existen dos conjuntos de restricciones: las que están basadas en la semántica de las instrucciones (“funciones de transferencia”) y las que están basadas en el flujo de control.Funciones de transferenciaLos valores del flujo de datos antes y después de una instrucción se restringen mediante la semántica de la instrucción. Por ejemplo, suponga que nuestro análisis del flujo de datos implicael determinar el valor constante de las variables en los puntos. Si la variable a tiene el valor vantes de ejecutar la instrucción b = a, entonces tanto a como b tendrán el valor v después de lainstrucción. Esta relación entre los valores del flujo de datos antes y después de la instrucciónde asignación se conoce como una función de transferencia.Las funciones de transferencia son de dos tipos: la información puede prepararse haciadelante, a lo largo de los caminos de ejecución, o puede fluir hacia atrás, hacia arriba de loscaminos de ejecución. En un problema de flujo hacia delante, la función de transferencia de una9.2 Introducción al análisis del fl ujo de datos 599Maq. Cap_9_Aok.indd 599 11/10/07 1:01:49 AM600 Capítulo 9. Optimizaciones independientes de la máquinainstrucción s que, por lo general, denotamos como f s, recibe el valor del flujo de datos antes dela instrucción y produce un nuevo valor del flujo de datos después de la instrucción. Es decir,SAL[s ] = f s (ENT[s ]).Por el contrario, en un problema de flujo hacia atrás, la función de transferencia f s para la instrucción s convierte un valor de flujo de datos después de la instrucción a un nuevo valor delflujo de datos antes de la instrucción. Es decir,ENT[s ] = f s (SAL[s ]).Restricciones del flujo de controlEl segundo conjunto de restricciones en los valores del flujo de datos se deriva del flujo decontrol. Dentro de un bloque básico, el flujo de control es simple. Si un bloque B consiste enlas instrucciones s1, s2,…, s n en ese orden, entonces el valor del flujo de control fuera de s i esel mismo que el valor de flujo de control que entra a s i +1. Es decir,ENT[s i +1] = SAL[s i ] para todas las i = 1, 2,..., n − 1.Sin embargo, las aristas del flujo de control entre los bloques básicos crean restricciones máscomplejas entre la última instrucción de un bloque básico y la primera instrucción del siguientebloque. Por ejemplo, si nos interesa recolectar todas las definiciones que pueden llegar a un puntodel programa, entonces el conjunto de definiciones que llega a la instrucción líder de un bloquebásico es la unión de las definiciones después de las últimas instrucciones de cada uno de los bloques predecesores. La siguiente sección proporciona los detalles de cómo fluyen los datos entrelos bloques.9.2.3 Esquemas del flujo de datos en bloques básicosMientras que, técnicamente, un esquema de flujo de datos involucra a los valores del flujo dedatos en cada punto del programa, podemos ahorrar tiempo y espacio al reconocer que lo queocurre dentro de un bloque es, por lo general, muy simple. El control fluye desde el inicio hastael final del bloque, sin interrupción ni bifurcación. Por ende, podemos reformular el esquema entérminos de los valores del flujo de datos que entran y salen de los bloques. Denotamos los valores del flujo de datos justo antes y justo después de cada bloque básico B mediante ENT[B ] ySAL[B ], respectivamente. Las restricciones que involucran a ENT[B ] y SAL[B ] pueden derivarsede aquellas que involucran a ENT[s ] y SAL[s ] para las diversas instrucciones s en B, como semuestra a continuación.Suponga que el bloque B consiste en las instrucciones s 1,…, s n, en ese orden. Si s 1 es laprimera instrucción del bloque básico B, entonces ENT[B ] = ENT[s 1]. De manera similar, si s nes la última instrucción del bloque básico B, entonces SAL[B ] = SAL[s n]. La función de transferencia de un bloque básico B, que denotamos como fB, puede derivarse mediante la composiciónde las funciones de transferencia de las instrucciones del bloque. Es decir, dejamos que fs i sea lafunción de transferencia de la instrucción s i. Entonces fB = fsn ◦ … ◦ fs 2 ◦ fs 1. La relación entreel inicio y el final del bloque es:SAL[B ] = fB (ENT[B ]).Maq. Cap_9_Aok.indd 600 11/10/07 1:01:49 AMLas restricciones debidas al flujo de control entre los bloques básicos pueden rescribirse confacilidad mediante la sustitución de ENT[B ] y SAL[B ] por ENT[s 1] y SAL[s n], respectivamente.Por ejemplo, si los valores del flujo de datos son información acerca de los conjuntos de constantes que pueden asignarse una variable, entonces tenemos un problema de flujo hacia delante,en el cual:ENT[B ] = ∪P un predecesor de B SAL[P ].Cuando el flujo de datos es hacia atrás, como pronto veremos en el análisis de variablesvivas, las ecuaciones son similares, pero con los papeles de los valores ENT y SAL invertidos.Es decir,ENT[B ] = fB(SAL[B ])SAL[B ] = ∪S un sucesor de B ENT[S ].A diferencia de las ecuaciones aritméticas lineales, las ecuaciones del flujo de datos, por logeneral, no tienen una solución única. Nuestro objetivo es encontrar la solución más “precisa”que satisfaga los dos conjuntos de restricciones: del flujo de control y de transferencia. Es decir, necesitamos una solución que respalde las mejoras de código válido, pero que no justifiquelas transformaciones inseguras: aquellas que cambian lo que el programa calcula. En brevehablaremos sobre esta cuestión, en el recuadro titulado “Estimación conservadora” y con másdetalles en la sección 9.3.4. En las siguientes subsecciones, hablaremos sobre algunos de losejemplos más importantes de los problemas que pueden resolverse mediante el análisis del flujode datos.9.2.4 Definiciones de alcance“Las definiciones de alcance” es uno de los esquemas del flujo de datos más útiles y comunes.Al conocer en qué parte de un programa puede haberse definido cada variable x cuando elcontrol llega al punto p, podemos determinar muchas cosas acerca de x. Para sólo dos ejemplos,un compilador sabe entonces si x es una constante en el punto p, y un depurador puede sabersi es posible que x sea una variable indefinida, en caso de que x se utilice en p.Decimos que una definición d llega a un punto p si hay un camino desde el punto que vajusto después de d hacia p, de tal forma que d no se “elimine” a lo largo de ese camino. Eliminamos una definición de una variable x si hay cualquier otra definición de x en cualquier parte a lolargo del camino.3 De manera intuitiva, si una definición d de cierta variable x llega al punto p,entonces d podría ser el lugar en el que se definió por última vez el valor de x utilizado en p.Una definición de una variable x es una instrucción que asigna, o puede asignar, un valor a x.Los parámetros de los procedimientos, los accesos a los arreglos y las referencias indirectas pueden tener alias, y no es fácil saber si una instrucción está haciendo referencia a una variable x enespecial. El análisis del programa debe ser conservador; si no sabemos si una instrucción s estáasignando un valor a x, debemos asumir que puede asignarle un valor; es decir, la variable x después9.2 Introducción al análisis del fl ujo de datos 6013Observe que el camino puede tener ciclos, por lo que podríamos llegar a otra ocurrencia de d a lo largo del camino,lo cual no “elimina” a d.Maq. Cap_9_Aok.indd 601 11/10/07 1:01:50 AM602 Capítulo 9. Optimizaciones independientes de la máquinade la instrucción s puede tener su valor original antes de s, o el nuevo valor creado por s. Por cuestión de simplicidad, el resto del capítulo supone que estamos tratando sólo con variables que notienen alias. Esta clase de variables incluye a todas las variables escalares locales en la mayoría delos lenguajes; en el caso de C y C++, se excluyen las variables locales cuyas direcciones se hayancalculado en algún punto.Ejemplo 9.9: En la figura 9.13 se muestra un grafo de flujo con siete definiciones. Vamos aenfocarnos en las definiciones que llegan al bloque B 2. Todas las definiciones en el bloque B1llegan al inicio del bloque B 2. La definición d 5: j = j−1 en el bloque B 2 también llega al iniciodel bloque B 2, ya que no pueden encontrarse otras definiciones de j en el ciclo que conduce devuelta a B 2. Sin embargo, esta definición elimina a la definición d 2: j = n, la cual evita quellegue a B 3 o B 4. Pero la instrucción d 4: i = i+1 en B 2 no llega al inicio de B 2, ya que la variable i siempre se redefine mediante d 7: i = u3. Por último, la definición d 6: a = u2 tambiénllega al inicio del bloque B 2. ✷Al especificar las definiciones de alcance como lo hemos hecho, en ocasiones les permitimosimprecisiones. Sin embargo, todas se encuentran en la dirección “segura” o “conservadora”. Porejemplo, observe nuestra suposición de que pueden recorrerse todos las flechas de un grafo deflujo. Esta suposición tal vez no se aplique en la práctica. Por ejemplo, para ningún valor de ay de b el flujo de control puede llegar realmente a la instrucción 2 en el siguiente fragmento deprograma:if (a == b) instrucción 1; else if (a == b) instrucción 2;El proceso de decidir en general si se puede tomar cada camino en un grafo de flujo es un problema indecidible. Por ende, tan sólo suponemos que cada camino en el grafo de flujo puede seguirseen cierta ejecución del programa. En la mayoría de las aplicaciones de las definiciones de alcance, esconservador asumir que una definición puede llegar a un punto aun cuando no sea así. En consecuencia, podemos permitir caminos que nunca se recorrerán en ninguna ejecución del programa, ypodemos permitir que las definiciones pasen a través de definiciones ambiguas de la misma variableen forma segura.Detección de los posibles usos antes de la definiciónHe aquí cómo utilizamos una solución al problema de las definiciones de alcance para detectar los usos antes de la definición. El truco es introducir una definición falsa para cadavariable x en la entrada al grafo de flujo. Si la definición falsa de x llega a un punto p endonde podría usarse x, entonces podría presentarse la oportunidad de usar x antes de ladefinición. Tenga en cuenta que nunca podremos estar completamente seguros de que elprograma tiene un error, ya que puede haber alguna razón, que posiblemente involucreel uso de un argumento lógico complejo, por lo que nunca se podrá tomar el camino a lolargo de la cual se llega a p sin una definición real de x.Maq. Cap_9_Aok.indd 602 11/10/07 1:01:50 AMEcuaciones de transferencia para las definiciones de alcanceAhora vamos a establecer las restricciones para el problema de las definiciones de alcance. Empezaremos por examinar los detalles de una sola instrucción. Considere la siguiente definición:d: u = v+wAquí, y con frecuencia en el resto de la explicación, + se utiliza como un operador binariogenérico.Esta instrucción “genera” una definición d de la variable u y “elimina” a todas las demásdefiniciones en el programa que definen a la variable u, al mismo tiempo que no afecta al resto delas definiciones entrantes. Por lo tanto, la función de transferencia de la definición d puede expresarse como:fd(x ) = gend ∪ (x − eliminar d) (9.1)en donde gend = {d}, el conjunto definiciones generadas por la instrucción, y eliminar d es elconjunto de todas las demás definiciones de u en el programa.Como vimos en la sección 9.2.2, la función de transferencia de un bloque básico puede encontrarse mediante la composición de las funciones de transferencia de las instrucciones ahí contenidas. La composición de funciones de la forma (9.1), a la cual nos referiremos como “forma9.2 Introducción al análisis del fl ujo de datos 603Estimación conservadora en el análisis del flujo de datosComo todos los esquemas del flujo de datos calculan aproximaciones a la verdad fundamental (según se define mediante todos los posibles caminos de ejecución del programa),estamos obligados a asegurar que cualquier error se encuentre en la dirección “segura”. Ladecisión de una política es segura (o estimación conservadora) si nunca nos permite modificar lo que el programa calcula. Las políticas seguras pueden, por desgracia, hacer quepasemos por alto ciertas mejoras de código que podrían retener el significado del programa,pero en casi todas las optimizaciones de código no hay una política segura que garantice noomitir nada. Por lo general, sería inaceptable utilizar una política insegura (una que aumente la velocidad del código a expensas de modificar lo que el programa calcula).Por ende, al diseñar un esquema de flujo de datos, debemos estar conscientes de laforma en que se utilizará la información, y asegurarse de que cualquier aproximación quehagamos se encuentre en la dirección “conservadora” o “segura”. Cada esquema y aplicación deben considerarse en forma independiente. Por ejemplo, si utilizamos definicionesde alcance para el cálculo previo de constantes, es seguro pensar que una definición llegacuando no lo hace (podríamos pensar que x no es una constante, cuando de hecho lo esy podría haberse calculado antes), pero no es seguro pensar que una definición no llegacuando si lo hace (podríamos sustituir x por una constante, cuando el programa algunasveces tuviera un valor para x distinto de una constante).Maq. Cap_9_Aok.indd 603 11/10/07 1:01:51 AM604 Capítulo 9. Optimizaciones independientes de la máquinagen B1eliminareliminareliminareliminarB1d1d2d3:::i = mï1j = na = u1d4d5::i = i+1j = jï1gen B2B2gen B3B3d6 : a = u2gen B4B4d7 : i = u3B 1B 2B 4B 3d1 d2 d3d4 d5 d6 d7d4 d5d1 d2 d7d6d3d7d1 d4SALIDA= {= {= {= {= {= {= {= {, ,,,,,, ,,}}}}}}}}ENTRADAFigura 9.13: Grafo de flujo para ilustrar las definiciones de alcancegen-eliminar”, es también de esa forma, como podemos ver a continuación. Suponga que haydos funciones f 1(x ) = gen 1 ∪ (x − eliminar 1) y f 2(x ) = gen 2 ∪ (x − eliminar 2). Entonces: f 2(f 1(x )) = gen 2 ∪ (gen 1 ∪ (x − eliminar 1) − eliminar 2) = (gen 2 ∪ (gen 1 − eliminar 2)) ∪ (x − (eliminar 1 ∪ eliminar 2))Esta regla se extiende a un bloque que consiste en cualquier número de instrucciones. Suponga que el bloque B tiene n instrucciones, con funciones de transferencia fi(x ) = geni ∪ (x− eliminari) para i = 1, 2,…, n. Entonces, la función de transferencia para el bloque B puederescribirse como:fB(x ) = genB ∪ (x − eliminarB),en dondeeliminarB = eliminar 1 ∪ eliminar 2 ∪ … ∪ eliminarnygenB = genn ∪ (genn−1 − eliminarn) ∪ (genn−2 − eliminarn−1 − eliminarn) ∪··· ∪ (gen 1 − eliminar 2 − eliminar 3 − … − eliminarn)Maq. Cap_9_Aok.indd 604 11/10/07 1:01:52 AMPor ende, al igual que una instrucción, un bloque básico también genera un conjunto dedefiniciones y elimina a un conjunto de definiciones. El conjunto gen contiene todas las definiciones dentro del bloque que son “visibles” justo después del bloque; nos referimos a ellas comoexpuestas hacia abajo. Una definición está expuesta hacia abajo en un bloque básico sólo si nola “elimina” una definición siguiente a la misma variable dentro del mismo bloque básico. Elconjunto eliminar de un bloque básico es tan sólo la unión de todas las definiciones que eliminanlas instrucciones individuales. Observe que una definición puede aparecer tanto en el conjuntogen como en el conjunto eliminar de un bloque básico. De ser así, el hecho de que se encuentraen gen tiene precedencia, ya que en la forma gen-eliminar, el conjunto eliminar se aplica antesdel conjunto gen.Ejemplo 9.10: El conjunto gen para el siguiente bloque básico:d 1: a = 3d 2: a = 4es {d 2 }, ya que d 1 no está expuesta hacia abajo. El conjunto eliminar contiene tanto a d 1 como ad 2, ya que d 1 elimina a d 2 y viceversa. Sin embargo, como la resta del conjunto eliminar precedea la operación de unión con el conjunto gen, el resultado de la función de transferencia paraeste bloque siempre incluye la definición d 2. ✷Ecuaciones de flujo de controlA continuación vamos a considerar el conjunto de restricciones que se derivan del flujo decontrol entre los bloques básicos. Como una definición llega a un punto de programa siemprey cuando exista por lo menos un camino a lo largo del cual pueda llegar la definición, SAL[P ]⊆ ENT[B ] cada vez que hay una flecha de flujo de control de P a B. No obstante, como unadefinición no puede llegar a un punto, a menos que haya un camino a lo largo del cual puedallegar, ENT[B ] debe ser no mayor que la unión de las definiciones a las que se llega de todos losbloques predecesores. Es decir, es seguro suponer que:ENT[B ] = ∪P un predecesor de B SAL[P ]Nos referimos a la unión como el operador de reunión para las definiciones de alcance. En cualquier esquema de flujo de datos, el operador de reunión es el que utilizamos para crear un resumende las contribuciones de distintos caminos, en la confluencia de ellos.Algoritmo iterativo para las definiciones de alcanceAsumimos que todo grafo del flujo de control tiene dos bloques básicos vacíos, un nodo ENTRADA, el cual representa el punto inicial del grafo, y un nodo SALIDA, al cual van todas las salidasdel grafo. Como no hay definiciones que lleguen al inicio del grafo, la función de transferenciapara el bloque ENTRADA es una función constante simple que devuelve ∅ como respuesta. Esdecir, SAL[ENTRADA] = ∅.El problema de las definiciones de alcance se define mediante las siguientes ecuaciones:SAL[ENTRADA] = ∅9.2 Introducción al análisis del fl ujo de datos 605Maq. Cap_9_Aok.indd 605 11/10/07 1:01:53 AM606 Capítulo 9. Optimizaciones independientes de la máquinay para todos los bloques básicos B distintos de ENTRADA,SAL[B ] = genB ∪ (ENT[B ] − eliminarB)ENT[B ] = ∪P un predecesor de B SAL[P ].Estas ecuaciones pueden resolverse mediante el siguiente algoritmo. El resultado del algoritmoes el mínimo punto fijo de las ecuaciones; es decir, la solución cuyos valores asignados a losvalores de ENT y SAL estén contenidos en los valores correspondientes para cualquier otra solución a las ecuaciones. El resultado del algoritmo que se muestra a continuación es aceptable,ya que sin duda, cualquier definición en uno de los conjuntos ENT o SAL debe llegar al puntodescrito. Es una solución deseable, ya que no incluye las definiciones que podemos estar segurosque no llegarán.Algoritmo 9.11: Definiciones de alcance.ENTRADA: Un grafo de flujo para el cual se han calculado eliminarB y genB para cadabloque B.SALIDA: ENT[B ] y SAL[B ], el conjunto de definiciones que llegan a la entrada y la salida decada bloque B del grafo de flujo.MÉTODO: Utilizamos un método iterativo, en el cual empezamos con el “estimado” SAL[B ]= ∅ para todos los valores de B y convergemos a los valores deseados de ENT y SAL. Comodebemos iterar hasta que converjan los valores de ENT (y por ende los de SAL), podríamos usaruna variable booleana llamada cambio para registrar, en cada pasada a través de los bloques,si ha cambiado cualquier valor de SAL. No obstante, en este y en algoritmos similares que describiremos más adelante, vamos a suponer que el mecanismo exacto para llevar el registro de loscambios es comprensible, por lo que eludiremos esos detalles.El algoritmo se bosqueja en la figura 9.14. Las primeras dos líneas inicializan ciertos valoresdel flujo de datos.4 La línea (3) inicia el ciclo en el cual iteramos hasta la convergencia, y elciclo interno de las líneas (4) a la (6) aplica las ecuaciones del flujo de datos a todos los bloquesexcepto el de entrada. ✷De manera intuitiva, el Algoritmo 9.11 propaga las definiciones hasta donde lleguen sinque las eliminen, simulando así todas las ejecuciones posibles del programa. En un momentodado, el Algoritmo 9.11 se detendrá, ya que para cada B, SAL[B ] nunca se reduce; una vezque se agrega una definición, permanece ahí para siempre (vea el ejercicio 9.2.6). Como elconjunto de todas los definiciones es finito, en un momento dado debe haber una pasada delciclo while durante el cual no se agrega nada a ningún valor de SAL, y entonces el algoritmo termina. Es seguro terminar entonces, ya que si los valores de SAL no han cambiado, los4El lector observador descubrirá que podríamos combinar con facilidad las líneas (1) y (2). No obstante, en algoritmos de flujo de datos similares, puede ser necesario inicializar al nodo de entrada o de salida en forma distinta a laque usamos para inicializar los demás nodos. Por ende, seguimos un patrón en todos los algoritmos iterativos de aplicaruna “condición del límite” como la línea (1) en forma separada de la inicialización de la línea (2).Maq. Cap_9_Aok.indd 606 11/10/07 1:01:54 AM1) SAL[ENTRADA] = ∅;2) for (cada bloque básico B que no sea ENTRADA) SAL[B ] = ∅;3) while (ocurran cambios a cualquier SAL)4) for (cada bloque básico B que no sea ENTRADA) {5) ENT[B ] = ∪P un predecesor de B SAL[P ];6) SAL[B ] = genB ∪ (ENT[B ] − eliminarB ); }Figura 9.14: Algoritmo iterativo para calcular las definiciones de alcancevalores de ENT no cambiarán la siguiente pasada. Y, si los valores de ENT no cambian, los valores de SAL tampoco, por lo que en todas las pasadas siguientes no puede haber cambios.El número de nodos en el grafo de flujo es un límite superior sobre el número de veces quese recorre el ciclo while. La razón es que si una definición llega a un punto, puede hacerlo alo largo de un camino sin ciclos, y número de nodos en un grafo de flujo es un límite superiorsobre el número de nodos en un camino sin ciclo. Cada vez que se recorre el ciclo while, cadadefinición progresa por lo menos un nodo a lo largo del camino en cuestión, y a menudo progresa más de un nodo, dependiendo del orden en el que se visiten los nodos.De hecho, si ordenamos en forma apropiada los bloques en el ciclo for de la línea (5), existeuna evidencia empírica de que el número promedio de iteraciones del ciclo while es menor de 5(vea la sección 9.6.7). Como los conjuntos de definiciones se pueden representar mediante vectores de bits, y las operaciones sobre estos conjuntos se pueden implementar mediante operacioneslógicas en los vectores de bits, el Algoritmo 9.11 es muy eficiente en la práctica.Ejemplo 9.12: Vamos a representar las siete definiciones d 1, d 2,…, d 7 en el grafo de flujo dela figura 9.13 mediante vectores de bits, en donde el bit i de la izquierda representa a la definición d i. La unión de conjuntos se calcula tomando la operación OR lógica de los correspondientes vectores de bits. La diferencia de dos conjuntos S − T se calcula mediante el complementodel vector de bits de T, y después se toma la operación AND lógica de ese complemento, con elvector de bits para S.En la tabla de la figura 9.15 se encuentran los valores tomados por los conjuntos ENT y SALen el Algoritmo 9.11. Los valores iniciales, que se indican mediante un superíndice 0, como enSAL[B ]0, se asignan mediante el ciclo de la línea (2) de la figura 9.14. Cada uno de ellos es elconjunto vacío, representado por el vector de bits 000 0000. Los valores de las siguientes pasadas del algoritmo también se indican mediante súper índices, y se etiquetan como ENT[B ]1 ySAL[B ]1 para la primera pasada, y ENT[B ]2 y SAL[B ]2 para la segunda.Suponga que el ciclo for de las líneas (4) a la (6) se ejecuta cuando B toma los siguientesvalores:B1, B 2, B 3, B 4, SALIDAen ese orden. Con B = B1, como SAL[ENTRADA] = ∅, ENT[B ]1 es el conjunto vacío, y SAL[B ]1 esgenB1. Este valor difiere del valor anterior SAL[B1]0, por lo que ahora sabemos que hay un cambioen la primera ronda (y procederemos a una segunda ronda).9.2 Introducción al análisis del fl ujo de datos 607Maq. Cap_9_Aok.indd 607 11/10/07 1:01:54 AM608 Capítulo 9. Optimizaciones independientes de la máquinaSALIDABloque SAL ENT SAL ENT SALFigura 9.15: Cálculo de ENT y SALDespués consideramos B = B 2 y calculamosENT[B 2]1 = SAL[B1]1 ∪ SAL[B 4]0= 111 0000 + 000 0000 = 111 0000SAL[B 2]1 = gen[B 2] ∪ (ENT[B 2]1 − eliminar [B 2]) = 000 1100 + (111 0000 − 110 0001) = 001 1100Este cálculo se resume en la figura 9.15. Por ejemplo, al final de la primera pasada, SAL[B 2]1= 001 1100, reflejando el hecho de que d 4 y d 5 se generan en B 2, mientras que d 3 llega al iniciode B 2 y no se elimina en B 2.Observe que después de la segunda ronda, SAL[B 2] ha cambiado para reflejar el hecho deque d 6 también llega al inicio de B 2 y éste no la elimina. No aprendimos ese hecho en la primerapasada, ya que el camino de d 6 hasta el final de B 2, que es B 3 → B 4 → B 2, no se recorre enese orden mediante una sola pasada. Es decir, para cuando sabemos que d 6 llega al final de B 4,ya hemos calculado ENT[B 2] y SAL[B 2] en la primera pasada.No hay cambios en ninguno de los conjuntos SAL después de la segunda pasada. Por ende,después de una tercera pasada, el algoritmo termina, con los valores de ENT y SAL como en lasúltimas dos columnas de la figura 9.15. ✷9.2.5 Análisis de variables vivasAlgunas transformaciones para mejorar el código dependen de la información que se calcula enla dirección opuesta al flujo de control de un programa; vamos a examinar uno de esos ejemplosahora. En el análisis de variables vivas deseamos conocer para la variable x y el punto p si elvalor de x en p podría utilizarse a lo largo de algún camino en el grafo de flujo, empezando en p.Si es así, decimos que x está viva en p; en cualquier otro caso, x está muerta en p.Un uso importante para la información de las variables vivas es la repartición de registrospara los bloques básicos. En las secciones 8.6 y 8.8 se presentaron los aspectos relacionados conesta cuestión. Después de calcular un valor en un registro, y supuestamente utilizarlo dentrode un bloque, no es necesario almacenar ese valor si está muerto al final del bloque. Además,si todos lo registros están llenos y necesitamos otro registro, es preferible usar un registro conun valor muerto, ya que ese valor tiene que almacenarse.Maq. Cap_9_Aok.indd 608 11/10/07 1:01:55 AMAquí, definimos las ecuaciones del flujo de datos directamente en términos de ENT[B ] ySAL[B ], que representan el conjunto de variables vivas en los puntos justo antes y después delbloque B, respectivamente. Estas ecuaciones también pueden derivarse si primero definimoslas funciones de transferencia de las instrucciones individuales y las componemos para crear lafunción de transferencia de un bloque básico. Se definen:1. defB como el conjunto de variables definidas (es decir, los valores asignados en formadefinitiva) en B, antes de cualquier uso de esa variable en B.2. usoB como el conjunto de variables cuyos valores pueden usarse en B, antes de cualquierdefinición de la variable.Ejemplo 9.13: Por ejemplo, el bloque B 2 en la figura 9.13 utiliza a i en forma definitiva.También utiliza a j antes de cualquier redefinición de j, a menos que sea posible que i y j seanalias una de la otra. Suponiendo que no hay alias entre las variables de la figura 9.13, entoncesusoB2 = {i, j}. Además, B 2 define claramente a i y j. Suponiendo que no hay alias, defB2 = {i, j},también. ✷Como consecuencia de las definiciones, cualquier variable en usoB debe considerarse viva alentrar al bloque B, mientras que las definiciones de las variables en defB definitivamente estánmuertas al inicio de B. En efecto, la membresía en defB “elimina” cualquier oportunidad de queuna variable esté viva, debido a los caminos que empiezan en B.Por ende, las ecuaciones que relacionan a def y uso con los valores desconocidos de ENT ySAL se definen de la siguiente manera:ENT[SALIDA] = ∅y para todos los bloques básicos B distintos de SALIDA,ENT[B ] = usoB ∪ (SAL[B ] − defB)SAL[B ] = ∪S un sucesor de B ENT[S ]La primera ecuación especifica la condición delimitadora, la cual establece que no hay variablesvivas al salir del programa. La segunda ecuación dice que una variable está viva al entrar a unbloque si se utiliza antes de su redefinición en el bloque, o si está viva al salir del bloque y no seredefine en el mismo. La tercera ecuación dice que una variable está viva al salir de un bloquesi, y sólo si está viva al entrar a uno de sus sucesores.Hay que tener en cuenta la relación entre las ecuaciones para el estado de vida y lasecuaciones de las definiciones de alcance:9.2 Introducción al análisis del fl ujo de datos 609Maq. Cap_9_Aok.indd 609 11/10/07 1:01:56 AM610 Capítulo 9. Optimizaciones independientes de la máquina• Ambos conjuntos de ecuaciones tienen la unión como operador de reunión. La razón esque en cada esquema de flujo de datos propagamos información a lo largo de los caminos,y sólo nos preocupa saber si existe algún camino con las propiedades deseadas, en vez desaber si algo es verdadero a lo largo de todos los caminos.• Sin embargo, el flujo de información para el estado de vida viaja “a la inversa”, en sentidoopuesto a la dirección del flujo de control, ya que en este problema deseamos asegurarnosde que el uso de una variable x en un punto p se transmita a todos los puntos anterioresa p en algún camino de ejecución, de manera que sepamos en el punto anterior que seutilizará el valor de x.Para resolver un problema inverso, en vez de inicializar SAL[ENTRADA], inicializamos ENT[SALIDA]. Se intercambian los papeles de los conjuntos ENT y SAL, además uso y def sustituyen a gen y eliminar, respectivamente. En cuanto a las definiciones de alcance, la solución alas ecuaciones del estado de vida no es necesariamente única, y deseamos la solución con losconjuntos más pequeños de variables vivas. El algoritmo utilizado es en esencia una versióninversa del Algoritmo 9.11.Algoritmo 9.14: Análisis de variables vivas.ENTRADA: Un grafo de flujo en el que se calculan def y uso para cada bloque.SALIDA: ENT[B ] y SAL[B ], el conjunto de variables vivas al entrar y salir de cada bloque Bdel grafo de flujo.MÉTODO: Ejecute el programa de la figura 9.16. ✷ENT[SALIDA] = ∅;for (cada bloque básico B que no sea SALIDA) ENT[B ] = ∅;while (se realicen modificaciones a cualquier ENT)for (cada bloque básico B que no sea SALIDA) {SAL[B ] = ∪S un sucesor de B ENT[S ];ENT[B ] = usoB ∪ (SAL[B ] − defB); }Figura 9.16: Algoritmo iterativo para calcular las variables vivas9.2.6 Expresiones disponiblesUna expresión x + y está disponible en el punto p si todas los caminos desde el nodo de entradahacia p se evalúan como x + y, y si después de la última evaluación de este tipo, antes de llegara p, no hay asignaciones siguientes para x o y.5 Para el esquema de flujo de datos de expresionesdisponibles decimos que un bloque elimina a la expresión x + y si asigna (o puede asignar) a x o a y,5Observe que, como es usual en este capítulo, utilizamos el operador + como un operador genérico, que no necesariamente representa a la suma.Maq. Cap_9_Aok.indd 610 11/10/07 1:01:57 AMy no vuelve a calcular posteriormente la expresión x + y. Un bloque genera la expresión x + y sien definitiva evalúa x + y y no define posteriormente a x o a y.Observe que la noción de “eliminar” o “generar” una expresión disponible no es exactamente igual que para las definiciones de alcance. Sin embargo, estas nociones de “eliminar” y“generar” se comportan en esencia de la misma forma que para las definiciones de alcance.El uso principal de la información sobre expresiones disponibles es para detectar las subexpresiones comunes globales. Por ejemplo, en la figura 9.17(a), la expresión 4 ∗ i en el bloqueB 3 será una subexpresión común si 4 ∗ i está disponible en el punto de entrada del bloque B 3.Estará disponible si no se asigna a i un nuevo valor en el bloque B 2, o si, como en la figura9.17(b), 4 ∗ i se vuelve a calcular después de que i se asigna en B 2.t1 = 4*it2 = 4*iBBB123t1 = 4*it2 = 4*iBBB123? i =t1 = 4*i (a) (b) Figura 9.17: Subexpresiones comunes potenciales entre los bloquesPodemos calcular el conjunto de expresiones generadas para cada punto en un bloque, trabajando desde el principio hasta el final del bloque. En el punto antes del bloque, no se generanexpresiones. Si en el punto p está disponible el conjunto S de expresiones, y q es el punto después de p, con la instrucción x = y+z entre ellos, entonces formamos el conjunto de expresionesdisponible en q mediante los siguientes dos pasos.1. Agregar a S la expresión y + z.2. Eliminar de S cualquier expresión que involucre a la variable x.Hay que tener en cuenta que los pasos deben realizarse en el orden correcto, ya que xpodría ser igual que y o z. Una vez que llegamos al final del bloque, S es el conjunto deexpresiones generadas para el bloque. El conjunto de expresiones eliminadas consiste en todaslas expresiones, por decir y + z, de tal forma que se defina y o z en el bloque, y que el bloqueno genere a y + z.Ejemplo 9.15: Considere las cuatro instrucciones de la figura 9.18. Después de la primera, b + cestá disponible. Después de la segunda instrucción, a − d se vuelve disponible, pero b + c yano está disponible, debido a que b se ha definido de nuevo. La tercera instrucción no hace queb + c esté disponible otra vez, ya que el valor de c se modifica de inmediato.9.2 Introducción al análisis del fl ujo de datos 611Maq. Cap_9_Aok.indd 611 11/10/07 1:01:57 AM612 Capítulo 9. Optimizaciones independientes de la máquinaDespués de la última instrucción, a − d ya no está disponible, debido a que d ha cambiado. Porende no se generan expresiones, y se eliminan todas las expresiones en las que se involucren a,b, c o d. ✷Figura 9.18: Cálculo de las expresiones disponiblesPodemos encontrar las expresiones disponibles de una forma parecida a la manera en laque se calculan las definiciones de alcance. Suponga que U es el conjunto “universal” de todaslas expresiones que aparecen a la derecha de una o más instrucciones del programa. Para cadabloque B, hagamos que ENT[B ] sea el conjunto de expresiones en U que estén disponibles enel punto justo antes del inicio de B. Hagamos que SAL[B ] sea igual que para el punto que vadespués del final de B. Definamos a e-genB para que represente a las expresiones que se generanen B, y a e-eliminarB para ser el conjunto de expresiones en U eliminadas en B. Observe que ENT,SAL, e-gen, y e-eliminar pueden representarse mediante vectores de bits. Las siguientes ecuaciones relacionan los valores desconocidos ENT y SAL entre sí, y las cantidades conocidas e-geny e-eliminar:SAL[ENTRADA] = ∅y para todos los bloques básicos B que no sean ENTRADA, SAL[B ] = e-genB ∪ (ENT[B ] − e-eliminarB ) ENT[B ] = ∩P un predecesor de B SAL[P ].Las ecuaciones anteriores se ven casi idénticas a las ecuaciones para las definiciones dealcance. Al igual que las definiciones de alcance, la condición delimitadora es SAL[ENTRADA]= ∅, ya que en la salida del nodo ENTRADA no hay expresiones disponibles. La diferencia másimportante es que el operador de reunión es la intersección, en vez de la unión. Este operador esel apropiado, ya que una expresión está disponible al inicio de un bloque sólo si está disponibleal final de todos sus predecesores. En contraste, una definición llega al final de un bloque cadavez que llega al final de uno o más de sus predecesores.Instrucción Expresiones disponiblesMaq. Cap_9_Aok.indd 612 11/10/07 1:01:58 AMEl uso de ∩ en vez de ∪ hace que las ecuaciones de las expresiones disponibles se comporten en forma distinta a las de las definiciones de alcance. Aunque ningún conjunto tiene unasolución única, para las definiciones de alcance, la solución con los conjuntos más pequeños esla que corresponde a la definición de “alcance”, y obtuvimos esa solución empezando con lasuposición de que nada llegaba a ninguna parte, y progresamos hasta llegar a la solución. Enesa forma, nunca se supuso que una definición d podría llegar a un punto p, a menos de quepudiera encontrarse un camino actual que se propagara de d hasta p. En contraste, para lasecuaciones de las expresiones disponibles queremos la solución con los conjuntos más grandesde expresiones disponibles, por lo que empezamos con una aproximación demasiado grande yla vamos reduciendo.Tal vez no sea evidente que al empezar con la suposición “todo (es decir, el conjunto U)está disponible en cualquier parte, excepto al final del bloque de entrada” y eliminar sólo aquellas expresiones para las cuales podamos descubrir un camino a lo largo de la cual no esté disponible, podemos llegar a un conjunto de expresiones verdaderamente disponibles. En el casode las expresiones disponibles, es conservador producir un subconjunto del conjunto exacto deexpresiones disponibles. El argumento para que los subconjuntos sean conservadores es que eluso que pretendemos de la información es sustituir el cálculo de una expresión disponible porun valor calculado con anterioridad. Al no saber que una expresión está disponible no podemosmejorar el código, mientras que al creer que una expresión está disponible cuando en realidadno es así, podría modificar lo que el programa calcula.BB12Figura 9.19: La inicialización de los conjuntos SAL a ∅ es demasiado restrictivaEjemplo 9.16: Vamos a concentrarlos en un solo bloque, B 2 en la figura 9.19, para ilustrarel efecto de la aproximación inicial de SAL[B 2] sobre ENT[B 2]. Hagamos que G y K sean abreviaciones de e-genB 2 y e-eliminarB 2, respectivamente. Las ecuaciones del flujo de datos para elbloque B 2 son:ENT[B 2] = SAL[B1] ∩ SAL[B 2]SAL[B 2] = G ∪ (ENT[B 2] − K)9.2 Introducción al análisis del fl ujo de datos 613Maq. Cap_9_Aok.indd 613 11/10/07 1:01:59 AM614 Capítulo 9. Optimizaciones independientes de la máquinaEstas ecuaciones pueden rescribirse como recurrencias, en donde I j y O j son las j-ésimas aproximaciones de ENT[B 2] y SAL[B 2], respectivamente:I j +1 = SAL[B1] ∩ O jO j +1 = G ∪ (I j +1 − K)Empezando con O0 = ∅, obtenemos I 1 = SAL[B1] ∩ O0 = ∅. Sin embargo, si empezamos conO0 = U, entonces obtenemos I 1= SAL[B1] ∩ O0 = SAL[B1], como deberíamos. Por intuición, lasolución que se obtiene empezando con O0 = U es una mejor opción, ya que refleja de maneracorrecta el hecho de que las expresiones en SAL[B1] que B 2 no elimina están disponibles al finalde B 2. ✷Algoritmo 9.17: Expresiones disponibles.ENTRADA: Un grafo del flujo en el cual se calculan e-eliminarB y e-genB para cada bloque B.El bloque inicial es B1.SALIDA: ENT[B] y SAL[B], el conjunto de expresiones disponibles en la entrada y salida decada bloque B del grafo de flujos.MÉTODO: Ejecute el algoritmo de la figura 9.20. La explicación de los pasos es similar a lade la figura 9.14. ✷SAL[ENTRADA] = ∅;for (cada bloque básico B que no sea ENTRADA) SAL[B ] = U;while (ocurra algún cambio en SAL)for (cada bloque básico B que no sea ENTRADA) {ENT[B ] = ∩P un predecesor de B SAL[P ];SAL[B ] = e-genB ∪ (ENT[B ] − e-eliminarB); }Figura 9.20: Algoritmo iterativo para calcular las expresiones disponibles9.2.7 ResumenEn esta sección, hablamos sobre tres instancias de los problemas del flujo de datos: las definiciones de alcance, las variables vivas y las expresiones disponibles. Como se resume en lafigura 9.21, la definición de cada problema se proporciona mediante el dominio de los valoresdel flujo de datos, la dirección del flujo de datos, la familia de funciones de transferencia, lacondición delimitadora y el operador de reunión. Denotamos el operador de reunión en formagenérica como ∧.La última fila muestran los valores iniciales utilizados en el algoritmo iterativo. Estos valores se eligen de tal forma que el algoritmo iterativo encuentre la solución más precisa a lasecuaciones. Esta elección no forma estrictamente una parte de la definición del problema delMaq. Cap_9_Aok.indd 614 11/10/07 1:02:00 AMflujo de datos, ya que es un dispositivo necesario para el algoritmo iterativo. Existen otrasformas de resolver el problema. Por ejemplo, vimos cómo la función de transferencia de unbloque básico puede derivarse mediante la composición de las funciones de transferencia de lasinstrucciones individuales en el bloque; puede utilizarse un método de composición similar alcalcular una función de transferencia para todo el procedimiento; o funciones de transferenciadesde la entrada del procedimiento hasta cualquier punto del programa. En la sección 9.7 hablaremos sobre dicho método.Definiciones de alcance Variables vivas Expresiones disponiblesDominio Conjuntos de definiciones Conjuntos de variables Conjuntos de expresionesDirección Hacia delante Hacia atrás Hacia delanteFunción detransferenciagenB ∪ (x − eliminarB) usoB ∪ (x − defB) e-genB ∪ (x − eeliminarB)Límite SAL[ENTRADA] = ∅ ENT[SALIDA] = ∅ SAL[ENTRADA] = ∅Reunión (∧) ∪ ∪∩Ecuaciones SAL[B ] = fB(ENT[B ])ENT[B ] =∧P,pred (B) SAL[P ]ENT[B ] = fB(SAL[B ])SAL[B ] =∧S,suc (B) ENT[S ]SAL[B ] = fB(ENT[B ])ENT[B ] =∧P,pred (B) SAL[P ]Inicializar SAL[B ] = ∅ ENT[B ] = ∅ SAL[B ] = UFigura 9.21: Resumen de tres problemas de flujo de datos9.2.8 Ejercicios para la sección 9.2Ejercicio 9.2.1: Para el grafo de flujo de la figura 9.10 (vea los ejercicios de la sección 9.21),calcule:a) Los conjuntos gen y eliminar para cada bloque.b) Los conjuntos ENT y SAL para cada bloque.Ejercicio 9.2.2: Para el grafo de flujo de la figura 9.10, calcule los conjuntos e-gen, e-eliminar, ENT y SAL para las expresiones disponibles.Ejercicio 9.2.3: Para el grafo de flujo de la figura 9.10, calcule los conjuntos def, uso, ENT ySAL para el análisis de variables vivas.Ejercicio 9.2.4: Suponga que V es el conjunto de números complejos. ¿Cuál de las siguientesoperaciones puede servir como la operación de reunión para un semi-lattice en V?a) Suma: (a + ib) ∧ (c + id) = (a + b) + i(c + d).b) Multiplicación: (a + ib) ∧ (c + id) = (ac − bd ) + i(ad + bc).9.2 Introducción al análisis del fl ujo de datos 615!Maq. Cap_9_Aok.indd 615 11/10/07 1:02:01 AM616 Capítulo 9. Optimizaciones independientes de la máquinac) Componente mínimo: (a + ib) ∧ (c + id) = min(a, c) + i min(b, d).d) Componente máximo: (a + ib) ∧ (c + id) = max(a, c) + i max(b, d).Ejercicio 9.2.5: Afirmamos que si un bloque B consiste en n instrucciones, y la i-ésima instrucción tiene los conjuntos gen y eliminar geni y eliminari, entonces la función de transferenciapara el bloque B tiene los conjuntos gen y eliminar genB y eliminarB dados por:eliminarB = eliminar 1 ∪ eliminar 2 ∪ ... ∪ eliminarngenB = gen n ∪ (genn−1 − eliminarn) ∪ (gen n −2 − eliminarn−1 − eliminarn) ∪... ∪ (gen1 − eliminar 2 − eliminar 3 − ... − eliminarn).Demuestre esta afirmación mediante la inducción sobre n.Ejercicio 9.2.6: Demuestre mediante la inducción sobre el número de iteraciones del ciclo forde las líneas (4) a la (6) del Algoritmo 9.11, que ninguno de los valores de ENT o SAL se reduce.Es decir, una vez que se coloca una definición en uno de sus conjuntos en cierta ronda, nuncadesaparece en una ronda siguiente.Por qué funciona el algoritmo de expresiones disponiblesDebemos explicar por qué al iniciar todos los valores de SAL excepto los bloques de entrada con U, el conjunto de todas las expresiones, nos lleva a una solución conservadorapara las ecuaciones de flujo de datos; es decir, todas las expresiones que se encuentrancomo disponibles en realidad lo están. En primer lugar, debido a que la intersección esel operador de reunión en este esquema de flujo de datos, cualquier motivo por el que seencuentre que una expresión x + y no está disponible en un punto se propagará hacia delante en el grafo de flujo, junto con todas los caminos posibles, hasta que x + y se vuelvaa calcular y sea disponible otra vez. En segundo lugar, sólo hay dos razones por las quex + y no podría estar disponible:1. x + y se elimina en el bloque B debido a que x o y están definidas sin un cálculosiguiente de x + y. En este caso, la primera vez que apliquemos la función de transferencia fB, x + y se eliminará de SAL[B ].2. x + y nunca se calcula a lo largo de algún camino. Como x + y nunca está enSAL[ENTRADA], y nunca se genera a lo largo del camino en cuestión, podemos mostrar mediante la inducción sobre la longitud del camino que x + y se elimina en unmomento dado de los valores de ENT y SAL a lo largo del camino.Así, una vez que terminen los cambios, la solución proporcionada por el algoritmo iterativo de la figura 9.20 sólo incluirá expresiones que en verdad estén disponibles.!!Maq. Cap_9_Aok.indd 616 11/10/07 1:02:02 AMEjercicio 9.2.7: Muestre que el algoritmo 9.11 es correcto. Es decir, muestre que:a) Si la definición d se coloca en ENT[B ] o SAL[B ], entonces hay un camino que va desde dhasta el inicio o final del bloque B, respectivamente, a lo largo de la cual la variable definida por d podría no redefinirse.b) Si la definición d se coloca en ENT[B ] o SAL[B ], entonces no hay un camino que vayadesde d hasta el inicio o final del bloque B, respectivamente, a lo largo de la cual la variable definida por d podría no redefinirse.Ejercicio 9.2.8: Demuestre lo siguiente acerca del algoritmo 9.14:a) Los valores de ENT y SAL nunca se reducen.b) Si la variable x se coloca en ENT[B ] o SAL[B ], entonces hay un camino que va desde elinicio o final del bloque B, respectivamente, a lo largo de la cual x podría usarse.c) Si la variable x no se coloca en ENT[B ] o SAL[B ], entonces no hay un camino que vayadesde el inicio o final del bloque B, respectivamente, a lo largo de la cual x podría usarse.Ejercicio 9.2.9: Demuestre lo siguiente acerca del Algoritmo 9.17:a) Los valores de ENT y SAL nunca aumentan; es decir, los valores sucesivos de estos conjuntos son subconjuntos (no necesariamente propios) de sus valores anteriores.b) Si la expresión e se elimina de ENT[B ] o SAL[B ], entonces hay un camino que va desde laentrada del grafo de flujo hasta el inicio o final del bloque B, respectivamente, a lo largode la cual e nunca se calcula, o después de su último cálculo, uno de sus argumentos podría redefinirse.c) Si la expresión e permanece en ENT[B ] o SAL[B ], entonces a lo largo de cada camino queva desde la entrada del grafo de flujo hasta el inicio o final del bloque B, respectivamente,e se calcula y, después del último cálculo, no podría redefinirse ningún argumento de e.Ejercicio 9.2.10: El lector astuto observará que en el Algoritmo 9.11 podríamos haber ahorrado algo de tiempo, inicializando SAL[B ] con genB para todos los bloques B. De igual forma,en el Algoritmo 9.14 podríamos haber inicializado ENT[B ] con genB. No lo hicimos por cuestiónde uniformidad en el tratamiento del tema, como veremos en el Algoritmo 9.25. Sin embargo,¿es posible inicializar SAL[B ] con e-genB en el Algoritmo 9.17? ¿Por qué sí o por qué no?Ejercicio 9.2.11: Hasta ahora, nuestros análisis del flujo de datos no aprovechan la semánticade las instrucciones condicionales. Suponga que al final de un bloque básico encontramos unaprueba como: if (x < 10) goto ...¿Cómo podríamos usar nuestra comprensión de lo que significa la prueba x < 10 para mejorarnuestro conocimiento de las definiciones de alcance? Recuerde que aquí “mejorar” significaque eliminamos ciertas definiciones de alcance que en realidad nunca podrán llegar a un ciertopunto del programa.9.2 Introducción al análisis del fl ujo de datos 617!!!!!Maq. Cap_9_Aok.indd 617 11/10/07 1:02:02 AM618 Capítulo 9. Optimizaciones independientes de la máquina9.3 Fundamentos del análisis del flujo de datosDespués de mostrar varios ejemplos útiles en la abstracción del flujo de datos, ahora estudiaremos la familia de los esquemas del flujo de datos como un todo, en forma abstracta. Vamosa responder de manera formal varias preguntas básicas acerca de los algoritmos de flujo dedatos:1. ¿Bajo qué circunstancias es correcto el algoritmo iterativo que se utiliza en el análisis delflujo de datos?2. ¿Qué tan precisa es la solución obtenida por el algoritmo iterativo?3. ¿Convergerá el algoritmo iterativo?4. ¿Cuál es el significado de la solución a las ecuaciones?En la sección 9.2, tratamos cada una de las preguntas anteriores de manera informal, aldescribir el problema de las definiciones de alcance. En vez de responder a las mismas preguntas para cada problema siguiente partiendo desde cero, nos basamos en las analogías con losproblemas que ya habíamos expuesto para explicar los problemas nuevos. Aquí presentamos unmétodo general que responde a todas esas preguntas, de una vez por todas, con rigor y parauna larga familia de problemas de flujo de datos. Primero identificamos las propiedades deseadas de los esquemas del flujo de datos y demostramos las implicaciones de estas propiedades enla exactitud, precisión y convergencia del algoritmo de flujo de datos, así como el significado de lasolución. Así, para comprender algoritmos viejos o formular nuevos, sólo mostramos que las definiciones propuestas al problema del flujo de datos tienen ciertas propiedades, y las respuestasa todas las preguntas difíciles anteriores están disponibles de inmediato.El concepto de tener un marco de trabajo común teórico para una clase de esquemas también tiene implicaciones prácticas. El marco de trabajo nos ayuda a identificar los componentesreutilizables del algoritmo en nuestro diseño de software. No sólo se reduce el esfuerzo de codificación, sino que los errores de programación también lo hacen al no tener que volver a codificarlos detalles similares varias veces.Un marco de trabajo de análisis del flujo de datos (D, V, ∧, F) consiste en:1. Una dirección del flujo de datos D, que puede ser HACIADELANTE o HACIAATRAS.2. Un semi-lattice (vea la sección 9.3.1 para la definición), que incluye un dominio de valores V y un operador de reunión ∧.3. Una familia F de funciones de transferencia de V a V. Esta familia debe incluir funcionesapropiadas para las condiciones delimitadoras, que son funciones de transferencia deconstantes para los nodos especiales ENTRADA y SALIDA en cualquier grafo de flujo.9.3.1 Semi-latticesUn semi-lattice consiste en un conjunto V y un operador de reunión binario ∧ tal que paratodas las x, y y z en V:Maq. Cap_9_Aok.indd 618 11/10/07 1:02:03 AM1. x ∧ x = x (el operador de reunión es idempotente).2. x ∧ y = y ∧ x (el operador de reunión es conmutativo).3. x ∧ (y ∧ z) = (x ∧ y ) ∧ z (el operador de reunión es asociativo).Un semi-lattice tiene un elemento superior, denotado como 	, de tal forma quepara todas las x en V, 	 ∧ x = x.De manera opcional, un semi-lattice puede tener un elemento inferior, denotado como ⊥, detal forma quepara todas las x en V, ⊥ ∧ x = ⊥.Órdenes parcialesComo veremos más adelante, el operador de reunión de un semilattice define un orden parcialsobre los valores del dominio. Una relación ≤ es un orden parcial sobre un conjunto V si paratodas las x, y y z en V:1. x ≤ x (el orden parcial es reflexivo).2. Si x ≤ y y y ≤ x, entonces x = y (el orden parcial es antisimétrico).3. Si x ≤ y y y ≤ z, entonces x ≤ z (el orden parcial es transitivo).El par (V, ≤) se llama poset, o conjunto parcialmente ordenado. También es conveniente teneruna relación < para un poset, la cual se define como:x < y si, y sólo si (x ≤ y ) y (x ≠ y ).El orden parcial para un semi-latticeEs útil definir un orden parcial ≤ para un semi-lattice (V, ∧). Para todas las x y y en V, definimosx ≤ y si, y sólo si x ∧ y = x.Como el operador de reunión ∧ es idempotente, conmutativo y asociativo, el orden ≤ se definecomo reflexivo, antisimétrico y transitivo. Para ver por qué, tome en cuenta que:• Reflexividad: para todas las x, x ≤ x. La prueba es que x ∧ x = x, ya que el operador dereunión es idempotente.• Antisimetría: si x ≤ y y y ≤ x, entonces x = y. Para demostrar esto, x ≤ y significa quex ∧ y = x y y ≤ x significa que y ∧ x = y. Por la condición conmutativa de ∧, x = (x ∧ y ) =(y ∧ x ) = y.9.3 Fundamentos del análisis del f lujo de datos 619Maq. Cap_9_Aok.indd 619 11/10/07 1:02:04 AM620 Capítulo 9. Optimizaciones independientes de la máquina• Transitividad: si x ≤ y y y ≤ z, entonces x ≤ z. Para demostrar esto, x ≤ y y y ≤ z significaque x ∧ y = x y y ∧ z = y. Entonces (x ∧ z) = ((x ∧ y ) ∧ z) = (x ∧ (y ∧ z)) = (x ∧ y )= x, usando la asociatividad del operador de reunión. Como x ∧ z = x se ha demostrado,tenemos x ≤ z, lo cual demuestra la condición transitiva.Ejemplo 9.18: Los operadores de reunión utilizados en los ejemplos de la sección 9.2 son launión y la intersección de conjuntos. Ambos son idempotentes, conmutativos y asociativos.Para la unión de conjuntos, el elemento superior es ∅ y el elemento inferior es U, el conjuntouniversal, ya que para cualquier subconjunto x de U, ∅ ∪ x = x y U ∪ x = U. Para la intersección de conjuntos, 	 es U y ⊥ es ∅. V, el dominio de valores del semi-lattice, es el conjuntode todos los subconjuntos de U, que algunas veces se le llama el conjunto potencia de U y se denota como 2U.Para todas las x y y en V, x ∪ y = x implica x ⊇ y; por lo tanto, el orden parcial impuesto poruna unión de conjuntos es ⊇, el conjunto inclusión. De manera correspondiente, el orden parcialimpuesto por la intersección de conjuntos es ⊆, la contención de conjuntos. Es decir, para la intersección de conjuntos, los conjuntos con menos elementos se consideran más pequeños en elorden parcial. No obstante, para la unión de conjuntos, los conjuntos con más elementos se consideran más pequeños en el orden parcial. Es contra intuitivo decir que los conjuntos de mayortamaño son más pequeños en el orden parcial; sin embargo, esta situación es una consecuenciainevitable de las definiciones.6Como vimos en la sección 9.2, por lo general, hay muchas soluciones para un conjunto deecuaciones de un flujo de datos, en donde la solución más grande (en el sentido del orden parcial ≤) es la más precisa. Por ejemplo, en las definiciones de alcance, la más precisa de todas lassoluciones a las ecuaciones de un flujo de datos es la que tiene el menor número de definiciones,que corresponde al mayor elemento en el orden parcial definido por la operación de reunión, launión. En las expresiones disponibles, la solución más precisa es la que tiene el mayor númerode expresiones. De nuevo, es la solución más grande en el orden parcial definido por la intersección como operación de reunión. ✷Límites inferiores más grandesHay otra relación útil entre la operación de reunión y el orden parcial que impone. Supongaque (V, ∧) es un semi-lattice. Un límite menor más grande (o glb) de los elementos del dominiox y y es un elemento g tal que:1. g ≤ x,2. g ≤ y, y3. Si z es un elemento tal que z ≤ x y z ≤ y, entonces z ≤ g.Resulta que el operador de reunión de x y y es su único límite inferior más grande. Para verpor qué, hagamos que g = x ∧ y. Observe que:6Y si definiéramos el orden parcial como ¦ en vez de ≤, entonces el problema surgiría cuando el operador dereunión fuera la intersección, aunque no para la unión.Maq. Cap_9_Aok.indd 620 11/10/07 1:02:04 AM• g ≤ x, ya que (x ∧ y ) ∧ x = x ∧ y. La prueba implica usos simples de condiciones asociatividad, conmutatividad e idempotencia. Es decir,• g ≤ y por un argumento similar.• Suponga que z es un elemento tal que z ≤ x y z ≤ y. Afirmamos que z ≤ g y, por lo tanto,z no puede ser un glb de x y y, a menos que también sea g. Como demostración: (z∧g)= (z∧ (x∧y )) = ((z∧x )∧y ). Como z ≤ x, sabemos que (z∧x ) = z, por lo que (z∧g ) =(z∧y ). Como z ≤ y, sabemos que z∧y = z y, por lo tanto, z∧g = z. Hemos demostradoque z ≤ g y concluimos que g = x ∧ y es el único glb de x y y.Diagramas de latticesA menudo es útil dibujar el dominio V como un diagrama de lattices, el cual es un grafo cuyosnodos son los elementos de V, y cuyas flechas se dirigen hacia abajo, desde x hasta y si y ≤ x. Porejemplo, la figura 9.22 muestra el conjunto V para un esquema de flujo de datos con definicionesde alcance, en donde hay tres definiciones: d 1, d 2 y d 3. Como ≤ es ⊇, una flecha se dirige haciaabajo desde cualquier subconjunto de estas tres definiciones hasta cada uno de sus superconjuntos. Como ≤ es transitivo, por convención omitimos la flecha que va desde x hasta y, siempre y9.3 Fundamentos del análisis del f lujo de datos 621Uniones, límites superiores más pequeños (lub) y latticesEn simetría con la operación glb sobre los elementos de un poset, podemos definir ellímite superior más pequeño (o lub) de los elementos x y y como aquél elemento b tal quex ≤ b, y ≤ b, y si z es cualquier elemento tal que x ≤ z y y ≤ z, entonces b ≤ z. Podemosmostrar que hay por lo menos un elemento b así, si es que existe.En un verdadero lattice, hay dos operaciones sobre elementos del dominio, la operación de reunión ∧, que hemos visto, y el operador combinación, denotado como ∨, el cualproporciona el lub de dos elementos (que, por lo tanto, siempre deben existir en el lattice).Sólo hemos hablado sobre los “semi” lattices, en donde sólo existe uno de los operadoresde reunión y de combinación. Es decir, nuestros semi-lattices son semi-laticces de reunión.También podríamos hablar de los semi-lattices de combinación, en donde sólo existe eloperador de combinación, y de hecho cierta literatura sobre el análisis de los programasutiliza la notación de semi-lattices de combinación. Como la literatura tradicional sobreflujos de datos habla de los semi-lattices de reunión, también lo haremos en este libro.Maq. Cap_9_Aok.indd 621 11/10/07 1:02:05 AM622 Capítulo 9. Optimizaciones independientes de la máquinacuando haya otro camino de x a y a la izquierda en el diagrama. Por ende, aunque {d 1, d 2, d 3} ≤{d 1}, no dibujamos esta flecha ya que se representa mediante el camino a través de {d 1, d 2}, porejemplo.d1 d2 d1 d3 d2 d3d1 d2 d3 { } { } { }d1 d2 d3{ , } { , } { , }{} ( ){ , , } ( )Figura 9.22: Lattices de subconjuntos de definicionesTambién es útil observar que podemos leer el operador de reunión de dichos diagramas.Como x ∧ y es el glb, siempre es la z más alta para la cual hay caminos hacia abajo, desde xo y. Por ejemplo, si x es {d 1} y y es {d 2}, entonces z en la figura 9.22 es {d 1, d 2}, lo cual tienesentido, ya que el operador de reunión es la unión. El elemento superior aparecerá en la partesuperior del diagrama del lattice; es decir, hay un camino hacia abajo, desde 	 hacia cada elemento. De igual forma, el elemento inferior aparecerá en la parte inferior, con un camino haciaabajo desde cada elemento hacia ⊥.Laticces de productosMientras que la figura 9.22 sólo involucra tres definiciones, el diagrama de laticces de un programa común puede ser bastante extenso. El conjunto de valores del flujo de datos es el conjuntode potencia de las definiciones que, por lo tanto, contiene 2n elementos si hay n definiciones en elprograma. No obstante, el que una definición llegue a un programa o no es independiente de lacapacidad de alcance de las demás definiciones. Así, podemos expresar el lattice7 de definicionesen términos de un “lattice de productos”, creado a partir de un simple lattice para cada definición. Es decir, si sólo hubiera una definición de d en el programa, entonces el lattice tendría doselementos: {}, el conjunto vacío, que es el elemento superior, y {d}, que es el elemento inferior.De manera formal, podemos construir lattices de productos de la siguiente manera. Suponga que (A, ∧ A) y (B, ∧ B) son (semi) lattices. El lattice de productos para estos dos lattice sedefine de la siguiente manera:1. El domino del lattice de productos es A × B.7En esta explicación y de aquí en adelante, con frecuencia omitiremos el “semi”, ya que los lattices como el de estaexplicación tienen un operador de combinación o lub, aun cuando no lo utilizamos.Maq. Cap_9_Aok.indd 622 11/10/07 1:02:06 AM2. El operador de reunión ∧ para el lattice de productos se define de la siguiente forma. Si(a, b) y (a, b) son elementos del dominio del lattice de productos, entonces (a, b) ∧ (a, b ) = (a ∧ a, b ∧ b ). (9.19)Es simple expresar el orden parcial ≤ para el lattice de productos en términos de los órdenesparciales ≤A y ≤B para A y B (a, b) ≤ (a, b ) si y sólo si a ≤A a y b ≤B b. (9.20)Para ver por qué (9.20) sigue a partir de (9.19) debemos tener en cuenta que:(a, b) ∧ (a, b ) = (a ∧A a, b ∧B b ).Por lo tanto, podríamos preguntar ¿bajo qué circunstancias (a ∧A a, b ∧B b ) = (a, b)? Estoocurre exactamente cuando a ∧A a = a y b ∧B b = b. Pero estas dos condiciones son las mismas que a ≤A a y b ≤B b.El producto de los lattices es una operación asociativa, por lo que podemos demostrar quelas reglas (9.19) y (9.20) se extienden a cualquier número de lattices. Es decir, si recibimos loslattices (Ai, ∧i) para i = 1, 2,…, k, entonces el producto de todos los k lattices, en este orden,tiene el dominio A 1 × A2 × … × Ak , un operador de reunión definido por(a1, a2,…, a k ) ∧ (b1, b 2,…, b k ) = (a1 ∧1 b 1, a2 ∧2 b 2,…, a k ∧ k b k ).y un orden parcial definido por(a1, a2,…, a k ) ≤ (b1, b2,…, b k ) si y sólo si a i ≤ b i para todas las i.Altura de un semi-latticePodemos aprender algo acerca de la velocidad de convergencia de un algoritmo de análisis de flujode datos, si estudiamos la “altura” del semi-lattice asociado. Una cadena ascendente en un poset(V, ≤) es una secuencia en donde x1 < x2 < … < x n. La altura de un semi-lattice es el númeromás grande de relaciones < en cualquier cadena ascendente; es decir, la altura es uno menos queel número de elementos en la cadena. Por ejemplo, la altura del semi-lattice de definiciones dealcance para un programa con n definiciones es n.Es mucho más sencillo mostrar la convergencia de un algoritmo de flujo de datos iterativosi el semi-lattice tiene una altura finita. Es evidente que un lattice que consiste en un conjuntofinito de valores tendrá una altura finita; también es posible que un lattice con un número infinito de valores tenga una altura finita. El lattice que se utiliza en el algoritmo de propagaciónde constantes es uno de estos ejemplos que examinaremos más de cerca en la sección 9.4.9.3.2 Funciones de transferenciaLa familia de funciones de transferencia F : V → V en un marco de trabajo de un flujo de datostiene las siguientes propiedades:9.3 Fundamentos del análisis del f lujo de datos 623Maq. Cap_9_Aok.indd 623 11/10/07 1:02:07 AM624 Capítulo 9. Optimizaciones independientes de la máquina1. F tiene una función de identidad I, de tal forma que I(x ) = x para todas las x en V.2. F se cierra bajo la composición; es decir, para dos funciones f y g cualesquiera en F, lafunción h definida por h(x ) = g (f (x )) está en F.Ejemplo 9.21: En las definiciones de alcance, F tiene la identidad, la función en donde gen yeliminar son el conjunto vacío. El cierre bajo la composición se mostró de hecho en la sección9.2.4; aquí repetiremos el argumento brevemente. Suponga que tenemos dos funciones:f 1(x ) = G 1 ∪ (x − K 1) y f 2(x ) = G2 ∪ (x − K 2).Entonces:f 2(f 1(x )) = G2 ∪ ((G 1 ∪ (x − K 1)) − K 2).El lado derecho de la ecuación anterior es algebraicamente equivalente a:(G 2 ∪ (G 1 − K 2)) ∪ (x − (K 1 ∪ K 2)).Si hacemos que K = K 1 ∪ K 2 y G = G 2 ∪ (G 1 − K 2), entonces hemos demostrado que lacomposición de f 1 y f 2, que es f (x ) = G ∪ (x − K), es de la forma que la convierte en miembrode F. Si consideramos las expresiones disponibles, los mismos argumentos que se utilizan paralas definiciones de alcance también demuestran que F tiene una identidad y se cierra bajo lacomposición. ✷Marcos de trabajo monótonosPara crear un algoritmo iterativo para el trabajo del análisis de un flujo de datos, necesitamosque el marco de trabajo del flujo de datos cumpla con una condición más. Decimos que unmarco de trabajo es monótono si cuando aplicamos una función de transferencia f en F a dosmiembros de V, siendo el primero no mayor que el segundo, el primer resultado no es mayorque el segundo.De manera formal, el marco de datos de un flujo de datos (D, F, V, ∧) es monótono si:Para todas las x y y en V y f en F, x ≤ y implica que f (x ) ≤ f (y ). (9.22)De manera equivalente, la monotonicidad puede definirse como:Para todas las x y y en V y f en F, f (x∧y ) ≤ f (x ) ∧ f (y ). (9.23)La ecuación (9.23) dice que si tomamos la reunión de dos valores y después aplicamos f, elresultado nunca será mayor de lo que se obtiene al aplicar f a los valores, primero de maneraindividual y después “reuniendo” los resultados. Como las dos definiciones de monotonicidadparecen tan distintas, ambas son útiles. Descubriremos que una o la otra es más útil bajodistintas circunstancias. Más adelante realizaremos el bosquejo de una prueba para demostrarque, sin duda, son equivalentes.Primero vamos a suponer que se cumple la (9.22) y demostraremos que la (9.23) es válida.Como x ∧ y es el límite inferior más grande de x y y, sabemos que:x ∧ y ≤ x y x ∧ y ≤ y.Maq. Cap_9_Aok.indd 624 11/10/07 1:02:08 AMEn consecuencia, según (9.22),f (x ∧ y ) ≤ f (x ) y f (x ∧ y ) ≤ f (y ).Como f (x ) ∧ f (y ) es el límite inferior más grande de f (x ) y f (y ), tenemos a (9.23).Por el contrario, vamos a suponer que se cumple la (9.23) y demostraremos la (9.22). Suponemos que x ≤ y y usamos (9.23) para concluir que f (x ) ≤ f (y ), con lo cual se demuestra la(9.22). La ecuación (9.23) nos dice que:f (x ∧ y ) ≤ f (x ) ∧ f (y ).Pero como se supone que x ≤ y, x ∧ y = x, por definición. Así, la ecuación (9.23) dice que:f (x ) ≤ f (x ) ∧ f (y ).Como f (x ) ∧ f (y ) es el glb de f (x ) y f (y ), sabemos que f (x ) ∧ f (y ) ≤ f (y ). Por ende,f (x ) ≤ f (x ) ∧ f (y ) ≤ f (y )y la ecuación (9.23) implica a la (9.22).Marcos de trabajo distribuidosA menudo, un marco de trabajo obedece a una condición más fuerte que la (9.23), a lo cual lellamamos la condición distributiva,f (x ∧ y ) = f (x ) ∧ f (y )para todas las x y y en V, y f en F. Sin duda, si a = b, entonces a ∧ b = a por la idempotencia,así que a ≤ b. Por ende, la distributividad implica a la condición de monotonicidad, aunqueesto no es válido al revés.Ejemplo 9.24: Hagamos que y y z sean conjuntos de definiciones en el marco de trabajo delas definiciones de alcance. Hagamos que f sea una función definida por f (x ) = G ∪ (x − K)para algunos conjuntos de definiciones G y K. Podemos verificar que el marco de trabajo de lasdefiniciones de alcance cumpla con la distributividad, si comprobamos que:G ∪ ((y ∪ z) − K) = (G ∪ (y − K)) ∪ (G ∪ (z − K))Aunque la ecuación anterior puede parecer formidable, considere primero aquellas definicionesen G. Estas definiciones sin duda se encuentran en los conjuntos definidos tanto por el ladoizquierdo como por el derecho. Así, sólo tenemos que considerar las definiciones que no estánen G. En ese caso, podemos eliminar a G de todas partes, y verificar la siguiente igualdad:(y ∪ z) − K = (y − K) ∪ (z − K).Esta última igualdad puede comprobarse con facilidad, mediante el uso de un diagrama deVenn. ✷9.3 Fundamentos del análisis del f lujo de datos 625Maq. Cap_9_Aok.indd 625 11/10/07 1:02:09 AM626 Capítulo 9. Optimizaciones independientes de la máquina9.3.3 El algoritmo iterativo para los marcos de trabajo generalesPodemos generalizar el Algoritmo 9.11 para hacer que funcione en una gran variedad de problemas de flujo de datos.Algoritmo 9.25: Solución iterativa a los marcos de trabajo de flujo de datos generales.ENTRADA: Un marco de trabajo del flujo de datos con los siguientes componentes:1. Un grafo de flujo de datos, con nodos ENTRADA y SALIDA etiquetados en forma especial.2. Una dirección del flujo de datos D.3. Un conjunto de valores V.4. Un operador de reunión ∧.5. Un conjunto de funciones F, en donde fB en F es la función de transferencia para el bloque B.6. Un valor constante vENTRADA o vSALIDA en V, que representa la condición delimitadora paralos marcos de trabajo de avance e inversos, respectivamente.SALIDA: Valores en V para ENT[B ] y SAL[B ] para cada bloque B en el grafo de flujo dedatos.MÉTODO: Los algoritmos para resolver problemas de flujos de datos hacia delante y haciaatrás se muestran en las figuras 9.23(a) y 9.23(b), respectivamente. Al igual que con los algoritmos de flujo de datos iterativos familiares de la sección 9.2, calculamos ENT y SAL para cadabloque mediante la aproximación sucesiva. ✷Es posible escribir las versiones hacia delante y hacia atrás del algoritmo 9.25, de tal formaque una función que implemente el operador de reunión sea un parámetro, así como una función que implemente la función de transferencia para cada bloque. El mismo grafo de flujos yel valor delimitador son también parámetros. De esta forma, el implementador del compiladorpuede evitar tener que volver a codificar el algoritmo iterativo básico para cada marco de trabajo del flujo de datos utilizado por la fase de optimización del compilador.Podemos usar el marco de trabajo abstracto que hemos visto hasta ahora para probar varias propiedades útiles del algoritmo iterativo:1. Si el algoritmo 9.25 converge, el resultado es una solución a las ecuaciones del flujo dedatos.2. Si el marco de trabajo es monótono, entonces la solución encontrada es el punto fijomáximo (MFP) de las ecuaciones del flujo de datos. Un punto fijo máximo es una solución con la propiedad de que en cualquier otra solución, los valores de ENT[B ] y SAL[B ]son ≤ los valores correspondientes del MFP.3. Si el semi-lattice del marco de trabajo es monótono y de altura finita, entonces garantizaque el algoritmo va a converger.Maq. Cap_9_Aok.indd 626 11/10/07 1:02:09 AM1) SAL[ENTRADA] = v ENTRADA;2) for (cada bloque básico B que no sea ENTRADA) SAL[B] = 	;3) while (ocurran cambios a cualquier SAL)4) for (cada bloque básico B que no sea ENTRADA) {5) ENT[B] = ∧P un predecesor de B SAL[P];6) SAL[B] = fB(ENT[B]); }(a) algoritmo iterativo para un problema de flujo de datos hacia delante.1) ENT[SALIDA] = v SALIDA;2) for (cada bloque básico B que no sea SALIDA) ENT[B] = 	;3) while (ocurran cambios a cualquier ENT)4) for (cada bloque básico B que no sea SALIDA) {5) SAL[B] = ∧S un sucesor de B ENT[S];6) ENT[B] = fB(SAL[B]); }(b) Algoritmo iterativo para un problema de flujo de datos hacia atrás.Figura 9.23: Versiones hacia delante y hacia atrás del algoritmo iterativoVamos a argumentar estos puntos, asumiendo que el marco de trabajo es de avance. El casode los marcos de trabajo inversos es en esencia el mismo. La primera propiedad es fácil de demostrar. Si las ecuaciones no se satisfacen para cuando termina el ciclo while, entonces habrápor lo menos un cambio a un valor de SAL (en el caso de avance) o ENT (en el caso inverso), ydebemos recorrer el ciclo otra vez.Para demostrar la segunda propiedad, primero mostramos que los valores que recibenENT[B] y SAL[B] para cualquier B sólo pueden disminuir (en el sentido de la relación ¥ paralos lattices) a medida que el algoritmo itera. Esta afirmación puede demostrarse mediante lainducción.BASE: El caso base es para mostrar que el valor de ENT[B] y SAL[B] después de la primeraiteración no es mayor que el valor inicializado. Esta instrucción es trivial, ya que ENT[B] ySAL[B] para todos los bloques B ≠ ENTRADA se inicializan con 	.INDUCCIÓN: Suponga que después de la k-ésima iteración, ninguno de los valores es mayorque los que se obtienen después de la (k − 1)-ésima iteración, y muestran lo mismo para laiteración k + 1, comparada con la iteración k. La línea (5) de la figura 9.23(a) tiene:un predecesor deENT SALVamos a usar la notación ENT[B]i y SAL[B]i para denotar los valores de ENT[B] y SAL[B]después de la iteración i. Suponiendo que SAL[P]k ¥ SAL[P]k− 1, sabemos que ENT[B]k+ 1 ¥9.3 Fundamentos del análisis del f lujo de datos 627Maq. Cap_9_Aok.indd 627 11/10/07 1:02:10 AM628 Capítulo 9. Optimizaciones independientes de la máquinaENT[B]k, debido a las propiedades del operador de reunión. A continuación, la línea (6) diceque:SAL[B ] = fB(ENT[B ]).Como ENT[B ]k+1 ≤ ENT[B ]k, tenemos que SAL[B ]k+1 ≤ SAL[B ]k por la monotonicidad.Hay que tener en cuenta que cualquier cambio que se observe para los valores de ENT[B ] ySAL[B ] es necesario para satisfacer la ecuación. Los operadores de reunión devuelven el límiteinferior más grande de sus entradas, y las funciones de transferencia devuelven la única soluciónconsistente con el bloque en sí y su entrada dada. Por ende, si el algoritmo iterativo termina, elresultado debe tener valores que sean por lo menos tan grandes como los valores correspondientesen cualquier otra solución; es decir, el resultado del Algoritmo 9.25 es el MFP de la ecuación.Por último, considere el tercer punto, en donde el marco de trabajo del flujo de datos tieneuna altura finita. Como todos los valores de los conjuntos ENT[B ] y SAL[B ] disminuyen concada cambio, y el algoritmo se detiene si en cierta ronda no cambia nada, se garantiza que elalgoritmo convergerá después de una cantidad de rondas no mayor que el producto de la alturadel marco de trabajo y el número de nodos del grafo de flujo.9.3.4 Significado de una solución de un flujo de datosAhora sabemos que la solución que se encuentra usando el algoritmo iterativo es el punto fijomáximo, pero ¿qué representa el resultado, desde un punto de vista de la semántica del programa? Para comprender la solución del marco de trabajo de un flujo de datos (D, F, V, ∧), primerovamos a describir lo que sería una solución ideal para el marco de trabajo. Demostraremos que,en general, no puede obtenerse la solución ideal, pero que el Algoritmo 9.25 se aproxima al idealde una manera conservadora.La solución idealSin perder la generalidad, vamos a suponer por ahora que el marco de trabajo del flujo de datos de interés es un problema que fluye hacia adelante. Considere el punto de entrada de un bloque básico B. La solución ideal empieza por encontrar todos los caminos de ejecución posibles, queconducen desde la entrada del programa hasta el inicio de B. Un camino es “posible” sólo si hayalgún cálculo del programa que siga exactamente ese camino. Entonces, la solución ideal seríacalcular el valor del flujo de datos al final de cada posible camino y aplicar el operador de reunióna estos valores para encontrar su límite inferior más grande. Así, ninguna ejecución del programapuede producir un valor más pequeño para ese punto del programa. Además, el límite es estrecho;no hay un valor mayor del flujo de datos que un glb para el valor que se calcula a lo largo de cadaposible camino hacia B en el grafo de flujo.Ahora trataremos de definir la solución ideal con más formalidad. Para cada bloque B en ungrafo de flujo, hagamos que fB sea la función de transferencia para B. Considere cualquier caminoP = ENTRADA → B1 → B2 → … → B k −1 → B kdesde el nodo inicial ENTRADA hasta cierto bloque Bk. El camino del programa puede tener ciclos, por lo que un bloque básico puede aparecer varias veces en el camino P. Defina la funciónMaq. Cap_9_Aok.indd 628 11/10/07 1:02:11 AMde transferencia para P, fP, como la composición de fB1, fB2 …, fBk−1. Observe que fBk no formaparte de la composición, lo cual refleja el hecho de que este camino se toma para llegar al iniciodel bloque Bk, no a su final. El valor del flujo de datos creado al ejecutar este camino es, por lotanto, fP (v ENTRADA), en donde v ENTRADA es el resultado de la función de transferencia constanteque representa al nodo inicial ENTRADA. El resultado ideal para el bloque B es por ende: IDEAL [B ] = ∧ f P (v ENTRADA).P, un posible camino desde ENTRADA hasta BAfirmamos que, en términos del orden parcial ≤ teórico del lattice para el marco de trabajoen cuestión,• Cualquier respuesta que sea mayor que IDEAL es incorrecta.• Cualquier valor más pequeño o igual que el ideal es conservador; es decir, seguro.Por intuición, entre más cercano sea el valor al ideal, será más preciso.8 Para ver por qué lassoluciones deben ser ≤ que la solución ideal, observe que cualquier solución más grande queIDEAL para cualquier bloque podría obtenerse ignorando cierto camino de ejecución que elprograma pudiera tomar, y no podemos estar seguros que no haya cierto efecto a lo largo deese camino para invalidar cualquier mejora al programa que podríamos realizar, con base en lasolución más grande. Por el contrario, cualquier solución menor que IDEAL puede verse comola inclusión de ciertos caminos que, o no existen en el grafo de flujo, o que sí existen pero queel programa nunca podrá seguir. La solución menor sólo permitiría transformaciones que seancorrectas para todas las posibles ejecuciones del programa, pero puede prohibir ciertas transformaciones que IDEAL permitiría.La solución de reunión sobre los caminos (Meet-Over-Paths, MOP)Sin embargo, como vimos en la sección 9.1, el proceso de buscar todos los posibles caminos deejecución es indecidible. Por lo tanto, debemos aproximar. En la abstracción del flujo de datos,suponemos que puede tomarse cada camino en el grafo de flujo. Por ende, podemos definir lasolución de reunión sobre los caminos para B de la siguiente forma: MOP[B ] = ∧ fP (v ENTRADA).P, un camino desde ENTRADA hasta BObserve que, en lo que respecta a IDEAL, la solución MOP[B ] proporciona valores para ENT[B ]en los marcos de trabajo con flujo hacia delante. Si consideramos los marcos de trabajo conflujo hacia atrás, entonces tenemos que considerar a MOP[B ] como un valor para SAL[B ].Los caminos que se consideran en la solución MOP son un súper conjunto de todos los caminos que es posible ejecutar. Por ende, la solución MOP reúne no sólo los valores del flujo dedatos de todos los caminos ejecutables, sino también los valores adicionales asociados con loscaminos que no es posible ejecutar. Si tomamos la reunión de la solución ideal más los términos9.3 Fundamentos del análisis del f lujo de datos 6298Observe que en los problemas hacia delante, el valor IDEAL [B ] es lo que quisiéramos que fuera ENT[B ]. En losproblemas hacia atrás, que no veremos aquí, definiríamos a IDEAL [B ] como el valor ideal de SAL[B ].Maq. Cap_9_Aok.indd 629 11/10/07 1:02:11 AM630 Capítulo 9. Optimizaciones independientes de la máquinaadicionales, no podemos crear una solución más grande que la ideal. Así, para todos los valoresde B tenemos que MOP[B ] ≤ IDEAL [B ], y simplemente diremos que MOP ≤ IDEAL.Comparación entre el punto fijo máximo y la solución MOPObserve que en la solución MOP, el número de caminos considerados sigue siendo ilimitado siel grafo de flujo contiene ciclos. Por ende, la definición MOP no se presta para un algoritmodirecto. Sin duda, el algoritmo iterativo no busca primero todos los caminos que conducen a unbloque básico antes de aplicar el operador de reunión.En vez de ello,1. El algoritmo iterativo visita los bloques básicos, no necesariamente en el orden de ejecución.2. En cada punto de confluencia, el algoritmo aplica el operador de reunión a los valores delflujo de datos que se han obtenido hasta ese momento. Algunos de estos valores utilizadosse introdujeron de manera artificial en el proceso de inicialización, lo cual no representa elresultado de ninguna ejecución desde el inicio del programa.Entonces, ¿cuál es la relación entre la solución MOP y la solución del punto fijo máximo producida por el Algoritmo 9.25?Primero hablaremos sobre el orden en el cual se visitan los nodos. En una iteración, podemos visitar un bloque básico antes de haber visitado a sus predecesores. Si el predecesor esel nodo ENTRADA, SAL[ENTRADA] ya se habría inicializado con el valor constante apropiado.De no ser así, significa que ya se ha inicializado con 	, un valor que no es más pequeño quela respuesta final. Por la condición de monotonía, el resultado que se obtiene al usar 	 comoentrada no es más pequeño que la solución deseada. En un sentido, podemos considerar que 	no representa ninguna información.B BBBENTRADA2341Figura 9.24: Grafo de flujo que ilustra el efecto de la reunión anticipada sobre los caminos¿Cuál es el efecto de aplicar el operador de reunión de manera anticipada? Considere elejemplo simple de la figura 9.24, y suponga que nos interesa el valor de ENT[B 4]. Por la definición de MOP,Maq. Cap_9_Aok.indd 630 11/10/07 1:02:12 AMMOP[B 4] = ((fB3 ° fB1) ∧ (fB3 ° fB2))(v ENTRADA)En el algoritmo iterativo, si visitamos los nodos en el orden B1, B2, B3, B4, entoncesENT[B 4] = fB3((fB1(v ENTRADA) ∧ fB2(v ENTRADA)))Mientras que el operador de reunión se aplica al final en la definición del MOP, el algoritmoiterativo lo aplica antes. La respuesta es la misma, sólo si el marco de trabajo del flujo de datoses distributivo. Si el marco de trabajo del flujo de datos es monótono pero no distributivo, aúntenemos que ENT[B 4] ≤ MOP[B 4]. Recuerde que, en general, una solución ENT[B ] es segura(conservadora) si ENT[B ] ≤ IDEAL [B ] para todos los bloques B. Con seguridad, MOP[B ] ≤IDEAL [B ].Ahora vamos a proporcionar un breve bosquejo de por qué en general la solución de puntofijo máximo que proporciona el algoritmo iterativo siempre es segura. Una inducción sencillasobre i muestra que los valores que se obtienen después de i iteraciones son más pequeños oiguales que la reunión sobre todos los caminos de longitud i o menor. Pero el algoritmo iterativosólo termina si llega a la misma respuesta que se obtendría al iterar un número ilimitado deveces. Por ende, el resultado no es mayor que la solución MOP. Como MOP ≤ IDEAL y MFP ≤MOP, sabemos que MFP ≤ IDEAL y, por lo tanto, la solución de punto fijo máximo proporcionada por el algoritmo iterativo es segura.9.3.5 Ejercicios para la sección 9.3Ejercicio 9.3.1: Construya un diagrama de lattices para el producto de tres lattices, cada unode ellos basado en una sola definición d i, para i = 1, 2, 3. ¿Cómo se relaciona su diagrama delattices con el de la figura 9.22?Ejercicio 9.3.2: En la sección 9.3.3 argumentamos que si el marco de trabajo tiene una alturafinita, entonces el algoritmo iterativo converge. He aquí un ejemplo en donde el marco de trabajo no tiene una altura finita, y el algoritmo iterativo no converge. Hagamos que el conjuntode valores V sea los números reales no negativos, y que el operador de reunión sea el mínimo.Hay tres funciones de transferencia: i. La identidad, f I(x ) = x. ii. “mitad”; es decir, la función fM (x ) = x/2.iii. “uno”; es decir, la función fU (x ) = 1.El conjunto de funciones de transferencia F es estas tres funciones, más las funciones formadasal componerlas en todas las formas posibles.a) Describa el conjunto F.b) ¿Cuál es la relación ≤ para este marco de trabajo?9.3 Fundamentos del análisis del f lujo de datos 631!Maq. Cap_9_Aok.indd 631 11/10/07 1:02:13 AM632 Capítulo 9. Optimizaciones independientes de la máquinac) Proporcione un ejemplo de un grafo de flujo con funciones de transferencia asignadas, demanera que el Algoritmo 9.25 no converja.d) ¿Este marco de trabajo es monótono? ¿Es distributivo?Ejercicio 9.3.3: Argumentamos que el Algoritmo 9.25 converge si el marco de trabajo es monótono y de altura finita. He aquí un ejemplo de un marco de trabajo que muestra que la monotonicidad es esencial; no basta con una altura finita. El dominio V es {1, 2}, el operador de reuniónes mín, y el conjunto de funciones F es sólo la función identidad (f I) y la función “intercambio”(f S (x ) = 3 − x ), que intercambia 1 y 2.a) Muestre que este marco de trabajo es de altura finita, pero no monótono.b) Proporcione un ejemplo del grafo de flujo y la asignación de funciones de transferencia,de manera que el Algoritmo 9.25 no converja.Ejercicio 9.3.4: Hagamos que MOPi [B ] sea la reunión sobre todos los caminos de longitud io menores, desde la entrada hasta el bloque B. Demuestre que después de i iteraciones del Algoritmo 9.25, ENT[B ] ≤ MOPi [B ]. Además, demuestre que como consecuencia, si el Algoritmo9.25 converge, entonces convergerá en algo que sea ≤ que la solución MOP.Ejercicio 9.3.5: Suponga que el conjunto F de funciones para un marco de trabajo consistesólo en funciones de la forma gen-eliminar. Es decir, el dominio V es el conjunto de potenciade cierto conjunto, y f (x ) = G ∪ (x − K) para ciertos conjuntos G y K. Demuestre que si eloperador de reunión es (a) la unión o (b) la intersección, entonces el marco de trabajo es distributivo.9.4 Propagación de constantesTodos los esquemas del flujo de datos que vimos en la sección 9.2 son en realidad ejemplossimples de marcos de trabajo distributivos con una altura finita. Por ende, el Algoritmo 9.25iterativo se aplica a ellos, ya sea en su versión hacia delante o hacia atrás, y produce la soluciónMOP en cada caso. En esta sección, examinaremos con detalle un útil marco de trabajo del flujode datos con propiedades más interesantes.Recuerde que la propagación de constantes, o “cálculo previo de constantes”, sustituye a lasexpresiones que se evalúan con la misma constante cada vez que se ejecutan, por esa constante.El marco de trabajo de propagación de constantes que describiremos a continuación es distintode todos los problemas de flujos de datos que hemos visto hasta ahora, en cuanto a que:a) Tiene un conjunto ilimitado de posibles valores del flujo de datos, incluso para un grafode flujo fijo.b) No es distributivo.La propagación de constantes es un problema del flujo de datos hacia delante. El semi-lattice que representa los valores del flujo de datos y la familia de funciones de transferencia sepresentan a continuación.!!!Maq. Cap_9_Aok.indd 632 11/10/07 1:02:13 AM9.4.1 Valores del flujo de datos para el marco de trabajode propagación de constantesEl conjunto de valores del flujo de datos es un lattice de productos, con un componente paracada variable en un programa. El lattice para una sola variable consiste en lo siguiente:1. Todas las constantes apropiadas para el tipo de la variable.2. El valor NAC, que representa “no es una constante”. Una variable se asigna a este valor sise determina que no tiene un valor constante. La variable puede haber recibido un valorde entrada, o se puede derivar de una variable que no sea una constante, o se le puedenhaber asignado distintas constantes a lo largo de diferentes caminos que conducen haciael mismo punto en el programa.3. El valor UNDEF, que representa “indefinido”. Una variable recibe este valor si todavía nohay nada que pueda afirmarse; presuntamente, no se ha descubierto una definición de lavariable para llegar al punto en cuestión.Observe que NAC y UNDEF no son iguales; en esencia son valores opuestos. NAC indica quehemos visto muchas formas en las que podría definirse una variable, por lo cual sabemos que noes constante; UNDEF indica que hemos visto muy poco sobre la variable, por lo cual no podemossaber nada certero.El semi-lattice para una variable típica con valor entero se muestra en la figura 9.25. Aquíel elemento superior es UNDEF, y el elemento inferior es NAC. Es decir, el valor más grande en elorden parcial es UNDEF y el menor es NAC. Los valores constantes están desordenados, pero sonmenores que UNDEF y mayores que NAC. Como vimos en la sección 9.3.1, la reunión de dos valoreses su límite menor más grande. Así, para todos los valores v,UNDEF ∧ v = v y NAC ∧ v = NAC.Para cualquier constante c,c ∧ c = cy dadas dos constantes distintas c 1 y c 2,c 1 ∧ c 2 = NAC.Un valor del flujo de datos para este marco de trabajo es un mapa que va desde cada variable en el programa, hacia uno de los valores en el semi-lattice de constantes. El valor de unavariable v en un mapa m se denota mediante m(v ).9.4.2 La reunión para el marco de trabajo de propagaciónde constantesEl semi-lattice de valores del flujo de datos es sólo el producto de los semi- lattices como lafigura 9.25, uno para cada variable. Por ende, m ≤ m si y sólo si para todas las variables v,tenemos que m(v ) ≤ m(v ). Dicho de otra forma, m ∧ m = m si m(v ) = m(v ) ∧ m(v ) paratodas las variables v.9.4 Propagación de constantes 633Maq. Cap_9_Aok.indd 633 11/10/07 1:02:14 AM634 Capítulo 9. Optimizaciones independientes de la máquinaï3 ï2 ï10 1 . . .NACUNDEF2 3 . . .Figura 9.25: Semi-lattice que representa a los posibles “valores” de una sola variable entera9.4.3 Funciones de transferencia para el marco de trabajode propagación de constantesA continuación vamos a suponer que un bloque básico contiene sólo una instrucción. Las funciones de transferencia para los bloques básicos que contienen varias instrucciones pueden construirse mediante la composición de las funciones correspondientes a las instrucciones individuales.El conjunto F consiste en ciertas funciones de transferencia que aceptan un mapa de variablesasignadas a valores en el semi-lattice de constantes, y devuelven otro mapa de ese tipo.F contiene la función de identidad, la cual recibe un mapa como entrada y devuelve esemismo mapa como salida. F también contiene la función de transferencia de constantes parael nodo ENTRADA. Esta función de transferencia, que recibe un mapa de entrada, devuelve unmapa m0, en donde m0(v ) = UNDEF para todas las variables v. Esta condición delimitadoratiene sentido, ya que antes de ejecutar cualquier instrucción del programa no hay definicionespara ninguna variable.En general, hagamos que f s sea la función de transferencia de la instrucción s, y hagamos quem y m representen valores del flujo de datos de tal forma que m = f(m). Describiremos a f sen términos de la relación entre m y m.1. Si s no es una instrucción de asignación, entonces f s es simplemente la función de identidad.2. Si s es una asignación para la variable x, entonces m(v ) = m(v ) para todas las variablesv ≠ x, siempre y cuando sea válida una de las siguientes condiciones: (a) Si el lado derecho (Right-Hand-Side, RHS) de la instrucción s es una constante c,entonces m(x ) = c. (b) Si el RHS es de la forma y + z, entonces9m(x ) = m(y) + m(z ) si m(y) y m(z ) son valores constantesNAC si m(y) o m(z ) es NACUNDEF en cualquier otro caso (c) Si el RHS es cualquier otra expresión (por ejemplo, una llamada a una función o unaasignación a través de un apuntador), entonces m(x ) = NAC.9Como siempre, + representa a un operador genérico, no necesariamente la suma.Maq. Cap_9_Bok.indd 634 11/10/07 1:03:12 AM9.4.4 Monotonía en el marco de trabajo de propagaciónde constantesVamos a demostrar que el marco de trabajo de propagación constante es monótono. Primero,podemos considerar el efecto de una función fs en una sola variable. En todos los casos exceptoel 2(b), f s no modifica el valor de m(x ), o cambia el mapa para regresar una constante o NAC.En estos casos, f s debe sin duda ser monótona.Para el caso 2(b), el efecto de f s se tabula en la figura 9.26. Las columnas primera y segundarepresentan los posibles valores de entrada de y y z; la última representa el valor de salida de x.Los valores se ordenan del más grande al más pequeño en cada columna o subcolumna. Paramostrar que la función es monótona, comprobamos que para cada posible valor de entrada de y,el valor de x no se haga más grande a medida que el valor de z se hace más pequeño. Por ejemplo,en el caso en donde y tiene un valor constante c1, a medida que varía el valor de z desde UNDEFhasta c2, y luego hasta NAC, el valor de x varía desde UNDEF, hasta c1 + c2, y después hastaNAC, respectivamente. Podemos repetir este procedimiento para todos los valores posibles de y.Debido a la simetría, ni siquiera tenemos que repetir el procedimiento para el segundo operando,para poder concluir que el valor de salida no puede hacerse más grande a medida que la entradase hace más pequeña.Figura 9.26: La función de transferencia de propagación de constantes para x = y+z9.4.5 La distributividad del marco de trabajo de propagaciónde constantesEl marco de trabajo de propagación de constantes es monótono según su definición, pero no distributivo. Es decir, la solución iterativa de punto fijo máximo es segura, pero puede ser más pequeña que la solución MOP. Un ejemplo demostrará que el marco de trabajo no es distributivo.Ejemplo 9.26: En el programa de la figura 9.27, x y y se establecieron a 2 y 3 en el bloqueB1, y a 3 y 2, respectivamente, en el bloque B 2. Sabemos que sin importar qué camino se tome,el valor de z al final del bloque B 3 es 5. Sin embargo, el algoritmo iterativo no descubre estehecho. En lugar de eso, aplica el operador de reunión a la entrada de B 3, y obtiene NACs como9.4 Propagación de constantes 635Maq. Cap_9_Bok.indd 635 11/10/07 1:03:17 AM636 Capítulo 9. Optimizaciones independientes de la máquinax = 2y = 3 y = 2x = 3B BB1 23SALIDAz = x+yFigura 9.27: Un ejemplo que demuestra que el marco de trabajo de propagación de constantesno es distributivovalores para x y y. Como la suma de dos NACs produce un NAC, la salida producida por elAlgoritmo 9.25 es que z = NAC al final del programa. El resultado es seguro, pero impreciso.El Algoritmo 9.25 es impreciso, ya que no lleva el registro de la correlación de cada vez quex es 2, y es 3, y viceversa. Es posible, pero considerablemente más costoso, usar un marco detrabajo más complejo que rastree todas las igualdades posibles que sean válidas entre paresde expresiones que involucren a las variables en el programa; en el ejercicio 9.4.2 hablaremossobre este método.En teoría, podemos atribuir esta pérdida de precisión a la no distributividad del marco detrabajo de propagación de constantes. Hagamos que f 1, f 2 y f 3 sean las funciones de transferenciaque representan a los bloques B1, B 2 y B 3, respectivamente. Como se muestra en la figura 9.28,f 3 (f1 (m0) ∧ f2 (m0)) < f3 (f 1 (m0)) ∧ f3 (f 2 (m0))con lo cual el marco de trabajo se convierte en no distributivo. ✷Figura 9.28: Ejemplo de funciones de transferencia no distributivasMaq. Cap_9_Bok.indd 636 11/10/07 1:03:18 AM9.4.6 Interpretación de los resultadosEl valor UNDEF se utiliza en el algoritmo iterativo para dos fines: inicializar el nodo ENTRADAe inicializar los puntos interiores del programa antes de las iteraciones. El significado es unpoco distinto en los dos casos. El primero dice que las variables están indefinidas al inicio dela ejecución del programa; el segundo dice que por falta de información al inicio del procesoiterativo, aproximamos la solución con el elemento superior UNDEF. Al final del proceso iterativo, las variables a la salida del nodo ENTRADA seguirán conteniendo el valor UNDEF, ya queSAL[ENTRADA] nunca cambia.Es posible que puedan aparecer valores UNDEF en algunos otros puntos del programa.Cuando lo hacen, significa que no se han observado definiciones para esa variable a lo largode cualquiera de los caminos que conducen hasta ese punto del programa. Observe que con laforma en que definimos el operador de reunión, siempre y cuando exista un camino que definauna variable que llegue a un punto del programa, la variable no tendrá un valor UNDEF. Sitodas las definiciones que llegan a un punto del programa tienen el mismo valor constante, lavariable se considera una constante, aun cuando tal vez no esté definida a lo largo de ciertocamino del programa.Al suponer que el programa es correcto, el algoritmo puede encontrar más constantes quede otro modo no lo haría. Es decir, el algoritmo elige de manera conveniente ciertos valorespara aquellas variables que posiblemente están indefinidas, para que el programa pueda sermás eficiente. El cambio es legal en la mayoría de los lenguajes de programación, ya que se permite que las variables indefinidas tomen cualquier valor. Si la semántica del lenguaje requiereque todas las variables indefinidas reciban cierto valor específico, entonces debemos cambiarnuestra formulación del problema de manera acorde. Y si en vez de ello nos interesa buscar lasvariables que quizá estén indefinidas en un programa, podemos formular un análisis de flujo dedatos distinto para proveer ese resultado (vea el ejercicio 9.4.1).Ejemplo 9.27: En la figura 9.29, los valores de x son 10 y UNDEF en la salida de los bloquesbásicos B 2 y B 3, respectivamente. Como UNDEF ∧ 10 = 10, el valor de x es 10 al entrar al bloque B 4. Por ende, el bloque B 5, en donde se usa x, puede optimizarse al sustituir x por 10. Siel camino ejecutado hubiera sido B1 → B3 → B4 → B5, el valor de x al llegar al bloque básicoB 5 hubiera sido indefinido. Por lo tanto, parece incorrecto sustituir el uso de x por 10.No obstante, si es imposible que el predicado Q sea falso mientras que Q es verdadero, entonces este camino de ejecución nunca ocurrirá. Aunque el programador puede estar conscientede este hecho, el poder determinarlo puede estar más allá de la capacidad de cualquier análisis deflujo de datos. Por ende, si suponemos que el programa es correcto y que todas las variablesestán definidas antes de usarlas, sin duda es correcto que el valor de x al inicio del bloque básicoB 5 sólo puede ser 10. Y si, para empezar, el programa es incorrecto, entonces elegir 10 como elvalor de x no puede ser peor que permitir que x asuma algún valor aleatorio. ✷9.4.7 Ejercicios para la sección 9.4Ejercicio 9.4.1: Suponga que deseamos detectar todas las posibilidades de que una variableesté sin inicializar a lo largo de cualquier camino, hasta un punto en el que se utilice. ¿Cómomodificaría el marco de trabajo de esta sección para detectar dichas situaciones?9.4 Propagación de constantes 637!Maq. Cap_9_Bok.indd 637 11/10/07 1:03:20 AM638 Capítulo 9. Optimizaciones independientes de la máquinaif Q gotox = 10if Q· goto= xBB25BBBBBBB12 345 67Figura 9.29: Reunión de UNDEF y una constanteEjercicio 9.4.2: Un marco de trabajo de análisis del flujo de datos interesante y poderoso seobtiene al imaginar que el dominio V consiste en todas las posibles particiones de expresiones,de tal forma que dos expresiones se encuentren en la misma clase si, y sólo si es seguro quetendrán el mismo valor a lo largo de cualquier camino hacia el punto en cuestión. Para evitar tener que presentar una infinidad de expresiones, podemos representar a V mencionando sólo lospares mínimos de expresiones equivalentes. Por ejemplo, si ejecutamos las siguientes instrucciones:a = bc = a + dentonces el conjunto mínimo de equivalencias es {a ≡ b, c ≡ a + d}. A éstas les siguen otrasequivalencias, como c ≡ b + d y a + e ≡ b + e, pero no hay necesidad de presentarlas en forma explícita.a) ¿Cuál es el operador de reunión apropiado para este marco de trabajo?b) Proporcione una estructura de datos para representar valores de dominio y un algoritmopara implementar el operador de reunión.c) ¿Cuáles son las funciones apropiadas para asociar con instrucciones? Explique el efectoque una instrucción como a = b+c debería tener sobre una partición de expresiones (esdecir, sobre un valor en V).d) ¿Este marco de trabajo es monótono? ¿distributivo?!!Maq. Cap_9_Bok.indd 638 11/10/07 1:03:20 AM9.5 Eliminación de redundancia parcialEn esta sección, consideraremos con detalle cómo minimizar el número de evaluaciones deexpresiones. Es decir, queremos considerar todas las secuencias posibles de ejecución en ungrafo de flujo, y analizar el número de veces que se evalúa una expresión como x + y. Aldesplazarnos alrededor de los lugares en los que x + y se evalúa, manteniendo el resultado enuna variable temporal siempre que sea necesario, a menudo podemos reducir el número deevaluaciones de esta expresión a lo largo de muchas de los caminos de ejecución, sin incrementar al mismo tiempo ese número a lo largo de cualquier otro camino. Observe que puedeaumentar el número de lugares distintos en el grafo de flujo en donde x + y se evalúa, peroeso casi no tiene importancia, siempre y cuando se reduzca el número de evaluaciones de laexpresión x + y.Al aplicar la transformación de código aquí desarrollada, se mejora el rendimiento delcódigo resultante ya que, como veremos, nunca se aplica una operación a menos que sea absolutamente necesario. Cada compilador optimizador implementa algo parecido a la transformación aquí descrita, incluso si utiliza un algoritmo menos “agresivo” que el de esta sección.No obstante, existe otra motivación para hablar sobre el problema. Para encontrar el lugar olugares adecuados en el grafo de flujo, en donde se debe evaluar cada expresión, se requierencuatro tipos distintos de análisis del flujo de datos. Por ende, el estudio de la “eliminaciónde redundancia parcial”, como se le llama al proceso de minimizar el número de expresiones deevaluación, mejorará nuestra comprensión del papel que juega el análisis del flujo de datos enun compilador.La redundancia en los programas existe en varias formas. Como vimos en la sección 9.1.4,puede existir en la forma de subexpresiones comunes, en donde varias evaluaciones de la expresión producen el mismo valor. También puede existir en la forma de una expresión invariantede ciclo, que se evalúe con el mismo valor en cada iteración del ciclo. La redundancia tambiénpuede ser parcial, si se encuentra a lo largo de algunas de los caminos, aunque no necesariamente a lo largo de todas ellas. Podemos ver a las subexpresiones comunes y las expresionesinvariantes de ciclo como casos especiales de redundancia parcial; así, podemos idear un soloalgoritmo de eliminación de redundancia parcial para eliminar todas las diversas formas deredundancia.A continuación, primero hablaremos sobre las distintas formas de redundancia, para poderdesarrollar nuestra intuición acerca del problema. Después describiremos el problema de eliminación de redundancia generalizado, y por último presentaremos el algoritmo. Este algoritmoes en especial interesante, ya que implica resolver varios problemas de flujo de datos, tanto endirección hacia delante como hacia atrás.9.5.1 Los orígenes de la redundanciaLa figura 9.30 ilustra las tres formas de redundancia: subexpresiones comunes, expresionesinvariantes de ciclo y expresiones con redundancia parcial. La figura muestra el código, antes ydespués de cada optimización.9.5 Eliminación de redundancia parcial 639Maq. Cap_9_Bok.indd 639 11/10/07 1:03:21 AM640 Capítulo 9. Optimizaciones independientes de la máquinaa = b+ct = b+ca = t(b)2 B B 32 B B 3BB142 BBB142 Ba = b+cd = b+ct = b+ca = tt = b+cd = t(c)b = 7BBB134d = b+c(a)t = b+ca = tb = 7t = b+cd = te = tBBB134a = b+ce = b+cFigura 9.30: Ejemplos de (a) subexpresión común global, (b) movimiento de código invariantede ciclo, (c) eliminación de redundancia parcialSubexpresiones comunes globalesEn la figura 9.30(a), la expresión b + c calculada en el bloque B 4 es redundante; ya se ha evaluadopara cuando el flujo de control llega a B 4, sin importar el camino que se tome para llegar ahí.Como podemos observar en este ejemplo, el valor de la expresión puede ser diferente en los distintos caminos. Podemos optimizar el código, almacenando el resultado de los cálculos de b + c enlos bloques B 2 y B 3 en la misma variable temporal, por decir t, y después asignando el valor det a la variable e en el bloque B 4, en vez de evaluar de nuevo la expresión. Si hubiera una asignación a b o c después del último cálculo de b + c pero antes del bloque B 4, la expresión en elbloque B 4 no sería redundante.De manera formal, decimos que una expresión b + c es (completamente) redundante en elpunto p, si es una expresión disponible, en el sentido de la sección 9.2.6, en ese punto. Es decir,la expresión b + c se ha calculado a lo largo de todos los caminos que llegan a p, y las variablesb y c no se redefinieron después de evaluar la última expresión. Esta última condición es necesaria, ya que aun cuando la expresión b + c se ejecuta textualmente antes de llegar al punto p,el valor de b + c calculado en el punto p hubiera sido distinto, debido a que los operandospodrían haber cambiado.Maq. Cap_9_Bok.indd 640 11/10/07 1:03:22 AMExpresiones invariantes de cicloLa figura 9.30(b) muestra un ejemplo de una expresión invariante de ciclo. La expresión b + ces una invariante de ciclo, suponiendo que ninguna de las variables b o c se redefinen dentro delciclo. Podemos optimizar el programa al sustituir todas las reejecuciones en un ciclo medianteun solo cálculo fuera del mismo. Asignamos el cálculo a una variable temporal, por decir t, ydespués sustituimos la expresión en el ciclo por t. Hay un punto más que debemos considerar alrealizar optimizaciones de “movimiento de código” como éstas. No debemos ejecutar ningunainstrucción que no se hubiera ejecutado sin la optimización. Por ejemplo, si es posible salir delciclo sin ejecutar la instrucción invariante de ciclo, entonces no debemos mover la instrucciónfuera del ciclo. Hay dos razones.1. Si la instrucción genera una excepción, entonces al ejecutarla se puede producir unaexcepción que no hubiera ocurrido en el programa original.2. Cuando el ciclo sale antes de tiempo, el programa “optimizado” requiere más tiempo queel programa original.Para asegurar que las expresiones invariantes de ciclo en los ciclos while puedan optimizarse, por lo general, los compiladores representan la siguiente instrucción:while c { S;}Búsqueda de subexpresiones comunes “profundas”El uso del análisis de las expresiones disponibles para identificar expresiones redundantes sólo funciona para las expresiones que son textualmente idénticas. Por ejemplo, unaaplicación de la eliminación de subexpresiones comunes reconocerá que t1 en el siguientefragmento de código:t1 = b + c; a = t1 + d;tiene el mismo valor que t2 en:t2 = b + c; e = t2 + d;siempre y cuando las variables b y c no se hayan redefinido entre una expresión y la otra.Sin embargo, no reconoce que a y e también son iguales. Es posible encontrar dichassubexpresiones comunes “profundas” al aplicar de nuevo la eliminación de subexpresiones comunes hasta que no se encuentren subexpresiones nuevas en una ronda. Tambiénes posible utilizar el marco de trabajo del ejercicio 9.4.2 para atrapar las subexpresionescomunes profundas.9.5 Eliminación de redundancia parcial 641Maq. Cap_9_Bok.indd 641 11/10/07 1:03:23 AM642 Capítulo 9. Optimizaciones independientes de la máquinade la misma forma que la siguiente instrucción:if c { repeat S; until not c;}De esta forma, las expresiones invariantes de ciclo pueden colocarse justo antes de la construcción repeat-until.A diferencia de la eliminación de subexpresiones comunes, en donde el cálculo de una expresión redundante simplemente se descarta, la eliminación de expresiones invariantes de ciclorequiere que una expresión del interior del ciclo se mueva fuera del mismo. Por ende, esta optimización se conoce como “movimiento de código invariante de ciclo”. Tal vez haya que repetirel movimiento de código invariante de ciclo, ya que una vez que se determina que una variabletiene un valor invariante de ciclo, las expresiones que utilizan dicha variable también puedenconvertirse en invariantes de ciclo.Expresiones parcialmente redundantesEn la figura 9.30(c) se muestra un ejemplo de una expresión parcialmente redundante. La expresión b + c en el bloque B 4 es redundante en el camino B1 → B2 → B4, pero no en el caminoB1 → B3 → B4. Podemos eliminar la redundancia en el camino anterior, colocando un cálculode b + c en el bloque B 3. Todos los resultados de b + c se escriben en una variable temporal t,y el cálculo en el bloque B 4 se sustituye con t. Así, al igual que el movimiento de código invariante de ciclo, la eliminación de redundancia parcial requiere la colocación de nuevos cálculosde expresiones.9.5.2 ¿Puede eliminarse toda la redundancia?¿Es posible eliminar todos los cálculos redundantes a lo largo de todos los caminos? La respuesta es “no”, a menos que podamos modificar el grafo de flujo mediante la creación de nuevosbloques.Ejemplo 9.28: En el ejemplo que se muestra en la figura 9.31(a), la expresión de b + c secalcula de manera redundante en el bloque B 4 si el programa sigue el camino de ejecución B1→ B 2 → B 4. Sin embargo, no podemos simplemente mover el cálculo de b + c al bloque B 3, yaque ello crearía un cálculo adicional de b + c al tomar el camino B1 → B3 → B5.Lo que tendríamos que hacer es insertar el cálculo de b + c sólo a lo largo de la flecha que vadel bloque B 3 al bloque B 4. Para ello, podemos colocar la instrucción en un nuevo bloque, pordecir B 6, y hacer que el flujo de control de B 3 pase a B 6 antes de llegar a B 4. La transformaciónse muestra en la figura 9.31(b). ✷Maq. Cap_9_Bok.indd 642 11/10/07 1:03:24 AMB 4B 2B 5BB B12 3 t = b+ca = tt = b+cd = tB 6(a) (b). . . a = b+cd = b+cBBB B134 5. . .Figura 9.31: B 3 → B 4 es una flecha críticoDefinimos una flecha crítico de un grafo de flujo como cualquier flecha que conduce de unnodo con más de un sucesor, hacia un nodo con más de un predecesor. Al introducir nuevosbloques a lo largo de las flechas críticas, siempre podemos encontrar un bloque para acomodarla colocación de la expresión deseada. Por ejemplo, la flecha que va de B 3 a B 4 en la figura9.31(a) es crítico, debido a que B 3 tiene dos sucesores, y B 4 tiene dos predecesores.Tal vez no sea suficiente agregar bloques para permitir la eliminación de todos los cálculosredundantes. Como se muestra en el ejemplo 9.29, puede ser necesario duplicar código paraaislar el camino en la que se encuentra la redundancia.Ejemplo 9.29: En el ejemplo que se muestra en la figura 9.32(a), la expresión de b + c secalcula de manera redundante a lo largo de B 1 → B 2 → B 4 → B 6. Tendríamos que eliminarel cálculo redundante de b + c del bloque B 6 en este camino y calcular la expresión sólo a lolargo del camino B 1 → B 3 → B 4 → B 6. Sin embargo, no hay un solo punto del programa oflecha en el programa de origen que corresponda en forma única al camino anterior. Para creardicho punto en el programa, podemos duplicar el par de bloques B 4 y B 6, en donde se llega aun par a través de B 2 y se llega a otro a través de B 3, como se muestra en la figura 9.32(b). Elresultado de b + c se guarda en la variable t en el bloque B 2, y se mueve a la variable d en B6,la copia de B 6 a la que se llega desde B 2. ✷Como el número de caminos es exponencial en el número de bifurcaciones condicionales enel programa, al eliminar todas las expresiones redundantes se puede incrementar de maneraconsiderable el tamaño del código optimizado. Por lo tanto, restringiremos nuestra discusiónsobre las técnicas de eliminación de redundancia a aquellas que puedan introducir bloques adicionales, pero que no dupliquen porciones del grafo de flujo de control.9.5 Eliminación de redundancia parcial 643Maq. Cap_9_Bok.indd 643 11/10/07 1:03:24 AM644 Capítulo 9. Optimizaciones independientes de la máquinaa = b+cd = b+ca = b+ct = ad = t d = b+cB BBBB BBBB BBB BB12 345 6712 34 B'B'45667(a) (b) Figura 9.32: Duplicación del código para eliminar redundancias9.5.3 El problema del movimiento de código diferidoEs conveniente que los programas optimizados con un algoritmo de eliminación de redundanciaparcial tengan las siguientes propiedades:1. Todos los cálculos redundantes de expresiones que puedan eliminarse sin la duplicaciónde código se eliminan.2. El programa optimizado no realiza ningún cálculo que no se encuentre en la ejecución delprograma original.3. Las expresiones se calculan lo más tarde que sea posible.La última propiedad es importante, ya que los valores de las expresiones que son redundantes,por lo general, se mantienen en registros, hasta que se utilizan. Al calcular un valor lo mástarde posible se minimiza su tiempo de vida: la duración entre el tiempo que se define el valory la última vez que se utilizó, lo cual a su vez disminuye el uso que hace de un registro. Nosreferimos a la optimización de eliminar la redundancia parcial, con el objetivo de retrasar loscálculos lo más que sea posible, como movimiento de código diferido.Para elaborar nuestra intuición del problema, primero vamos a ver cómo razonar sobrela redundancia parcial de una sola expresión, a lo largo de un solo camino. Por conveniencia,asumiremos durante el resto de la explicación que toda instrucción es un bloque básico por sísola.Maq. Cap_9_Bok.indd 644 11/10/07 1:03:26 AMRedundancia completaUna expresión e en el bloque B es redundante si, a lo largo de todos los caminos que llegan aB, e se ha evaluado y los operandos de e no se han definido de nuevo posteriormente. Hagamosque S sea el conjunto de bloques, cada uno de los cuales contiene la expresión e, que hacen quee en B sea redundante. El conjunto de flechas que salen de los bloques en S debe formar necesariamente un conjunto de corte (cutset), que si se elimina, desconecta al bloque B de la entradadel programa. Además, no se definen de nuevo operandos de e a lo largo de los caminos queconducen de los bloques en S hacia B.Redundancia parcialSi una expresión e en el bloque B es sólo parcialmente redundante, el algoritmo de movimientode código diferido trata de hacer que e sea completamente redundante en B, colocando copiasadicionales de las expresiones en el grafo de flujo. Si el intento tiene éxito, el grafo de flujo optimizado también tendrá un conjunto de bloques básicos S, cada uno de los cuales contendrá laexpresión e, y cuyas flechas salientes formen un conjunto de corte entre la entrada y B. Al igualque el caso completamente redundante, no se definen de nuevo los operandos de e a lo largo delos caminos que conducen de los bloques en S hacia B.9.5.4 Anticipación de las expresionesExiste una restricción más que se impone en las expresiones insertadas, para asegurar que nose ejecuten operaciones adicionales. Las copias de una expresión deben colocarse sólo en lospuntos del programa en donde se anticipa la expresión. Decimos que una expresión b + c seanticipa en el punto p si todas los caminos que conducen desde el punto p calculan en un momento dado el valor de la expresión b + c a partir de los valores de b y c que están disponiblesen ese punto.Ahora vamos a examinar lo que se requiere para eliminar la redundancia parcial a lo largode un camino acíclico B 1 → B 2 → … → B n. Suponga que la expresión e se evalúa sólo en losbloques B 1 y B n , y que los operandos de e no se definen de nuevo en los bloques a lo largodel camino. Hay flechas entrantes que se unen al camino y hay flechas salientes que salen deésta. Podemos ver que e no se anticipa a la entrada del bloque B i si, y sólo si existe una flechasaliente que salga del bloque B j , i ≤ j < n, que conduzca hacia un camino de ejecución queno utilice el valor de e. Por ende, la anticipación limita qué tan pronto se puede insertar unaexpresión.Podemos crear un conjunto de corte (cutset) que incluya la arista B i −1 → B i y que haga ae redundante en B n, si e está disponible o se anticipa a la entrada de Bi. Si e se anticipa perono está disponible a la entrada de B i, debemos colocar una copia de la expresión e a lo largode la flecha entrante.Podemos elegir en dónde colocar las copias de la expresión, ya que, por lo general, hayvarios conjuntos de corte en el grafo de flujo que cumplen con todos los requerimientos. Enel conjunto de corte anterior, el cálculo se introduce a lo largo de las flechas entrantes quevan hacia el camino de interés y, por lo tanto, la expresión se calcula lo más cerca posible deluso, sin introducir redundancia. Observe que estas operaciones introducidas pueden ser en sí9.5 Eliminación de redundancia parcial 645Maq. Cap_9_Bok.indd 645 11/10/07 1:03:27 AM646 Capítulo 9. Optimizaciones independientes de la máquinaparcialmente redundantes con otras instancias de la misma expresión en el programa. Dicharedundancia parcial puede eliminarse al mover estos cálculos más hacia arriba.En resumen, la anticipación de las expresiones limita qué tan pronto puede colocarse unaexpresión; no podemos colocar una expresión tan pronto que no se anticipe al lugar en dondela colocamos. Entre más anticipada se coloque una expresión, más será la redundancia quese pueda eliminar, y entre todas las soluciones que eliminen a las mismas redundancias, laque calcule las expresiones lo más tarde minimizará los tiempos de vida de los registros quecontienen los valores de las expresiones involucradas.9.5.5 El algoritmo de movimiento de código diferidoEn consecuencia, esta explicación motiva la creación de un algoritmo de cuatro pasos. El primer paso utiliza la anticipación para determinar en dónde pueden colocarse las expresiones; elsegundo paso busca el conjunto de corte más anticipado, entre aquellos que eliminan todas lasoperaciones redundantes posibles, sin duplicar el código y sin introducir cálculos indeseables.Este paso coloca los cálculos en puntos del programa en donde los valores de sus resultados seanticipan por primera vez. Después, el tercer paso empuja el conjunto de corte hacia abajo, hasta el punto en donde cualquier retraso adicional alteraría la semántica del programa, o introduciría redundancia. El cuarto paso (final) es una pasada simple para limpiar el código, eliminandoasignaciones a variables temporales que se utilizan sólo una vez. Cada paso se realiza con unapasada del flujo de datos: la primera y la cuarta son problemas de flujo hacia atrás, la segunday la tercera son problemas de flujo hacia delante.Generalidades del algoritmo1. Buscar todas las expresiones anticipadas en cada punto del programa, mediante unapasada de flujo de datos hacia atrás.2. El segundo paso coloca el cálculo en donde se anticipan por primera vez los valores delas expresiones a lo largo de cierto camino. Después de haber colocado copias de unaexpresión en donde ésta se anticipa por primera vez, la expresión estaría disponible en elpunto del programa p si se ha anticipado a lo largo de todos los caminos que lleguen a p.La disponibilidad puede resolverse mediante una pasada de flujo de datos hacia delante.Si deseamos colocar las expresiones en las posiciones más anticipadas posibles, podemossimplemente buscar esos puntos del programa en donde se anticipan las expresiones, perono están disponibles.3. Al ejecutar una expresión tan pronto como se anticipe se puede producir un valor muchoantes de utilizarlo. Una expresión es diferible en un punto del programa si la expresión seha anticipado y todavía no se utiliza a lo largo de cualquier camino que llegue al punto delprograma. Las expresiones diferibles se encuentran mediante una pasada de flujo de datoshacia delante. Colocamos expresiones en esos puntos del programa en donde ya no puedenposponerse.4. Se utiliza una pasada final simple de flujo de datos hacia atrás para eliminar las asignaciones a las variables temporales que se utilizan sólo una vez en el programa.Maq. Cap_9_Bok.indd 646 11/10/07 1:03:27 AMPasos de preprocesamientoAhora vamos a presentar el algoritmo completo de movimiento de código diferido. Paramantener este algoritmo simple, vamos a suponer que al principio cada instrucción se encuentraen su propio bloque básico, y sólo introducimos cálculos nuevos de expresiones en los iniciosde los bloques. Para asegurar que esta simplificación no reduzca la efectividad de la técnicainsertamos un nuevo bloque entre el origen y el destino de un camino si el destino tiene másde un predecesor. Es obvio que al hacer esto también nos hacemos cargo de todos los caminoscríticos en el programa.Abstraemos la semántica de cada bloque B con dos conjuntos: eusoB es el conjunto deexpresiones calculadas en B y eeliminarB es el conjunto de expresiones eliminadas; es decir, elconjunto de expresiones en las que cualquiera de sus operandos están definidos en B. Utilizaremos el ejemplo 9.30 a lo largo de la explicación de los cuatro análisis de flujo de datos, cuyasdefiniciones se sintetizan en la figura 9.34.Ejemplo 9.30: En el grafo de flujo de la figura 9.33(a), la expresión b + c aparece tres veces.Como el bloque B 9 es parte de un ciclo, la expresión puede calcularse muchas veces. El cálculoen el bloque B0 no solo es invariante de ciclo; es también una expresión redundante, ya que suvalor se ha utilizado de antemano en el bloque B 7. Para este ejemplo, debemos calcular b + csólo dos veces, una en el bloque B 5 y otra a lo largo del camino después de B 2 y antes de B 7.El algoritmo de movimiento de código diferido colocará los cálculos de la expresión al inicio delos bloques B 4 y B 5. ✷Expresiones anticipadasRecuerde que una expresión b + c se anticipa en un punto p del programa si todos los caminosque salen desde el punto p en un momento dado calculan el valor de la expresión b + c, a partirde los valores de b y c que están disponibles en ese punto.En la figura 9.33(a), todos los bloques que se anticipan a b + c en la entrada se muestrancomo cuadros con un ligero sombreado. La expresión b + c se anticipa en los bloques B 3, B 4,B 5, B 6, B 7 y B 9. No se anticipa en la entrada al bloque B 2, ya que el valor de c se recalcula dentro del bloque y, por lo tanto, el valor de b + c que se calcularía al inicio de B 2 no se utiliza a lolargo de ningún camino. La expresión b + c no se anticipa al entrar a B 1, ya que es innecesariaa lo largo de la bifurcación de B 1 a B 2 (aunque se utilizaría a lo largo de la camino B 1 → B 5→ B 6 ). De manera similar, la expresión no se anticipa al inicio de B 8, debido a la bifurcaciónde B 8 a B 11. La anticipación de una expresión puede oscilar a lo largo de un camino, como seilustra mediante B 7 → B 8 → B 9.Las ecuaciones del flujo de datos para el problema de las expresiones anticipadas se muestranen la figura 9.34(a). El análisis es una pasada invertida. Una expresión anticipada en la salidade un bloque B es una expresión anticipada en la entrada, sólo si no se encuentra en el conjuntoeeliminarB. Además, un bloque B genera como nuevos usos el conjunto de expresiones eusoB.Al salir del programa, ninguna de las expresiones es anticipada. Como nos interesa encontrar expresiones que sean anticipadas a lo largo de todos los caminos siguientes, el operador de reunión9.5 Eliminación de redundancia parcial 647Maq. Cap_9_Bok.indd 647 11/10/07 1:03:28 AM648 Capítulo 9. Optimizaciones independientes de la máquinaBBBBBBBBBBB1234567891011BBBBBBBBBBB1234567891011(a) (b)c = 2 a = b+cd = b+ce = b+cc = 2 t = b+ca = tt = b+cd = te = tanticipadasno disponiblesprimerasdiferiblesFigura 9.33: Grafo de flujo del ejemplo 9.30es la intersección de conjuntos. En consecuencia, los puntos interiores deben inicializarse con elconjunto universal U, como vimos para el problema de las expresiones disponibles en la sección9.2.6.Expresiones disponiblesAl final de este segundo paso, las copias de una expresión se colocarán en los puntos del programaen donde se anticipó la expresión por primera vez. Si ése es el caso, una expresión estará disponible en el punto p del programa si se anticipa a lo largo de todos los caminos que llegan a p. Esteproblema es similar a las expresiones disponibles descritas en la sección 9.2.6. Aunque la funciónde transferencia que se utiliza aquí es un poco distinta. Una expresión está disponible al salir deun bloque si está:Maq. Cap_9_Bok.indd 648 11/10/07 1:03:28 AM(a) Expresiones anticipadas (b) Expresiones disponiblesDominio Conjuntos de expresiones Conjuntos de expresionesDirección Hacia atrás Hacia delanteFunción detransferenciafB (x ) =eusoB ∪ (x − eeliminarB)fB (x ) =(anticipadas [B ].ent ∪ x ) − eeliminarBLímite ENT[SALIDA] = ∅ SAL[ENTRADA] = ∅Reunión (∧) ∩ ∩Ecuaciones ENT[B ] = fB (SAL[B ])SAL[B ] = ∧S, suc (B) ENT[S ]SAL[B ] = fB (ENT[B ])ENT[B ] = ∧P, pred (B) SAL[P ]Inicialización ENT[B ] = U SAL[B ] = U(c) Expresiones diferibles (d) Expresiones usadasDominio Conjuntos de expresiones Conjuntos de expresionesDirección Hacia delante Hacia atrásFunción detransferenciafB (x ) =(primeras [B ] ∪ x ) − eusoB)fB (x ) =(eusoB ∪ x ) − ultimas [B ])Límite SAL[ENTRADA] = ∅ ENT[SALIDA] = ∅Reunión (∧) ∩ ∪Ecuaciones SAL[B ] = fB (ENT[B ])ENT[B ] = ∧P, pred (B) SAL[P ]ENT[B ] = fB (SAL[B ])SAL[B ] = ∧S, suc (B) ENT[S ]Inicialización SAL[B ] = U ENT[B ] = ∅primeras [B ] = anticipadas [B ].ent − disponibles [B ].ent ultimas [B ] = (primeras [B ] ∪ diferibles [B ].ent) ∩(eusoB ∪ ¬(∩S, suc [B ] (primeras [S ] ∪ diferibles [S ].ent)))Figura 9.34: Cuatro pasadas de flujo de datos en la eliminación de redundancia parcial9.5 Eliminación de redundancia parcial 649Maq. Cap_9_Bok.indd 649 11/10/07 1:03:29 AM650 Capítulo 9. Optimizaciones independientes de la máquina1. Ya sea (a) Disponible. (b) En el conjunto de expresiones anticipadas al momento de entrar (es decir, podríahacerse disponible si optamos por calcularla aquí), y2. No se elimina en el bloque.Las ecuaciones de f lujo de datos para las expresiones disponibles se muestran en la figura9.34 (b). Para evitar confundir el significado de ENT, nos referimos al resultado de un análisisanterior adjuntando “[B ].ent ” al nombre del análisis anterior.Con la estrategia de colocación más anticipada, el conjunto de expresiones que se colocanen el bloque B, es decir, primeras [B ], se define como el conjunto de expresiones anticipadas queno están aún disponibles. Es decir,primeras [B ] = anticipadas [B ].ent − disponibles [B ].ent.Ejemplo 9.3.1: La expresión b + c en el grafo de flujo de la figura 9.35 no se anticipa a laentrada del bloque B 3, pero se anticipa a la entrada del bloque B 4. Sin embargo, no es necesariocalcular la expresión b + c en el bloque B 4, ya que la expresión se encuentra de antemanodisponible, debido al bloque B 2. ✷Ejemplo 9.3.2: En la figura 9.33(a) se muestran con cuadros sombreados en color oscurolos bloques para los cuales la expresión b + c no está disponible; éstos son B 1, B 2, B 3 y B 5.Las primeras posiciones colocadas se representan mediante los cuadros con sombreado claroy sombras oscuras, que por ende son los bloques B 3 y B 5. Observe, por ejemplo, que b + c seconsidera disponible al entrar a B 4, ya que hay un camino B 1 → B 2 → B 3 → B 4 a lo largo dela cual b + c se anticipa por lo menos una vez (en este caso, en B 3) y desde el inicio de B 3, nose recalcularon b o c. ✷Completar el cuadradoLas expresiones anticipadas (también conocidas como “expresiones ocupadas” en cualquier otra parte) son un tipo de análisis de flujo de datos que no hemos visto antes. Aunque hemos visto los marcos de trabajo con flujo hacia atrás como el análisis de variablesvivas (sección 9.2.5), y hemos visto marcos de trabajo en donde la reunión es la intersección como las expresiones disponibles (sección 9.2.6), éste es el primer ejemplo de unanálisis útil que tiene ambas propiedades. Casi todos los análisis que utilizamos puedencolocarse en uno de cuatro grupos, dependiendo de si fluyen hacia delante o hacia atrás,y dependiendo de si utilizan la unión o la intersección para la reunión. Observe tambiénque los análisis de unión siempre requieren preguntar si existe un camino a través de lacual algo sea verdadero, mientas que los análisis de intersección preguntan si algo esverdadero a lo largo de todos los caminos.Maq. Cap_9_Bok.indd 650 11/10/07 1:03:30 AMa = b+cd = b+cBBBB1234Figura 9.35: Grafo de flujo para el ejemplo 9.31, que ilustra el uso de la disponibilidadExpresiones diferiblesEl tercer paso difiere el cálculo de las expresiones lo más posible, preservando a la vez la semántica del programa original y minimizando la redundancia. El ejemplo 9.33 ilustra la importancia de este paso.Ejemplo 9.33: En el grafo de flujo que se muestra en la figura 9.36, la expresión b + c secalcula dos veces a lo largo del camino B 1 → B 5 → B 6 → B 7. La expresión b + c se anticipaincluso al inicio del bloque B 1. Si calculáramos la expresión tan pronto como se anticipa, habríamos calculado la expresión b + c en B 1. El resultado tendría que guardarse desde el inicio,a través de la ejecución del ciclo que comprende los bloques B 2 y B 3, hasta utilizarlo en elbloque B 7. En vez de ello, podemos retrasar el cálculo de la expresión b + c hasta el inicio deB 5 y hasta que el flujo de control esté a punto de cambiar de B 4 a B 7. ✷De manera formal, una expresión x + y es diferible a un punto p del programa si se encuentra una colocación anticipada de x + y a lo largo de todos los caminos desde el nodo de entradahasta p, y no hay un uso subsiguiente de x + y después de esa última colocación.Ejemplo 9.34: Vamos a considerar de nuevo la expresión b + c en la figura 9.33. Los dosprimeros puntos para b + c son B 3 y B 5; observe que éstos son los dos bloques que tienen sombreado claro y oscuro en la figura 9.33(a), indicando que b + c se anticipa y no está disponiblepara estos bloques, y sólo estos bloques. No podemos posponer b + c de B 5 a B 6, ya que b + cse utiliza en B 5. Sin embargo, podemos posponerla de B 3 a B 4.Pero no podemos posponer b + c de B 4 a B 7. La razón es que, aunque b + c no se utilizaen B 4, al colocar su cálculo mejor en B 7 se produciría un cálculo redundante de b + c a lo largo9.5 Eliminación de redundancia parcial 651Maq. Cap_9_Bok.indd 651 11/10/07 1:03:31 AM652 Capítulo 9. Optimizaciones independientes de la máquinaa = b+cd = b+cBBBBBBB1567432Figura 9.36: Grafo de flujo para el ejemplo 9.33, para ilustrar la necesidad de diferir unaexpresióndel camino B 5 → B 6 → B 7. Como veremos más adelante, B 4 es uno de los últimos lugares enlos que podemos calcular b + c. ✷ Las ecuaciones de flujo de datos para el problema de las expresiones diferibles se muestranen la figura 9.34(c). El análisis es una pasada hacia delante. No podemos “diferir” una expresión hasta la entrada del programa, por lo que SAL[ENTRADA] = ∅. Una expresión es diferiblehasta la salida del bloque B si no se utiliza en el bloque, y si es diferible hasta la entrada de Bo si se encuentra en primeras [B ]. Una expresión no es diferible hasta la entrada de un bloque amenos que todos sus predecesores incluyan a la expresión en sus conjuntos diferibles en sussalidas. Por ende, el operador de reunión es la intersección de conjuntos, y los puntos interioresdeben inicializarse con el elemento superior del semi-lattice: el conjunto universal.Hablando en general, una expresión se coloca en la frontera, en donde una expresión cambiade ser diferible a no serlo. Dicho en forma más específica, una expresión e puede colocarse alinicio de un bloque B sólo si la expresión se encuentra en el conjunto primeras o diferibles deB al momento de entrar. Además, B está en la frontera diferible/no diferible de e si una de lassiguientes afirmaciones es válida:1. e no se encuentra en diferibles [B ].sal. En otras palabras, e está en eusoB.2. e no puede diferirse hacia uno de sus sucesores. En otras palabras, existe un sucesor deB tal que e no se encuentra en el conjunto primeras o diferibles al momento de entrar aese sucesor.La expresión e puede colocarse al frente del bloque B en cualquiera de los dos casos anteriores,debido a los nuevos bloques introducidos por el paso de preprocesamiento en el algoritmo.Maq. Cap_9_Bok.indd 652 11/10/07 1:03:32 AMEjemplo 9.35: La figura 9.33(b) muestra el resultado del análisis. Los cuadros con sombreadoclaro representan a los bloques cuyo conjunto primeras incluye a b + c. Las sombras oscurasindican aquellos bloques que incluyen b + c en su conjunto diferibles. Las últimas colocaciones de las expresiones son, por lo tanto, las entradas de los bloques B 4 y B 5, ya que:1. b + c está en el conjunto diferibles de B 4, pero no en B 7.2. El conjunto primeras de B 5, incluye a b + c y utiliza a b + c.La expresión se almacena en la variable temporal t en los bloques B 4 y B 5, y t se utiliza en vezde b + c en cualquier otra parte, como se muestra en la figura. ✷Expresiones usadasPor último, se utiliza una pasada hacia atrás para determinar si las variables temporales introducidas se utilizan más allá del bloque en el que se encuentran. Decimos que una expresión seutiliza en el punto p si existe un camino que conduzca desde p, que utilice la expresión antesde evaluar de nuevo el valor. Este análisis es en esencia el análisis del estado de vida (paraexpresiones, en vez de variables).Las ecuaciones de flujo de datos para el problema de las expresiones usadas se muestran en lafigura 9.34(d). El análisis es una pasada hacia atrás. Una expresión usada a la salida de un bloque B es una expresión usada al entrar sólo si no se encuentra en el conjunto últimas. Un bloquegenera, como nuevos usos, el conjunto de expresiones en eusoB. A la salida del programa, ningunade las expresiones es usada. Como nos interesa encontrar expresiones que se utilicen en cualquiercamino siguiente, el operador de reunión es la unión de conjuntos. Así, los puntos interiores debeninicializarse con el elemento superior del semi-lattice: el conjunto vacío.Reunión de todos los conceptosTodos los pasos del algoritmo se sintetizan en el Algoritmo 9.36.Algoritmo 9.36: Movimiento de código diferido.ENTRADA: Un grafo de flujo para el cual se han calculado eusoB y eeliminarB para cadabloque B.SALIDA: Un grafo de flujo modificado, que cumple con las cuatro condiciones de movimientode código diferido de la sección 9.5.3.MÉTODO:1. Insertar un bloque vacío a lo largo de todas las flechas que entran a un bloque con másde un predecesor.2. Encontrar anticipadas [B ].ent para todos los bloques B, según su definición en la figura 9.34(a).3. Encontrar disponibles [B ].ent para todos los bloques B, según su definición en la figura9.34(b).9.5 Eliminación de redundancia parcial 653Maq. Cap_9_Bok.indd 653 11/10/07 1:03:33 AM654 Capítulo 9. Optimizaciones independientes de la máquina4. Calcular las primeras colocaciones para todos los bloques B:primeras [B ] = anticipadas [B ].ent − disponibles [B ].ent5. Encontrar diferibles [B ].ent para todos los bloques B, según su definición en la figura9.34(c).6. Calcular las últimas colocaciones para todos los bloques B:últimas [B ] = (primeras [B ] ∪ diferibles [B ].ent) ∩(eusoB ∪ ¬(∩S en suc [B ] (anticipadas [S ] ∪ diferibles [S ].ent))) Observe que ¬ denota la complementación con respecto al conjunto de todas las expresiones calculadas por el programa.7. Encontrar usadas [B ].sal para todos los bloques B, según su definición en la figura9.34(d).8. Para cada expresión (por decir x + y) calculada por el programa, hacer lo siguiente: (a) Crear un nuevo valor temporal, por decir t para x + y. (b) Para todos los bloques B tales que x + y se encuentre en últimas [B ] ∩ usadas [B ].sal, agregar t = x+y al inicio de B. (c) Para todos los bloques B tales que x + y se encuentre eneusoB ∩ (¬últimas [B ] ∪ usadas.sal[B ]) sustituir cada x + y original por t.✷ResumenLa eliminación de redundancia parcial encuentra muchas formas distintas de operacionesredundantes en un algoritmo unificado. Este algoritmo ilustra cómo pueden usarse múltiplesproblemas de flujo de datos para encontrar la colocación óptima de las expresiones.1. El análisis de las expresiones anticipadas proporciona las restricciones de colocación; éstees un análisis de flujo de datos hacia atrás, con un operador de reunión de intersecciónde conjuntos, ya que determina si las expresiones se utilizan en forma siguiente a cadapunto del programa, en todos los caminos.2. La primera colocación de una expresión se da mediante los puntos del programa en dondese anticipa la expresión, pero no está disponible. Las expresiones disponibles se encuentran mediante un análisis de flujo de datos hacia delante, con un operador de reunión deintersección de conjuntos que calcula si una expresión ha sido anticipada antes de cadapunto del programa, a lo largo de todos los caminos.Maq. Cap_9_Bok.indd 654 11/10/07 1:03:34 AM3. La última colocación de una expresión se da mediante los puntos del programa en dondeya no puede diferirse una expresión. Las expresiones pueden diferirse en un punto delprograma si para todos los caminos que llegan al punto del programa, no se ha encontradoun uso de la expresión. Las expresiones diferibles se encuentran mediante un análisis delflujo de datos hacia delante, con un operador de reunión de intersección de conjuntos.4. Las asignaciones temporales se eliminan, a menos que las utilice algún camino posteriormente. Para encontrar las expresiones usadas utilizamos un análisis del flujo de datoshacia atrás, esta vez con un operador de reunión de unión de conjuntos.9.5.6 Ejercicios para la sección 9.5Ejercicio 9.5.1: Para el grafo de flujo de la figura 9.37:a) Calcule anticipadas para el inicio y el final de cada bloque.b) Calcule disponibles para el inicio y el final de cada bloque.c) Calcule primeras para cada bloque.d) Calcule diferibles para el inicio y el final de cada bloque.e) Calcule usadas para el inicio y el final de cada bloque.f) Calcule últimas para cada bloque.g) Introduzca la variable temporal t; muestre en dónde se calcula y en dónde se utiliza.Ejercicio 9.5.2: Repita el ejercicio 9.5.1 para el grafo de flujo de la figura 9.10 (vea los ejercicios de la sección 9.1). Puede limitar su análisis a las expresiones a + b, c − a y b * d.Ejercicio 9.5.3: Los conceptos que vimos en esta sección también pueden aplicarse para eliminar el código parcialmente muerto. Una definición de una variable está parcialmente muertasi la variable está viva en ciertos caminos y no en otras. Podemos optimizar la ejecución delprograma con sólo realizar la definición a lo largo de los caminos en las que la variable está viva.A diferencia de la eliminación de redundancia parcial, en donde las expresiones se mueven antesde la original, las nuevas definiciones se colocan después de la original. Desarrolle un algoritmopara mover el código parcialmente muerto, de manera que las expresiones se evalúen sólo endonde se utilizarán en un momento dado.9.6 Ciclos en los grafos de flujoHasta ahora en nuestra discusión, no hemos manejado los ciclos de manera distinta; los hemos tratado justo igual que cualquier otro tipo de flujo de control. Sin embargo, los ciclosson importantes ya que los programas invierten la mayor parte de su tiempo ejecutándolos,9.6 Ciclos en los grafos de fl ujo 655!!Maq. Cap_9_Bok.indd 655 11/10/07 1:03:34 AM656 Capítulo 9. Optimizaciones independientes de la máquina= x+yx == x+y= x+yENTRADASALIDABBBB1234Figura 9.37: Grafo de flujo para el ejercicio 9.5.1y las optimizaciones que mejoran su rendimiento pueden tener un impacto considerable. Porende, es esencial que identifiquemos los ciclos y los tratemos de manera especial. Los ciclos también afectan el tiempo de ejecución de los análisis de los programas. Si unprograma no contiene ciclos, podemos obtener las respuestas a los problemas de flujo de datosrealizando sólo una pasada a través del programa. Por ejemplo, un problema de flujo de datos hacia delante puede resolverse visitando todos los nodos una vez, en orden topológico.En esta sección presentaremos los siguientes conceptos: dominadores, orden “primero enprofundidad”, aristas posteriores, profundidad de los grafos y capacidad de reducción. Cadauno de estos conceptos se requiere para nuestras explicaciones posteriores acerca de cómoencontrar ciclos y la velocidad de convergencia del análisis de flujos de datos iterativos.9.6.1 DominadoresDecimos que el nodo d de un grafo de flujo domina al nodo n, lo cual se escribe como d dom n,si cada camino desde el nodo de entrada del grafo de flujo hasta n pasa a través de d. Observeque bajo esta definición, cada nodo se domina a sí mismo.Ejemplo 9.37: Considere el grafo de flujo de la figura 9.38, con el nodo de entrada 1. El nodode entrada domina a todos los nodos (esta instrucción se aplica a todos los grafos de flujo).El nodo 2 se domina sólo a sí mismo, ya que el control puede llegar a cualquier otro nodo, alo largo de un camino que empiece con 1 → 3. El nodo 3 domina a todos los nodos, exceptoMaq. Cap_9_Bok.indd 656 11/10/07 1:03:35 AMal 1 y al 2. El nodo 4 domina a todos excepto 1, 2 y 3, ya que todos los caminos que parten delnodo 1 deben empezar con 1 → 2 → 3 → 4 o 1 → 3 → 4. Los nodos 5 y 6 sólo se dominan así mismos, ya que el flujo de control puede saltar a uno, pasando a través del otro. Por último,7 domina a 7, 8, 9 y 10; 8 domina a 8, 9 y 10; y 9 y 10 sólo se dominan a sí mismos. ✷12345 6789 10Figura 9.38: Un grafo de flujoUna manera útil de presentar la información de los dominadores es en un árbol, conocidocomo el árbol de dominadores, en el cual el nodo de entrada es la raíz, y cada nodo d dominasólo a sus descendientes en el árbol. Por ejemplo, la figura 9.39 muestra el árbol de dominadores para el grafo de flujo de la figura 9.38.12 3456789 10Figura 9.39: Árbol dominador para el grafo de flujo de la figura 9.38La existencia de árboles de dominadores proviene de una propiedad de dominadores: cadanodo n tiene un dominador inmediato m único, que es el último dominador de n en cualquier9.6 Ciclos en los grafos de fl ujo 657Maq. Cap_9_Bok.indd 657 11/10/07 1:03:36 AM658 Capítulo 9. Optimizaciones independientes de la máquinacamino desde el nodo de entrada hasta n. En términos de la relación dom, el dominador inmediato m tiene la propiedad de que si d ≠ n y d dom n, entonces d dom m.Vamos a proporcionar un simple algoritmo para calcular los dominadores de cada nodo n enun grafo de flujo, con base en el principio de que si p 1, p 2,…, p k son todos los predecesores de n,y d ≠ n, entonces d dom n si, y sólo si d dom p i para cada i. El problema puede formularse comoun análisis de flujo de datos hacia delante. Los valores del flujo de datos son conjuntos de bloques básicos. El conjunto de dominadores de un nodo, aparte de sí mismo, es la intersección delos dominadores de todos sus predecesores; por ende, el operador de reunión es la intersecciónde conjuntos. La función de transferencia para el bloque B simplemente agrega el mismo B alconjunto de nodos de entrada. La condición delimitadora es que el nodo ENTRADA se domina así mismo. Por último, la inicialización de los nodos interiores es el conjunto universal, es decir,el conjunto de todos los nodos.Algoritmo 9.38: Búsqueda de denominadores.ENTRADA: Un grafo de flujo G con el conjunto de nodos N, el conjunto de aristas F y el nodode entrada ENTRADA.SALIDA: D(n), el conjunto de nodos que dominan al nodo n, para todos los nodos n en N.MÉTODO: Buscar la solución al problema de flujo de datos, cuyos parámetros se muestran enla figura 9.40. Los bloques básicos son los nodos. D(n) = SAL[n] para todas las n en N. ✷El proceso de buscar dominadores usando este algoritmo de flujo de datos es eficiente.Los nodos en el grafo necesitan visitarse sólo unas cuantas veces, como veremos en la sección9.6.7.DominadoresDominio El conjunto potencia de NDirección Hacia delanteFunción de transferencia fB(x ) = x ∪ {B}Límite SAL[ENTRADA] = {ENTRADA}Reunión (∧) ∩Ecuaciones SAL[B ] = fB(ENT[B ])ENT[B ] = ∧P, pred (B) SAL[P ]Inicialización SAL[B ] = NFigura 9.40: Un algoritmo de flujo de datos para calcular dominadoresEjemplo 9.39: Vamos a regresar al grafo de flujo de la figura 9.38, y suponga que el ciclo forde las líneas (4) a (6) en la figura 9.23 visita los nodos en orden numérico. Hagamos que D(n)sea el conjunto de nodos en SAL[n]. Como 1 es el nodo de entrada, a D(1) se le asignó {1}Maq. Cap_9_Bok.indd 658 11/10/07 1:03:37 AMen la línea (1). El nodo 2 sólo tiene 1 como predecesor, por lo que D(2) = {2} ∪ D(1). Por ende,D(2) se establece en {1, 2}. Después se considera el nodo 3, con los predecesores 1, 2, 4 y 8.Como todos los nodos interiores se inicializan con el conjunto universal N,El resto de los cálculos se muestran en la figura 9.41. Como estos valores no cambian en lasegunda iteración a través del ciclo externo de las líneas (3) a (6) de la figura 9.23(a), son lasrespuestas finales al problema de los dominadores. ✷Figura 9.41: Término del cálculo de dominadores para el ejemplo 9.39Propiedades de la relación domUna observación clave acerca de los dominadores es que si tomamos cualquier caminoacíclico desde la entrada hasta el nodo n, entonces todos los dominadores de n aparecena lo largo de este camino, y además deben aparecer en el mismo orden a lo largo de cualquier camino de este tipo. Para ver por qué, suponga que sólo hubiera un camino acíclicoP 1 hacia n, a lo largo de la cual aparecieran los dominadores a y b en ese orden, y otrocamino P 2 hacia n, a lo largo de la cual b estuviera antes que a. Entonces, podríamosseguir P 1 hacia a y P 2 hacia n, con lo cual evitaríamos por completo a b. Por ende, b enrealidad no dominaría a a.Este razonamiento nos permite demostrar que dom es transitivo: si a dom b y b domc, entonces a dom c. Además, dom es antisimétrico: nunca es posible que tanto a dom bcomo b dom a sean válidas, si a ≠ b. Lo que es más, si a y b son dos dominadores de n,entonces debe ser válida a dom b o b dom a. Por último, resulta que cada nodo n (exceptoel de entrada) debe tener un dominador inmediato único; el dominador que aparezca máscerca de n a lo largo de cualquier camino acíclico desde la entrada hasta n.9.6 Ciclos en los grafos de fl ujo 659Maq. Cap_9_Bok.indd 659 11/10/07 1:03:38 AM660 Capítulo 9. Optimizaciones independientes de la máquina9.6.2 Ordenamiento “primero en profundidad”Como se presentó en la sección 2.3.4, una búsqueda “primero en profundidad” de un grafo vistaa todos los nodos en el grafo una vez, empezando en el nodo de entrada y visitando los nodosque estén más alejados del nodo de entrada, lo más rápido que sea posible. El camino de labúsqueda en el método “primero en profundidad” forma un árbol de expansión con búsquedaen profundidad (DFST). En la sección 2.3.4 vimos que un recorrido preorden visita a un nodoantes de visitar cualquiera de sus hijos, que después visita de manera recursiva, en orden deizquierda a derecha. Además, un recorrido postorden visita a los hijos de un nodo, de manerarecursiva y en orden de izquierda a derecha, antes de visitar al nodo en sí.Hay una variante más de ordenamiento que es importante para el análisis de grafos de flujo: un ordenamiento “primero en profundidad” es el inverso de un recorrido postorden. Es decir,en un ordenamiento “primero en profundidad”, visitamos un nodo, después recorremos su hijode más a la derecha, el hijo a su izquierda, y así en lo sucesivo. No obstante, antes de construirel árbol para el grafo de flujo, tenemos que elegir qué sucesor de un nodo se convierte en el hijode más a la derecha en el árbol, qué nodo se convierte en el siguiente hijo, y así sucesivamente.Antes de proporcionar el algoritmo para el ordenamiento “primero en profundidad”, vamos aconsiderar un ejemplo.Ejemplo 9.40: Una posible presentación tipo “primero en profundidad” del grafo de flujo enla figura 9.38 se ilustra en la figura 9.42. Las aristas sólidas forman el árbol; las aristas punteadas son las otras aristas del grafo de flujo. Un recorrido “primero en profundidad” del árbol se damediante: 1 → 3 → 4 → 6 → 7 → 8 → 10, después de regreso a 8, y luego a 9. Regresamosa 8 una vez más, nos retractamos a 7, 6 y 4, y después avanzamos a 5. Nos retractamos de 5hacia 4, después hacia 3 y luego a 1. De 1 vamos a 2, después nos retractamos de 2, de vueltaa 1, y hemos recorrido el árbol completo.Así, la secuencia preorden para el recorrido es:1, 3, 4, 6, 7, 8, 10, 9, 5, 2.La secuencia postorden para el recorrido del árbol en la figura 9.42 es:10, 9, 8, 7, 6, 5, 4, 3, 2, 1.El ordenamiento tipo primero en profundidad, que es el inverso de la secuencia postorden, es:1, 2, 3, 4, 5, 6, 7, 8, 9, 10.✷Ahora vamos a proporcionar un algoritmo que encuentra un árbol de expansión con búsqueda en profundidad y un ordenamiento “primero en profundidad” de un grafo. Este algoritmoes el que encuentra el DFST de la figura 9.42, a partir de la figura 9.38.Algoritmo 9.41: Árbol de expansión con búsqueda en profundidad y ordenamiento “primeroen profundidad”.ENTRADA: Un grafo de flujo G.SALIDA: Un DFST T de G y un ordenamiento de los nodos de G.Maq. Cap_9_Bok.indd 660 11/10/07 1:03:40 AM13 24675810 9Figura 9.42: Una presentación “primero en profundidad” del grafo de flujo de la figura 9.38MÉTODO: Utilizamos el procedimiento recursivo buscar (n) de la figura 9.43. El algoritmo inicializa todos los nodos de G a “no visitado”, después llama a buscar (n 0 ), en donde n 0 esla entrada. Cuando llama a buscar (n), primero marca a n como “visitado”, para evitar agregar n al árbol dos veces. Utiliza c para contar en orden descendente, desde el número de nodosde G hasta 1, asignando los números primero en profundidad dfn [n] a los nodos n, a medidaque avanzamos. El conjunto de aristas T forma el árbol de expansión con búsqueda en profundidad para G. ✷Ejemplo 9.42: Para el grafo de flujo de la figura 9.42, el Algoritmo 9.41 establece c en 10 yempieza la búsqueda, llamando a buscar (1). El resto de la secuencia de ejecución se muestra enla figura 9.44. ✷9.6.3 Aristas en un árbol de expansión con búsquedaen profundidadAl construir un DFST para un grafo de flujo, las aristas del grafo de flujo se dividen en trescategorías.1. Hay aristas, conocidos como aristas de avance, que van de un nodo m hasta un descendiente apropiado de m en el árbol. Todas las aristas en el mismo DFST son aristas deavance. No hay otros aristas de avance en la figura 9.42 pero, por ejemplo, si 4 → 8 fuerauna arista, estaría dentro de esta categoría.2. Hay aristas que pasan de un nodo m a un ancestro de m en el árbol (posiblemente almismo m). A estas aristas las llamaremos aristas de retirada. Por ejemplo, 4 → 3, 7 → 4,10 → 7 y 9 → 1 son las aristas de retirada en la figura 9.42.9.6 Ciclos en los grafos de fl ujo 661Maq. Cap_9_Bok.indd 661 11/10/07 1:03:40 AM662 Capítulo 9. Optimizaciones independientes de la máquinavoid buscar(n) { marcar n como “visitado”;for (cada sucesor s de n)if (s es “no visitado”) { agregar arista n → s a T; buscar(s); }dfn[n] = c;c = c − 1;}main() {T = ∅; /* conjunto de aristas */for (cada nodo n de G) marcar n como “no visitado”;c = número de nodos de G;buscar(n 0);}Figura 9.43: Algoritmo de búsqueda “primero en profundidad”3. Hay aristas m → n tales que ni m ni n son ancestros uno del otro en el DFST. Las aristas2 → 3 y 5 → 7 son los únicos ejemplos de este tipo en la figura 9.42. A estas aristas lasllamamos aristas de cruce. Una propiedad importante de las aristas de cruce es que, sidibujamos el DFST de manera que los hijos de un nodo se dibujen de izquierda a derechaen el orden en el que se agregaron al árbol, entonces todos las aristas de cruce viajan dederecha a izquierda.Hay que tener en cuenta que m → n es una arista de retirada si, y sólo si dfn [m] ≥ dfn [n].Para ver por qué, observe que si m es descendiente de n en el DFST, entonces buscar (m) termina antes de buscar (n), por lo que dfn [m] ¦ dfn [n]. Por el contrario, si dfn [m] ¦ dfn [n], entoncesbuscar (m) termina antes que buscar (n), o m = n. Pero buscar (n) debe haber empezado antesde buscar (m) si hay una arista m → n, o de lo contrario el hecho de que n es un sucesor de m habría convertido a n en descendiente de m en el DFST. En consecuencia, el tiempo que buscar (m)está activa es un subintervalo del tiempo que buscar (n) está activa, de lo cual resulta que n esun ancestro de m en el DFST.9.6.4 Aristas posteriores y capacidad de reducciónUna arista posterior es una arista a → b cuya cabeza b domina a su cola a. Para cualquiergrafo de flujo, toda arista posterior es de retirada, pero no toda arista de retirada es una aristaposterior. Se dice que un grafo de flujo es reducible si todas sus aristas de retirada en cualquierárbol de expansión con búsqueda en profundidad son también aristas posteriores. En otras palabras, si un grafo es reducible, entonces todos los DFSTs tienen el mismo conjunto de aristasMaq. Cap_9_Bok.indd 662 11/10/07 1:03:41 AMLlama a buscar (1) El nodo 1 tiene dos sucesores. Suponga que s = 3 seconsidera primero; agrega la arista 1 → 3 a T.Llama a buscar (3) Agrega la arista 3 → 4 a T.Llama a buscar (4) El nodo 4 tiene dos sucesores, 4 y 6. Suponga que s = 6 seconsidera primero; agrega la arista 4 → 6 a T.Llama a buscar (6) Agrega 6 → 7 a T.Llama a buscar (7) El nodo 7 tiene dos sucesores, 4 y 8. Pero 4 ya está marcadocomo “visitado” por buscar (4), por lo que no hace nadacuando s = 4. Para s = 8, agrega la arista 7 → 8 a T.Llama a buscar (8) El nodo 8 tiene dos sucesores, 9 y 10. Suponga que s = 10 seconsidera primero; agrega la arista 8 → 10.Llama a buscar (10) 10 tiene un sucesor, 7, pero 7 ya está marcado como“visitado”. Por ende, buscar (10) se completa estableciendodfn [10] = 10 y c = 9.Regresa a buscar (8) Establece s = 9 y agrega la arista 8 → 9 a T.Llama a buscar (9) El nodo 1, el único sucesor de 9, ya está “visitado”, por loque establece dfn [9] = 9 y c = 8.Regresa a buscar (8) El nodo 3, el último sucesor de 8, está “visitado”, por lo queno hace nada para s = 3. En este punto, todos los sucesoresde 8 se han considerado, por lo que establece dfn [8] = 8 yc = 7.Regresa a buscar (7) Todos los sucesores de 7 se han considerado, por lo queestablece dfn [7] = 7 y c = 6.Regresa a buscar (6) De manera similar, se han considerado todos los sucesores de6, por lo que establece dfn [6] = 6 y c = 5.Regresa a buscar (4) El sucesor 3 de 4 ha sido “visitado”, pero 5 no, por lo queagrega 4 → 5 al árbol.Llama a buscar (5) El sucesor 7 de 5 ha sido “visitado”, por lo cual establecedfn [5] = 5 y c = 4.Regresa a buscar (4) Se han considerado todos los sucesores de 4, establecedfn [4] = 4 y c = 3.Regresa a buscar (3) Establece dfn [3] = 3 y c = 2.Regresa a buscar (1) 2 no se ha visitado todavía, por lo que agrega 1 → 2 a T.Llama a buscar (2) Establece dfn [2] = 2, c = 1Regresa a buscar (1) Establece dfn [1] = 1 y c = 0.Figura 9.44: Ejecución del Algoritmo 9.41 con el grafo de flujo de la figura 9.439.6 Ciclos en los grafos de fl ujo 663Maq. Cap_9_Bok.indd 663 11/10/07 1:03:42 AM664 Capítulo 9. Optimizaciones independientes de la máquinade retirada, y esos son exactamente las aristas posteriores en el grafo. No obstante, si el grafoes no reducible, todas las aristas posteriores son aristas de retirada en cualquier DFST, perocada DFST puede tener aristas de retirada adicionales que no sean aristas posteriores. Estas aristas de retirada pueden ser distintas de un DFST a otro. Por ende, si eliminamos todas las aristasposteriores de un grafo de flujo y el grafo restante es cíclico, entonces el grafo es no reducible,y viceversa.Los grafos de flujo que se dan en la práctica casi siempre son reducibles. El uso exclusivo delas instrucciones de flujo de control estructuradas, como if-then-else, while-do, continue y breakproduce programas cuyos grafos de flujo siempre son reducibles. A menudo, incluso hasta los programas escritos que usan instrucciones goto resultan ser reducibles, ya que el programador piensalógicamente en términos de ciclos y bifurcaciones.Ejemplo 9.43: El grafo de flujo de la figura 9.38 es reducible. Las aristas de retirada en elgrafo son todas aristas posteriores; es decir, sus cabezas dominan a sus respectivas colas. ✷Ejemplo 9.44: Considere el grafo de flujo de la figura 9.45, cuyo nodo inicial es 1. El nodo 1domina a los nodos 2 y 3, pero 2 no domina a 3, ni viceversa. Así, este grafo de flujo no tiene aristas posteriores, ya que ninguna cabeza de ninguna arista domina a su cola. Hay dos posiblesárboles de expansión con búsqueda en profundidad, dependiendo de si elegimos primero llamar abuscar (2) o a buscar (3), desde buscar (1). En el primer caso, la arista 3 → 2 es una arista deretirada pero no una arista posterior; en el segundo caso, 2 → 3 es la arista de retirada, perono posterior. Por intuición, la única razón por la cual este grafo de flujo no es reducible es quepodemos entrar al ciclo 2-3 en dos lugares distintos: los nodos 2 y 3. ✷12 3Figura 9.45: El grafo de flujo canónico, no reducible¿Por qué están las aristas posteriores retirando aristas?Suponga que a → b es una arista posterior, por ejemplo; su cabeza domina a su cola. Lasecuencia de llamadas de la función buscar en la figura 9.43 que lidera al nodo a debe serun camino en el grafo de flujo. Este camino debe, por supuesto, incluir cualquier dominadorde a. Tomando que una llamada a buscar (b) debe ser abierta cuando se llama a buscar (a).Sin embargo; b está en el árbol cuando a es agregada como un descendiente de b. Por ende,a → b debe ser una arista de retirada.Maq. Cap_9_Bok.indd 664 11/14/07 3:56:41 PM9.6.5 Profundidad de un grafo de flujoDado un árbol de expansión con búsqueda en profundidad para el grafo, la profundidad es elnúmero mayor de aristas de retirada en cualquier camino sin ciclos. Podemos probar que laprofundidad nunca es mayor de lo que podríamos intuitivamente llamar la profundidad del anidamiento de los ciclos en el grafo de flujo. Si un grafo de flujo es reducible, podemos sustituir“de retirada” por “posterior” en la definición de “profundidad”, ya que las aristas de retiradaen cualquier DFST son exactamente las aristas posteriores. La noción de profundidad entonces se vuelve independiente del DFST que se haya elegido, y podemos hablar en verdad de la“profundidad de un grafo de flujo”, en vez de la profundidad de un grafo de flujo en conexióncon uno de sus árboles de expansión con búsqueda en profundidad.Ejemplo 9.45: En la figura 9.42, la profundidad es 3, ya que hay un camino10 → 7 → 4 → 3con tres aristas de retirada, pero no hay un camino sin ciclos con cuatro o más aristas de retirada. Es una coincidencia que el camino “más profunda” aquí sólo tenga aristas de retirada; engeneral podemos tener una mezcla de aristas de retirada, de avance y de cruce en un caminomás profunda. ✷9.6.6 Ciclos naturalesLos ciclos pueden especificarse en un programa fuente de muchas formas distintas: puedenescribirse como ciclos for, ciclos while, o ciclos repeat; pueden incluso definirse mediante etiquetas e instrucciones goto. Desde el punto de vista del análisis de un programa, no importacómo aparecen los ciclos en el código fuente. Lo que importa es si tienen las propiedades quepermiten una fácil optimización. En especial, nos preocupamos si un ciclo tiene un solo nodode entrada; si es así, los análisis del compilador pueden asumir ciertas condiciones iniciales paraaplicar al inicio de cada iteración a través del ciclo. Esta oportunidad motiva la necesidad dela definición de un “ciclo natural”.Un ciclo natural se define por dos propiedades esenciales.1. Debe tener un solo nodo de entrada, llamado encabezado. Este nodo de entrada dominaa todos los nodos en el ciclo, o de lo contrario no sería la única entrada para el ciclo.2. Debe haber una arista posterior que entre al encabezado del ciclo. De no ser así, no es posible que el flujo de control regrese al encabezado directamente desde el “ciclo”; es decir,en realidad no hay ciclo.Dado una arista posterior n → d, definimos el ciclo natural de la arista como d más el conjunto de nodos que pueden llegar a n sin pasar a través de d. Observe que d es el encabezadodel ciclo.Algoritmo 9.46: Construcción del ciclo natural de una arista posterior.ENTRADA: Un grafo de flujo G y una arista posterior n → d.9.6 Ciclos en los grafos de fl ujo 665Maq. Cap_9_Bok.indd 665 11/10/07 1:03:43 AM666 Capítulo 9. Optimizaciones independientes de la máquinaSALIDA: El conjunto ciclo, que consiste en todos los nodos en el ciclo natural de n → d.MÉTODO: Haga que ciclo sea {n, d}. Marque d como “visitado”, de manera que la búsquedano vaya más allá de d. Realice una búsqueda por profundidad en el grafo de flujo de controlinverso, empezando con el nodo n. Inserte en ciclo todos los nodos visitados en esta búsqueda.Este procedimiento encuentra todos los nodos que llegan a n sin pasar a través de d. ✷Ejemplo 9.47: En la figura 9.38, hay cinco aristas posteriores, aquellos cuyas cabezas dominan sus colas: 10 → 7, 7 → 4, 4 → 3, 8 → 3 y 9 → 1. Observe que éstos son exactamente lasaristas que consideraríamos que forman ciclos en el grafo de flujo.La arista posterior 10 → 7 tiene el ciclo natural {7, 8, 10}, ya que 8 y 10 son los únicosnodos que pueden llegar a 10 sin pasar a través de 7. La arista posterior 7 → 4 tiene un ciclonatural que consiste en {4, 5, 6, 7, 8, 10} y, por lo tanto, contiene el ciclo de 10 → 7. Por ende,asumimos que este último es un ciclo interno, contenido dentro del anterior.Los ciclos naturales de las aristas posteriores 4 → 3 y 8 → 3 tienen el mismo encabezado,el nodo 3, y también tienen el mismo conjunto de nodos: {3, 4, 5, 6, 7, 8, 10}. Por lo tanto,vamos a combinar estos dos ciclos en uno. Este ciclo contiene los dos ciclos más pequeños quedescubrimos antes.Por último, la arista 9 → 1 tiene a todo el grafo de flujo completo como su ciclo natural, ypor ende es el ciclo más externo. En este ejemplo, los cuatro ciclos están anidados uno dentrode otro. Sin embargo, es común tener dos ciclos, en donde ninguno de los dos sea un subconjunto del otro. ✷En los grafos de flujo reducibles, como todos las aristas de retirada son aristas posteriores,podemos asociar un ciclo natural con cada arista de retirada. Esa instrucción no se aplica paralos grafos no reducibles. Por ejemplo, el grafo de flujo no reducible en la figura 9.45 tiene unciclo que consiste en los nodos 2 y 3. Ninguno de las aristas en el ciclo es una arista posterior,por lo que este ciclo no se ajusta a la definición de un ciclo natural. No identificamos el ciclocomo natural, y no está optimizado como tal. Esta situación es aceptable, ya que nuestrosanálisis de los ciclos pueden simplificarse si asumimos que todos los ciclos tienen nodos de unasola entrada, y de todas formas los programas no reducibles son raros en la práctica.Al considerar sólo los ciclos naturales como “ciclos”, tenemos la útil propiedad de que amenos que dos ciclos tengan el mismo encabezado, están desunidos o uno está anidado dentrodel otro. Por ende, tenemos una noción natural de los ciclos más internos: ciclos que nocontienen otros ciclos.Cuando dos ciclos naturales tienen el mismo encabezado, como en la figura 9.46, es difícildistinguir cuál es el ciclo interno. Por ende, vamos a suponer que cuando dos ciclos naturalestienen el mismo encabezado, y ninguno de ellos está contenido apropiadamente dentro del otro,se combinan y se tratan como un solo ciclo.Ejemplo 9.48: Los ciclos naturales de las aristas posteriores 3 → 1 y 4 → 1 en la figura 9.46son {1, 2, 3} y {1, 2, 4}, respectivamente. Vamos a combinarlos en un solo ciclo, {1, 2, 3, 4}.No obstante, si hubiera otra arista posterior 2 → 1 en la figura 9.46, su ciclo naturalsería {1, 2}, un tercer ciclo con el encabezado 1. Este conjunto de nodos está contenidoMaq. Cap_9_Bok.indd 666 11/10/07 1:03:44 AM123 4Figura 9.46: Dos ciclos con el mismo encabezadoapropiadamente dentro de {1, 2, 3, 4}, por lo que no se combinaría con los demás ciclos naturales, sino que se trataría como un ciclo interno, anidado dentro de ellos. ✷9.6.7 Velocidad de convergencia de los algoritmos de flujosde datos iterativosAhora estamos listos para hablar sobre la velocidad de convergencia de los algoritmos iterativos. Como vimos en la sección 9.3.3, el número máximo de iteraciones que el algoritmo puederealizar es el producto de la altura del lattice y el número de nodos en el grafo de flujo. Paramuchos análisis de flujos de datos, es posible ordenar la evaluación de tal forma que el algoritmo converja en un número mucho más pequeño de iteraciones. La propiedad de interés essi todos los eventos de importancia en un nodo se propagarán a ese nodo, a lo largo de algúncamino acíclico. Entre los análisis de flujos de datos que hemos visto hasta ahora, las definiciones de alcance, las expresiones disponibles y las variables vivas tienen esta propiedad, pero lapropagación de constantes no. Dicho en forma más específica:• Si una definición d está en ENT[B ], entonces hay un camino acíclico desde el bloque quecontiene d hasta B, de tal forma que d se encuentra en todos los ENT y SAL a lo largo deese camino.• Si una expresión x + y no está disponible a la entrada del bloque B, entonces hay uncamino acíclico que demuestra que el camino proviene del nodo de entrada y no incluyeuna instrucción que elimine o genere a x + y, o el camino proviene de un bloque queelimina a x + y y a lo largo de ese camino no hay una generación posterior de x + y.• Si x está viva al salir del bloque B, entonces hay un camino acíclico desde B hasta un usode x, a lo largo de la cual no hay definiciones de x.Deberíamos verificar que en cada uno de estos casos, los caminos con ciclos no agreguen nada.Por ejemplo, si se llega a un uso de x desde el final del bloque B a lo largo de un camino conun ciclo, podemos eliminar ese ciclo para buscar un camino más corta a lo largo de la cual aúnse pueda llegar al uso de x desde B.En contraste, la propagación de constantes no tiene esta propiedad. Considere un programasimple que tiene un ciclo, el cual contiene un bloque básico con las siguientes instrucciones:9.6 Ciclos en los grafos de fl ujo 667Maq. Cap_9_Bok.indd 667 11/10/07 1:03:44 AM668 Capítulo 9. Optimizaciones independientes de la máquinaL: a = b b = c c = 1 goto LLa primera vez que visitamos el bloque básico, encontramos que c tiene el valor constante 1,pero tanto a como b no están definidas. Al visitar el bloque por segunda vez, encontramos queb y c tienen los valores constantes de 1. Se requieren tres visitas al bloque básico para que elvalor constante 1 se asigne a c para llegar a a.Si toda la información se propaga a lo largo de caminos acíclicos, tenemos una oportunidadpara personalizar el orden en el que visitamos los nodos en los algoritmos de flujo de datositerativos, para que después de unas cuantas pasadas a través de los nodos podamos estarseguros de que la información ha pasado a lo largo de todos los caminos acíclicos.En la sección 9.6.3 vimos que si a → b es una arista, entonces el número tipo “primero enprofundidad” de b es menor que el de a, sólo cuando la arista es de retirada. Para los problemasde flujo de datos hacia delante, es conveniente visitar los nodos de acuerdo con el ordenamientotipo “primero en profundidad”. De manera específica, modificamos el algoritmo en la figura9.23(a) sustituyendo la línea (4), la cual visita los bloques básicos en el grafo de flujo con:for (cada bloque B distinto de ENTRADA, en orden “primero en profundidad”) {Ejemplo 9.49: Suponga que tenemos un camino a lo largo de la cual se propaga una definición d, como:3 → 5 → 19 → 35 → 16 → 23 → 45 → 4 → 10 → 17en donde los enteros representan a los números “primero en profundidad” de los bloques a lolargo del camino. Entonces, la primera vez a través del ciclo de las líneas (4) a (6) en el algoritmo de la figura 9.23(a), d se propagará de SAL[3] hacia ENT[5] y luego hacia SAL[5], y así en losucesivo, hasta llegar a SAL[35]. No llegará a ENT[16] en esa ronda, ya que como 16 precede a35, ya habíamos calculado ENT[16] para cuando d se puso en SAL[35]. Sin embargo, la siguientevez que pasamos por el ciclo de las líneas (4) a (6), al calcular ENT[16] se incluirá d, ya queestá en SAL[35]. La definición d también se propagará a SAL[16], ENT[23] y así en lo sucesivo,hasta SAL[45], en donde debe esperar debido a que ENT[4] ya se calculó en esta ronda. En latercera pasada, d va hacia ENT[4], SAL[4], ENT[10], SAL[10] y ENT[17], por lo que después detres pasadas establecemos que d llega al bloque 17. ✷No debe ser difícil extraer el principio general a partir de este ejemplo. Si utilizamos elorden por profundidad en la figura 9.23(a), entonces el número de pasadas necesarias parapropagar cualquier definición de alcance a lo largo de algún camino acíclico no es mayor deuno más que el número de aristas a lo largo de ese camino, que van desde un bloque con mayor numeración, hasta uno con menor numeración. Esas aristas son exactamente las aristas deretirada, por lo que el número de pasadas necesarias es uno más la profundidad. Desde luegoque el Algoritmo 9.11 no detecta el hecho de que todas las definiciones han llegado hasta donde pueden llegar, hasta que una pasada más ya no produce cambios. Por lo tanto, el límitesuperior en el número de pasadas que recibe ese algoritmo con ordenamiento de bloques porMaq. Cap_9_Bok.indd 668 11/10/07 1:03:45 AMprofundidad es en realidad de dos más la profundidad. Un estudio10 ha demostrado que losgrafos de flujo ordinarios tienen una profundidad promedio aproximada de 2.75. Por ende, elalgoritmo converge con mucha rapidez.En el caso de los problemas de flujo hacia atrás, al igual que las variables vivas, visitamoslos nodos en el orden inverso al orden tipo “primero en profundidad”. Por ende, podemos propagar un uso de una variable en el bloque 17, en sentido hacia atrás a lo largo del camino:3 → 5 → 19 → 35 → 16 → 23 → 45 → 4 → 10 → 17en una pasada a ENT[4], en donde debemos esperar la siguiente pasada para poder llegar aSAL[45]. En la segunda pasada llega a ENT[16], y en la tercera pasada va desde SAL[35] hasta SAL[3].En general, basta un número de pasadas igual a uno más la profundidad para llevar el usode una variable hacia atrás, a lo largo de cualquier camino acíclico. Sin embargo, debemoselegir el inverso del orden tipo “primero en profundidad” para visitar los nodos en una pasada, porque así, los usos se propagan a lo largo de cualquier secuencia decremental en una solapasada.El límite descrito hasta ahora es un límite superior en todos los problemas en los que loscaminos cíclicos no agregan información al análisis. En problemas especiales como los dominadores, el algoritmo converge incluso con mayor rapidez. En el caso en el que el grafo del flujode entrada es reducible, el conjunto correcto de dominadores para cada nodo se obtiene en laprimera iteración de un algoritmo de flujo de datos que visita los nodos en orden tipo “primeroen profundidad”. Si no sabemos que la entrada puede reducirse antes de tiempo, se requiereuna iteración adicional para determinar que ha ocurrido la convergencia.9.6.8 Ejercicios para la sección 9.6Ejercicio 9.6.1: Para el grafo de flujo de la figura 9.10 (vea los ejercicios de la sección 9.1): i. Calcule la relación de los dominadores. ii. Encuentre el dominador inmediato de cada nodo.Una razón para los grafos de flujo no reduciblesHay un lugar en el que, por lo general, no podemos esperar que un grafo de flujo sea reducible. Si invertimos las aristas de un grafo de flujo de un programa, como hicimos en elAlgoritmo 9.46 para encontrar ciclos naturales, entonces tal vez no obtendríamos un grafode flujo reducible. La razón intuitiva es que, aunque los programas ordinarios tienen cicloscon una sola entrada, esos ciclos algunas veces tienen varias salidas, que se convierten enentradas cuando invertimos las aristas.10D. E. Knuth, “An empirical study of FORTRAN programs”, Software – Practice and Experience 1:2 (1971),pp. 105-133.9.6 Ciclos en los grafos de fl ujo 669Maq. Cap_9_Bok.indd 669 11/10/07 1:03:45 AM670 Capítulo 9. Optimizaciones independientes de la máquina iii. Construya el árbol de dominadores. iv. Encuentre un ordenamiento tipo “primero en profundidad” para el grafo de flujo. v. Indique las aristas de avance, de retirada, de cruce y de árbol para su respuesta al inciso iv. vi. ¿Es reducible el grafo de flujo? vii. Calcule la profundidad del grafo de flujo. viii. Encuentre los ciclos naturales del grafo de flujo.Ejercicio 9.6.2: Repita el ejercicio 9.6.1 en los siguientes grafos de flujo:a) Figura 9.3.b) Figura 8.9.c) Su grafo de flujo del ejercicio 8.4.1.d) Su grafo de flujo del ejercicio 8.4.2.Ejercicio 9.6.3: Demuestre lo siguiente acerca de la relación dom:a) Si a dom b y b dom c, entonces a dom c (condición transitiva).b) No es posible que tanto a dom b como b dom a sean válidas, si a ≠ b (anti-simetría).c) Si a y b son dos dominadores de n, entonces una de las relaciones a dom b o b dom a debeser válida.d) Cada nodo n (excepto la entrada) tiene un dominador inmediato único: el dominadorque aparece más cerca de n, a lo largo de cualquier camino acíclico desde la entradahasta n.Ejercicio 9.6.4: La figura 9.42 es una presentación “primero en profundidad” del grafo deflujo de la figura 9.38. ¿Cuántas otras presentaciones tipo “primero en profundidad” hay de estegrafo de flujo? Recuerde, el orden de los hijos es importante al distinguir las presentaciones tipo“primero en profundidad”.Ejercicio 9.6.5: Demuestre que un grafo de flujo es reducible si, y sólo si al eliminar todaslas aristas posteriores (aquellos cuyas cabezas dominan a sus colas), el grafo de flujo resultantees acíclico.Ejercicio 9.6.6: Un grafo de flujo completo sobre n nodos tiene los arcos i → j entre dos nodosi y j cualesquiera (en ambas direcciones). ¿Para qué valores de n es reducible este grafo?Ejercicio 9.6.7: Un grafo de flujo acíclico completo sobre n nodos 1, 2,…, n tiene los arcosi → j para todos los nodos i y j tales que i < j. El nodo 1 es la entrada.!!!!!!Maq. Cap_9_Bok.indd 670 11/10/07 1:03:46 AMa) ¿Para qué valores de n es reducible este grafo?b) ¿Cambia su respuesta al inciso (a) si agrega autociclos i → i para todos los nodos i ?Ejercicio 9.6.8: El ciclo natural de una arista posterior n → h se definió como h más el conjunto de nodos que pueden llegar a n sin pasar a través de h. Muestre que h domina a todos losnodos en el ciclo natural de n → h.Ejercicio 9.6.9: Afirmamos que el grafo de flujo de la figura 9.45 no es reducible. Si los arcosse sustituyeran por caminos de nodos separados (excepto los puntos finales, desde luego), entonces el grafo de flujo aún sería no reducible. De hecho, el nodo 1 no necesita ser la entrada;puede ser cualquier nodo alcanzable desde la entrada, a lo largo de un camino cuyos nodosintermedios no forman parte de ninguna de los cuatro caminos que se muestran de manera explícita. Demuestre lo contrario: que todo grafo de flujo no reducible tiene un subgrafo como elde la figura 9.45, pero en donde tal vez los arcos se sustituyan por caminos de nodos separadosy el nodo 1 puede ser cualquier nodo alcanzable desde la entrada, por un camino que tienenodos separados de las otras cuatro caminos.Ejercicio 9.6.10: Muestre que cada presentación “primero en profundidad” para cada grafode flujo no reducible tiene una arista de retirada que no es una arista posterior.Ejercicio 9.6.11: Muestre que si la siguiente condición:f(a) ∧ g (a) ∧ a ≤ f(g (a))se aplica para todas las funciones f y g, y el valor a, entonces el algoritmo iterativo general, elAlgoritmo 9.25, con la iteración después de un ordenamiento tipo “primero en profundidad”,converge dentro de un número de pasadas igual a 2 más la profundidad.Ejercicio 9.6.12: Encuentre un grafo de flujo no reducible con dos DSFTs distintos, quetengan distintas profundidades.Ejercicio 9.6.13: Demuestre lo siguiente:a) Si una definición d está en ENT[B ], entonces hay un camino acíclico desde el bloque quecontiene d hasta B, de tal forma que d está en todos los ENT y SAL a lo largo de esecamino.b) Si una expresión x + y no está disponible en la entrada al bloque B, entonces hay uncamino acíclico que demuestra ese hecho; ya sea que el camino provenga del nodo deentrada y no incluya una instrucción para eliminar o generar a x + y, o que el caminoprovenga de un bloque que elimine a x + y y a lo largo del camino no haya una generación subsiguiente de x + y.c) Si x está viva al salir del bloque B, entonces hay una camino acíclico desde B hasta unuso de x, a lo largo de la cual no hay definiciones de x.9.6 Ciclos en los grafos de fl ujo 671!!!!!!!!!Maq. Cap_9_Bok.indd 671 11/10/07 1:03:46 AM672 Capítulo 9. Optimizaciones independientes de la máquina9.7 Análisis basado en regionesEl algoritmo de análisis de flujo de datos iterativo que hemos visto hasta ahora es sólo un método para resolver los problemas de flujos de datos. Aquí hablaremos sobre otro método llamadoanálisis basado en regiones. Recuerde que en el método del análisis iterativo, creamos funcionesde transferencia para los bloques básicos, y después encontramos la solución del punto fijo mediante varias pasadas a través de los bloques. En vez de crear funciones de transferencia sólopara bloques individuales, un análisis basado en regiones busca funciones de transferencia queresuman la ejecución de regiones cada vez más grandes del programa. En última instancia, seconstruyen funciones de transferencia para procedimientos completos y después se aplican, paraobtener directamente los valores deseados del flujo de datos.Mientras que el marco de trabajo del flujo de datos que utiliza un algoritmo iterativo seespecifica mediante un semi-lattice de valores del flujo de datos y una familia de funciones detransferencia que se cierran bajo la composición, el análisis basado en regiones requiere máselementos. Un marco de trabajo basado en regiones incluye tanto un semi-lattice de valores delflujo de datos como un semi-lattice de funciones de transferencia que deben poseer un operadorde reunión, un operador de composición y un operador de cierre. En la sección 9.7.4 veremoslo que todos estos elementos implican.Un análisis basado en regiones es muy útil para los problemas de flujo de datos en los quelos caminos que tienen ciclos pueden modificar los valores del flujo de datos. El operador decierre permite sintetizar el efecto de un ciclo de una manera más efectiva que el análisis iterativo. La técnica es también útil para el análisis entre procedimientos, en donde las funciones detransferencia asociadas con la llamada a un procedimiento pueden tratarse como las funcionesde transferencia asociadas con los bloques básicos.Por cuestión de simplicidad, vamos a considerar sólo problemas de flujo de datos haciadelante en esta sección. Primero ilustraremos cómo funciona el análisis basado en regiones,mediante el uso del conocido ejemplo de las definiciones de alcance. En la sección 9.8 mostraremos un uso más convincente de esta técnica, cuando estudiemos el análisis de las variablesde inducción.9.7.1 RegionesEn el análisis basado en regiones, un programa se ve como una jerarquía de regiones, que son(aproximadamente) porciones de un grafo de flujo que sólo tienen un punto de entrada. Esteconcepto de ver el código como una jerarquía de regiones nos debe parecer intuitivo, ya que unprocedimiento estructurado por bloques se organiza de manera natural como una jerarquía deregiones. Cada instrucción en un programa estructurado por bloques es una región, ya que el flujode control sólo puede entrar al inicio de una instrucción. Cada nivel de anidamiento de instrucciones corresponde a un nivel en la jerarquía de regiones.De manera formal, una región de un grafo de flujo es una colección de nodos N y aristas E,de tal forma que:1. Hay un encabezado h en N que domina a todos los nodos en N.2. Si algún nodo m puede llegar a un nodo n en N sin pasar a través de h, entonces m también está en N.Maq. Cap_9_Bok.indd 672 11/10/07 1:03:47 AM3. E es el conjunto de todas las aristas del flujo de control entre los nodos n 1 y n 2 en N,excepto (tal vez) algunos que entren a h.Ejemplo 9.50: Es evidente que un ciclo natural es una región, pero una región no tienenecesariamente una arista posterior, y no necesita contener ciclos. Por ejemplo, en la figura9.47, los nodos B 1 y B 2, junto con la arista B 1 → B 2 forman una región; lo mismo pasa con losnodos B 1, B 2 y B 3 con las aristas B 1 → B 2, B 2 → B 3 y B 1 → B 3.Sin embargo, el subgrafo con los nodos B 2 y B 3 con la arista B 2 → B 3 no forma una región,ya que el control puede entrar al subgrafo en ambos nodos B 2 y B 3. Dicho en forma másprecisa, B 2 no domina a B 3 ni viceversa, por lo que se viola la condición (1) para una región.Incluso si eligiéramos, por decir, a B 2 para que fuera el “encabezado”, violaríamos la condición(2), ya que podemos llegar a B 3 desde B 1 sin pasar a través de B 2, y B 1 no se encuentra en la“región”. ✷1 BB 2B 3B 4(ENTRADA)Figura 9.47: Ejemplos de regiones9.7.2 Jerarquías de regiones para grafos de flujo reduciblesA continuación vamos a suponer que el grafo de flujo es reducible. Si en ocasiones debemoslidiar con grafos de flujo no reducibles, entonces podemos usar una técnica conocida como“división de nodos”, que veremos en la sección 9.7.6.Para construir una jerarquía de regiones, identificamos los ciclos naturales. En la sección9.6.6 vimos que en un grafo de flujo reducible, dos ciclos naturales cualesquiera están separadoso uno está anidado dentro del otro. El proceso de “analizar” un grafo de flujo reducible en sujerarquía de ciclos empieza con cada bloque, como una región por sí solo. A estas regiones lesllamamos regiones hoja. Después, ordenamos los ciclos naturales desde el interior hacia fuera;es decir, empezando con los ciclos más internos. Para procesar un ciclo, sustituimos el ciclocompleto por un nodo en dos pasos:1. En primer lugar, el cuerpo del ciclo L (todos los nodos y aristas, excepto las aristas posteriores que van al encabezado) se sustituye por un nodo que representa a una región R.Las aristas que van al encabezado de L ahora entran al nodo para R. Una aristaproveniente de cualquier salida del ciclo L se sustituye por una arista de R que va almismo destino. No obstante, si la arista es una arista posterior, entonces se convierteen un ciclo en R. A R le llamamos región de cuerpo.9.7 Análisis basado en regiones 673Maq. Cap_9_Bok.indd 673 11/10/07 1:03:48 AM674 Capítulo 9. Optimizaciones independientes de la máquina2. A continuación, construimos una región R que representa a todo el ciclo natural L. A R lellamamos región de ciclo. La única diferencia entre R y R es que la última incluye las aristasposteriores que van al encabezado del ciclo L. Dicho de otra forma, cuando R sustituye aR en el grafo de flujo, todo lo que tenemos que hacer es eliminar la arista que va de R a símisma.Procedemos de esta forma, reduciendo ciclos cada vez más grandes a nodos individuales, primero con una arista de ciclo y después sin él. Como los ciclos de un grafo de flujo reducibleestán anidados o separados, el nodo de la región del ciclo puede representar a todos los nodosdel ciclo natural en la serie de grafos de flujo que se construyen mediante este proceso de reducción.En un momento dado, todos los ciclos naturales se reducen a nodos individuales. En esepunto, el grafo de flujo puede reducirse a un solo nodo, o puede haber varios nodos restantes,sin ciclos; es decir, el grafo de flujo reducido es un grafo acíclico de más de un nodo. En elprimer caso terminamos de construir la jerarquía de regiones, mientras que en el último casoconstruimos una región del cuerpo más para todo el grafo de flujo.Ejemplo 9.51: Considere el grafo de flujo de control en la figura 9.48(a). Hay una arista posterior en este grafo de flujo, que conduce de B 4 a B 2. La jerarquía de regiones se muestra en lafigura 9.48(b); las aristas que se muestran son las aristas en los grafos de flujo de las regiones.En total, hay 8 regiones:1. Las regiones R 1,…, R 5 son regiones hoja que representan a los bloques B 1 a B 5, respectivamente. Cada bloque es también un bloque de salida en esta región.2. La región del cuerpo R 6 representa al cuerpo del único ciclo en el grafo de flujo; consisteen las regiones R 2, R 3 y R 4, y tres aristas entre regiones: B 2 → B 3, B 2 → B 4 y B 3 →B 4. Tiene dos bloques de salida, B 3 y B 4, ya que ambos tienen aristas salientes que noestán contenidos en la región. La figura 9.49(a) muestra el grafo de flujo con R 6 reducidaa un solo nodo. Observe que, aunque las aristas R 3 → R 5 y R 4 → R 5 se han sustituidopor la arista R 6 → R 5, es importante recordar que la última arista representa a las dosprimeras aristas, ya que tenemos que propagar las funciones de transferencia a través deesta arista en un momento dado, y debemos saber que lo que sale de ambos bloques B 3y B 4 llegará al encabezado de R 5.3. La región del ciclo R 7 representa a todo el ciclo natural. Incluye una subregión, R 6, yuna arista posterior B 4 → B 2. También tiene dos nodos de salida, de nuevo B 3 y B 4. Lafigura 9.49(b) muestra el grafo de flujo después de que todo el ciclo natural se reducea R 7.4. Por último, la región del cuerpo R 8 es la región superior. Incluye tres regiones, R 1, R 7,R 5 y tres aristas entre regiones, B 1 → B 2, B 3 → B 5 y B 4 → B 5. Cuando reducimos elgrafo de flujo a R 8, se convierte en un solo nodo. Como no hay aristas posteriores quevayan a su encabezado, R 1, no hay necesidad de un paso final que reduzca esta regióndel cuerpo a una región de ciclo.Maq. Cap_9_Bok.indd 674 11/10/07 1:03:49 AMd1d2d3d4d6: i = mï1: j = na = u1: i = i+1: a = u2: j = u3BBBBB12453(ENTRADA)(SALIDA)(a)RRRRRRRR 12453678(b)d5Figura 9.48: (a) Un grafo de flujo de ejemplo para el problema de las definiciones de alcancey (b) su jerarquía de regiones9.7 Análisis basado en regiones 675Maq. Cap_9_Bok.indd 675 11/10/07 1:03:49 A676 Capítulo 9. Optimizaciones independientes de la máquina✷una región del cuerpo una región de ciclo(a) Después de reducir a (b) Después de reducir aRRRRR165175RFigura 9.49: Pasos en la reducción del grafo de flujo de la figura 9.47 a una sola regiónPara resumir el proceso de descomponer los grafos de flujo reducibles en forma jerárquica,ofrecemos el siguiente algoritmo.Algoritmo 9.52: Construir un orden de regiones de abajo hacia arriba, de un grafo de flujoreducible.ENTRADA: Un grafo de flujo reducible G.SALIDA: Una lista de regiones de G que pueden usarse en los problemas de flujo de datosbasados en regiones.MÉTODO:1. Empiece la lista con todas las regiones hoja que consistan en bloques individuales de G,en cualquier orden.2. Elija en forma repetida un ciclo natural L, de tal forma que si hay ciclos naturales contenidos dentro de L, entonces ya se han agregado las regiones de cuerpo y de ciclo de estosciclos a la lista. Agregue primero la región consistente en el cuerpo de L (es decir, L sinlas aristas posteriores que van al encabezado de L), y después la región de ciclo de L.3. Si todo el grafo de flujo no es en sí un ciclo natural, agregue al final de la lista la regiónconsistente en todo el grafo de flujo.✷9.7.3 Generalidades de un análisis basado en regionesPara cada región R, y para cada subregión R dentro de R, calculamos una función de transferencia fR, ENT[R ] que sintetice el efecto de ejecutar todos los caminos posibles que conducen de la entrada de R a la entrada de R, que al mismo tiempo permanecen dentro de R.Maq. Cap_9_Bok.indd 676 11/10/07 1:03:50 AMDecimos que un bloque B dentro de R es un bloque de salida de la región R si tiene una aristasaliente hacia algún bloque fuera de R. También calculamos una función de transferencia paracada bloque de salida B de R, denotado como fR, SAL[B ], que sintetiza el efecto de ejecutar todoslos caminos posibles dentro de R, que conducen de la entrada de R a la salida de B.Después procedemos hacia arriba en la jerarquía de regiones, calculando funciones de transferencia para regiones cada vez más grandes. Empezamos con regiones que sean bloques individuales, en donde fB, ENT[B ] es sólo la función identidad y fB, SAL[B ] es la función de transferenciapara el mismo bloque B. A medida que avanzamos hacia arriba en la jerarquía,• Si R es una región de cuerpo, entonces las aristas que pertenecen a R forman un grafo acíclico en las subregiones de R. Podemos proceder a calcular las funciones de transferenciaen un orden topológico de las subregiones.• Si R es una región de ciclo, entonces sólo tenemos que tomar en cuenta el efecto de lasaristas posteriores que van al encabezado de R.En un momento dado, llegamos a la parte superior de la jerarquía y calculamos las funciones de transferencia para la región R n que constituye el grafo completo. En el Algoritmo 9.53veremos cómo realizar cada uno de estos cálculos.El siguiente paso es calcular los valores del flujo de datos en la entrada y salida de cadabloque. Procesamos las regiones en el orden inverso, empezando con la región R n y avanzandohacia abajo en la jerarquía. Para cada región, calculamos los valores del flujo de datos en laentrada. Para la región R n, aplicamos fR n, ENT[R ] (ENT[ENTRADA]) para obtener los valores delflujo de datos en la entrada de las subregiones R en R n. Repetimos hasta llegar a los bloquesbásicos en las hojas de la jerarquía de regiones.Cuál es el origen de la“reducibilidad”Ahora podemos ver por qué los grafos de flujo reducibles recibieron ese nombre. Aunqueno vamos a demostrar este hecho, la definición de “grafo de flujo reducible” utilizadaen este libro, que involucra a las aristas posteriores del grafo, es equivalente a variasdefiniciones en las que reducimos de manera mecánica el grafo de flujo a un solo nodo.El proceso de colapsar los ciclos naturales descritos en la sección 9.7.2 es uno de ellos.Otra definición interesante es que los grafos de flujo reducibles son los únicos grafos quepueden reducirse a un solo nodo mediante las siguientes dos transformaciones:T1: Eliminar una arista que va de un nodo a sí mismo.T2: Si el nodo n tiene un solo predecesor m, y n no es la entrada del grafo de flujo,combinar m y n.9.7 Análisis basado en regiones 677Maq. Cap_9_Bok.indd 677 11/10/07 1:03:51 AM678 Capítulo 9. Optimizaciones independientes de la máquina9.7.4 Suposiciones necesarias sobre las funciones transformaciónPara que el análisis basado en regiones pueda funcionar, debemos hacer varias suposiciones sobrelas propiedades del conjunto de funciones de transferencia en el marco de trabajo. En específico,necesitamos tres operaciones primitivas relacionadas con las funciones de transferencia: composición, reunión y cerradura; sólo la primera se requiere para los marcos de trabajo de flujo de datosque utilizan el algoritmo iterativo.ComposiciónLa función de transferencia de una secuencia de nodos puede derivarse mediante la composiciónde las funciones que representan a los nodos individuales. Hagamos que f 1 y f 2 sean las funciones de transferencia de los nodos n 1 y n 2. El efecto de ejecutar n 1 seguido de n 2 se representamediante f 2 ° f 1. En la sección 9.2.2 hablamos sobre la composición de funciones, y en la sección9.2.4 se mostró un ejemplo mediante el uso de las definiciones de alcance. Para repasar, hagamos que geni y eliminar i sean los conjuntos gen y eliminar para fi. Entonces: f 2 ° f 1(x ) = gen 2 ∪ ((gen 1 ∪ (x − eliminar 1)) − eliminar 2) = (gen 2 ∪ (gen 1 − eliminar 2)) ∪ (x − (eliminar 1 ∪ eliminar 2))Por ende, los conjuntos gen y eliminar para f 2 ° f 1 son gen 2 ∪ (gen 1 − eliminar 2) y eliminar 1 ∪eliminar 2, respectivamente. La misma idea funciona para cualquier función de transferencia dela forma gen-eliminar. También pueden cerrarse otras funciones de transferencia, pero tenemosque considerar cada caso por separado.ReuniónAquí, las mismas funciones de transferencia son valores de un semi-lattice con un operador dereunión ∧f. La reunión de dos funciones de transferencia f 1 y f 2, f 1 ∧f f 2, se define mediante(f 1 ∧f f 2)(x ) = f 1(x ) ∧ f 2(x ), en donde ∧ es el operador de reunión para los valores del flujode datos. El operador de reunión en las funciones de transferencia se utiliza para combinarel efecto de los caminos alternativas de ejecución con los mismos puntos finales. En donde nosea ambiguo, de ahora en adelante nos referiremos al operador de reunión de las funciones detransferencia también como ∧. Para el marco de trabajo de las definiciones de alcance, tenemosque: (f 1 ∧ f 2)(x ) = f 1(x ) ∧ f 2(x ) = (gen 1 ∪ (x − eliminar 1)) ∪ (gen 2 ∪ (x − eliminar 2)) = (gen 1 ∪ gen 2) ∪ (x − (eliminar 1 ∩ eliminar 2))Es decir, los conjuntos gen y eliminar para f 1 ∧ f 2 son gen 1 ∪ gen 2 y eliminar 1 ∩ eliminar 2,respectivamente. De nuevo, se aplica el mismo argumento para cualquier conjunto de funcionesde transferencia gen-eliminar.Maq. Cap_9_Bok.indd 678 11/10/07 1:03:52 AMCerraduraSi f representa a la función de transferencia de un ciclo, entonces f n representa el efecto de recorrer el ciclo n veces. En el caso en el que no se conoce el número de iteraciones, tenemos quesuponer que el ciclo puede ejecutarse 0 o más veces. Representamos la función de transferenciade dicho ciclo mediante f *, la cerradura de f, que se define por:Observe que f 0 debe ser la función de transferencia identidad, ya que representa el efectode recorrer el ciclo cero veces; es decir, empezar en la entrada y no moverse. Si dejamos que Irepresente a la función de transferencia identidad, entonces podemos escribir:Suponga que la función de transferencia f en un marco de trabajo de definiciones de alcancetiene un conjunto gen y un conjunto eliminar. Entonces,f 2(x ) = f(f(x )) = (gen ∪ (( gen ∪ (x − eliminar)) − eliminar) = gen ∪ (x − eliminar)f 3(x ) = f(f 2(x )) = gen ∪ (x − eliminar)y así sucesivamente: cualquier f n(x ) es gen ∪ (x − eliminar). Es decir, al recorrer un ciclo nose ve afectada la función de transferencia, si es de la forma gen-eliminar. Por lo tanto,f *(x ) = I ∧ f 1(x ) ∧ f 2(x ) ∧ … = x ∪ (gen ∪ (x − eliminar)) = gen ∪ xEs decir, los conjuntos gen y eliminar para f * son gen y ∅, respectivamente. Por intuición,como en definitiva no podemos recorrer un ciclo, cualquier cosa en x alcanzará la entrada alciclo. En todas las iteraciones siguientes, las definiciones de alcance incluyen a las que estánen el conjunto gen.9.7 Análisis basado en regiones 679Maq. Cap_9_Bok.indd 679 11/10/07 1:03:53 AM680 Capítulo 9. Optimizaciones independientes de la máquina9.7.5 Un algoritmo para el análisis basado en regionesEl siguiente algoritmo resuelve un problema de análisis de flujo de datos hacia delante en ungrafo de flujo reducible, de acuerdo con cierto marco de trabajo que cumple con las suposicionesde la sección 9.7.4. Recuerde que fR, ENT[R] y fR, SAL[B ] se refieren a las funciones de transferenciaque transforman los valores del flujo de datos en la entrada a la región R, en el valor correctoen la entrada de la subregión R y en la salida del bloque de salida B, respectivamente.Algoritmo 9.53: Análisis basado en regiones.ENTRADA: Un marco de trabajo del flujo de datos, con las propiedades descritas en la sección9.7.4 y un grafo de flujo reducible G.SALIDA: Valores del flujo de datos ENT[B ] para cada bloque B de G.MÉTODO:1. Use el Algoritmo 9.52 para construir la secuencia de abajo hacia arriba de regiones de G,por decir R 1, R 2,…, R n, en donde R n es la región de más arriba.2. Realice el análisis ascendente para calcular las funciones de transferencia, sintetizando elefecto de ejecutar una región. Para cada región R 1, R 2,…, R n, en el orden ascendente,haga lo siguiente: (a) Si R es una región hoja que corresponde al bloque B, haga que fR, ENT[B ] = I y fR, SAL[B ]= fB, la función de transferencia asociada con el bloque B. (b) Si R es una región de cuerpo, realice el cálculo de la figura 9.50(a). (c) Si R es una región de ciclo, realice el cálculo de la figura 9.50(b).3. Realice la pasada de arriba hacia abajo para buscar los valores del flujo de datos al iniciode cada región. (a) ENT[R n] = ENT[ENTRADA]. (b) Para cada región R en {R 1,…, R n−1}, en el orden descendente, calcule ENT[R ] =fR, ENT[R ] (ENT[R]), en donde R es la región de cerradura inmediata de R.Primero vamos a ver los detalles del funcionamiento del análisis ascendente. En la línea (1)de la figura 9.50(a) visitamos las subregiones de una región de cuerpo, en cierto orden topológico.La línea (2) calcula la función de transferencia que representa todos los posibles caminos desdeel encabezado de R hasta el encabezado de S; después en las líneas (3) y (4) calculamos las funciones de transferencia que representan todos los posibles caminos que van del encabezado deR a las salidas de R; es decir, a las salidas de todos los bloques que tienen sucesores fuera de S.Observe que todos los predecesores B en R deben estar en regiones que precedan a S en el ordentopológico construido en la línea (1). Por ende, fR, SAL[B] ya se habría calculado en la línea (4) deuna iteración anterior, a través del ciclo externo.Maq. Cap_9_Bok.indd 680 11/10/07 1:03:54 AMPara las regiones de ciclo, realizamos los pasos de las líneas (1) a (4) en la figura 9.50(b). Lalínea (2) calcula el efecto de recorrer la región S de cuerpo del ciclo cero o más veces. Las líneas(3) y (4) calculan el efecto en las salidas del ciclo, después de una o más iteraciones.En la pasada de arriba hacia abajo del algoritmo, el paso 3(a) primero asigna la condicióndelimitadora a la entrada de la región de más arriba. Después, si R está contenido inmediatamente en R, simplemente podemos aplicar la función de transferencia fR, ENT[R] al valor delflujo de datos ENT[R ] para calcular ENT[R ]. ✷1) for (cada subregión S que está contenida inmediatamente en R, en orden topológico) {2) fR, ENT[S ] = ∧predecesores B en R del encabezado de S fR, SAL[B ]; /* si S es el encabezado de la región R, entonces fR, ENT[S ] es la reunión sobre nada, lo cual viene siendo la función identidad */3) for (cada bloque de salida B en S)4) fR, SAL[B ] = fS, SAL[B ] ° fR, ENT[S ]; }(a) Construcción de funciones de transferencia para una región de cuerpo R1) let S sea la región del cuerpo inmediatamente anidada dentro de R; es decir,S es R sin las aristas posteriores que van de R al encabezado de R;2) fR, ENT[S ] = (∧predecesores B en R del encabezado de S fS, SAL[B ])*;3) for (cada bloque de salida B en R)4) fR, SAL[B ] = fS, SAL[B ] ° fR, ENT[S ];(b) Construcción de funciones de transferencia para una región de ciclo RFigura 9.50: Detalles de los cálculos de flujo de datos basado en regionesEjemplo 9.54: Vamos a aplicar el Algoritmo 9.53 para buscar definiciones de alcance en elgrafo de flujo de la figura 9.48(a). El paso 1 construye el orden ascendente en el que se visitanlas regiones; este orden será el orden numérico de sus subíndices, R 1, R 2,…, R n.A continuación se resumen los valores de los conjuntos gen y eliminar para los cinco bloques:eliminarRecuerde las reglas simplificadas para las funciones de transferencia gen-eliminar, de la sección9.7.4:9.7 Análisis basado en regiones 681Maq. Cap_9_Bok.indd 681 11/10/07 1:03:54 AM682 Capítulo 9. Optimizaciones independientes de la máquina• Para tomar la reunión de funciones de transferencia, tome la unión de los conjuntos geny la intersección de los conjuntos eliminar.• Para componer funciones de transferencia, tome la unión de los conjuntos gen y eliminar. Sin embargo, como excepción, una expresión generada por la primera función, nogenerada por la segunda, pero eliminada por ésta no se encuentra en el conjunto gen delresultado.• Para tomar la cerradura de una función de transferencia, retenga su conjunto gen y sustituya el conjunto eliminar por ∅.Las primeras cinco regiones R 1,…, R 5 son los bloques B 1,…, B 5, respectivamente. Para1 ≤ i ≤ 5, fRi, ENT[Bi ] es la función de identidad, y fRi, SAL[Bi ] es la función de transferencia para elbloque Bi:FBi, SAL[Bi ](x ) = (x − eliminarBi) ∪ genBi.SALSALSALSALSALSALSALSALSALSALSALSALSALSALENTENTSALENTENTENTENTENTSALENTSALENTSALENTSALENTSALSALENTSALENTSALSALENTSALENTFunción de transferencia eliminarFigura 9.51: Cálculo de funciones de transferencia para el grafo de flujo en la figura 9.48(a),usando el análisis basado en regionesEl resto de las funciones de transferencia construidas en el paso 2 del Algoritmo 9.50 seresumen en la figura 9.51. La región R 6, que consiste en las regiones R 2, R 3 y R 4, representaMaq. Cap_9_Bok.indd 682 11/10/07 1:03:55 AMel cuerpo del ciclo y, por lo tanto, no incluye la arista posterior B 4 → B 2. El orden para procesar estas regiones será el único orden topológico: R 2, R 3, R 4. En primer lugar, R 2 no tienepredecesores dentro de R 6; recuerde que la arista B 4 → B 2 va hacia fuera de R 6. Por lo tanto,fR6,ENT[B2] es la función de identidad,11 y fR6,SAL[B2] es la función de transferencia para el mismo bloque B 2.El encabezado de la región B 3 tiene un predecesor dentro de R 6; a saber, R 2. La funciónde transferencia a su entrada es sólo la función de transferencia a la salida de B 2, fR6,SAL[B2],que ya se ha calculado. Componemos esta función con la función de transferencia de B 3 dentrode su propia región, para calcular la función de transferencia a la salida de B 3.Por último, para la función de transferencia a la entrada de R 4, debemos calcular:fR6,SAL[B2] ∧ f R6,SAL[B3]ya que tanto B 2 como B 3 son predecesores de B 4, el encabezado de R 4. Esta función de transferencia está compuesta con la función de transferencia fR4,SAL[B4] para obtener la funcióndeseada fR6,SAL[B4]. Por ejemplo, observe que d 3 no se elimina en esta función de transferencia,ya que el camino B 2 → B 4 no define de nuevo a la variable a.Ahora, considere la región de ciclo R 7. Sólo contiene una subregión R 6, que representa alcuerpo de su ciclo. Como sólo hay una arista posterior, B 4 → B 2 que va al encabezado de R 6,la función de transferencia que representa la ejecución del cuerpo del ciclo 0 o más veces es sólof *R6,SAL[B4]: el conjunto gen es {d 4, d 5, d 6} y el conjunto eliminar es ∅. Hay dos salidas de laregión R 7, los bloques B 3 y B 4. En consecuencia, esta función de transferencia está compuestacon cada una de las funciones de transferencia de R 6 para obtener las funciones correspondientes de transferencia de R 7. Por ejemplo, observe cómo d 6 está en el conjunto gen para fR7, R3debido a caminos como B 2 → B 4 → B 2 → B 3, o inclusive B 2 → B 3 → B 4 → B 2 → B 3.Por último, considere a R 8, el grafo de flujo completo. Sus subregiones son R 1, R 7 y R 5, queconsideraremos en ese orden topológico. Como antes, la función de transferencia fR8,ENT[B1] essimplemente la función de identidad, y la función de transferencia fR8,SAL[B1] es sólo fR1,SAL[B1],que a su vez es fB1.El encabezado de R 7, que es B 2, sólo tiene un predecesor, B 1, por lo que la función de transferencia a su entrada es simplemente la función de transferencia que sale de B 1 en la regiónR 8. Componemos fR8,SAL[B1] con las funciones de transferencia a las salidas de B 3 y B 4 dentrode R 7, para obtener sus funciones correspondientes de transferencia dentro de R 8. Por último,consideramos a R 5. Su encabezado, B 5, tiene dos predecesores dentro de R 8, es decir R 3 y R 4.Por lo tanto, calculamos fR8,SAL[B3] ∧ fR8,SAL[B4] para obtener fR8,ENT[B5]. Como la función detransferencia del bloque B 5 es la función de identidad, fR8,SAL[B5] = fR8,ENT[B5].El paso 3 calcula las definiciones de alcance actuales de las funciones de transferencia. Enel paso 3(a), ENT[R 8] = ∅, ya que no hay definiciones de alcance al inicio del programa. Lafigura 9.52 muestra cómo el paso 3(b) calcula el resto de los valores del flujo de datos. El pasoempieza con las subregiones de R 8. Como se ha calculado la función de transferencia desde el11Hablando en sentido estricto, queremos decir f R6 ,ENT[R2 ], pero cuando una región como R2 es un solo bloque,a menudo es más claro si utilizamos el nombre del bloque en vez del nombre de la región en este contexto.9.7 Análisis basado en regiones 683Maq. Cap_9_Bok.indd 683 11/10/07 1:03:58 AM684 Capítulo 9. Optimizaciones independientes de la máquinainicio de R 8 hasta el inicio de cada una de sus subregiones, una sola aplicación de la funciónde transferencia encuentra el valor del flujo de datos al inicio de cada subregión. Repetimos lospasos hasta obtener los valores del flujo de datos de las regiones hoja, que sólo son los bloquesbásicos individuales. Observe que los valores del flujo de datos que se muestran en la figura9.52 son exactamente lo que obtendríamos si aplicáramos el análisis de flujo de datos iterativoal mismo grafo de flujo, como desde luego debe ser el caso. ✷ENTENTENTENTENTENTENTENTENTENTENTENTENTENTENTENTENTENTENTENTENTENTFigura 9.52: Pasos finales del análisis de flujo basado en regiones9.7.6 Manejo de grafos de flujo no reduciblesSi se espera que los grafos de flujo no reducibles sean comunes para los programas que se van aprocesar por un compilador o cualquier otro software de procesamiento de programas, entoncesrecomendamos el uso de un método iterativo en vez de uno basado en jerarquías, para el análisis de flujos de datos. No obstante, si sólo necesitamos prepararnos para el grafo de flujo noreducible ocasional, entonces la siguiente técnica de “división de nodos” es adecuada.Construya regiones desde los ciclos naturales hasta la mayor extensión posible. Si el grafode flujo no es reducible, encontraremos que el grafo resultante de regiones tiene ciclos, pero noaristas posteriores, por lo que no podemos analizar más el grafo. En la figura 9.53(a) se sugiereuna situación ordinaria, la cual tiene la misma estructura que el grafo de flujo no reducible dela figura 9.45, pero los nodos en la figura 9.53 pueden de hecho ser regiones complejas, como losugieren los nodos más pequeños en su interior.Elegimos cierta región R que tenga más de un predecesor y que no sea el encabezado delgrafo de flujo completo. Si R tiene k predecesores, haga k copias del grafo de flujo R completo,y conecte cada predecesor del encabezado de R a una copia distinta de R. Recuerde que sólo elencabezado de una región podría llegar a tener un predecesor fuera de esa región. Resulta que,aun cuando no lo demostraremos, dicha división de nodos es una reducción de cuando menosuno en el número de regiones, después de identificar aristas nuevas posteriores y construir susregiones. El grafo resultante puede incluso no ser reducible, pero mediante la acción de alternaruna fase de división con una fase en donde se identifican los nuevos ciclos naturales y se colapsan con regiones, en un momento dado nos quedamos con una sola región; es decir, el grafo deflujo se ha reducido.Maq. Cap_9_Bok.indd 684 11/10/07 1:03:59 AMR 2R 3(a) (b)R RRRR1312a2bFigura 9.53: Duplicar una región para hacer que un grafo de flujo no reducible pueda reducirseEjemplo 9.55: La división que se muestra en la figura 9.53(b) ha convertido la arista R 2b → R 3en una arista posterior, ya que R 3 ahora domina a R 2b. Por lo tanto, estas dos regiones debencombinarse en una sola. Las tres regiones resultantes (R 1, R 2a y la nueva región) forman un grafoacíclico y, por lo tanto, pueden combinarse en una sola región de cuerpo. Por ende, hemos reducidoel grafo de flujo completo a una sola región. En general, puede ser necesario realizar divisiones adicionales y, en el peor de los casos, el número total de bloques básicos podría volverse exponencialen el número de bloques en el grafo de flujo original. ✷También debemos pensar acerca de cómo se relaciona el resultado del análisis del flujo dedatos en el grafo de flujo dividido con la respuesta que deseamos para el grafo de flujo original.Hay dos métodos que podríamos considerar.1. Dividir las regiones puede ser benéfico para el proceso de optimización, y sólo debemosmodificar el grafo de flujo para tener copias de ciertos bloques. Como sólo se entra a cadabloque duplicado a lo largo de un subconjunto de los caminos que llegaron al original,los valores del flujo de datos en estos bloques duplicados tenderán a contener más información específica de la que estaba disponible en el original. Por ejemplo, pueden llegarmenos definiciones a cada uno de los bloques duplicados que al bloque original.2. Si deseamos retener el grafo de flujo original, sin división, entonces después de analizarel grafo de flujo dividido, analizamos cada bloque B dividido, junto con su conjunto correspondiente de bloques B 1, B 2,…, Bk. Podemos calcular ENT[B ] = ENT[B 1] ∧ ENT[B 2]∧ … ∧ ENT[Bk], y de manera similar para los conjuntos SAL.9.7 Análisis basado en regiones 685Maq. Cap_9_Bok.indd 685 11/10/07 1:04:01 AM686 Capítulo 9. Optimizaciones independientes de la máquina9.7.7 Ejercicios para la sección 9.7Ejercicio 9.7.1: Para el grafo de flujo de la figura 9.10 (vea los ejercicios de la sección 9.1): i. Encuentre todas las regiones posibles. Sin embargo, puede omitir de la lista las regionesque consistan en un solo nodo y sin aristas. ii. Proporcione el conjunto de regiones anidadas construidas por el Algoritmo 9.52.iii. Proporcione una reducción T1-T2 del grafo de flujo, según la descripción en el recuadro“Cuál es el origen de la ‘reducibilidad’” en la sección 9.7.3.Ejercicio 9.7.2: Repita el ejercicio 9.7.1 con los siguientes grafos de flujo:a) Figura 9.3.b) Figura 8.9.c) Su grafo de flujo del ejercicio 8.4.1.d) Su grafo de flujo del ejercicio 8.4.2.Ejercicio 9.7.3: Demuestre que todo ciclo natural es una región.Ejercicio 9.7.4: Muestre que un grafo de flujo es reducible si, y sólo si puede transformarseen un nodo individual mediante:a) Las operaciones T1 y T2 descritas en el cuadro de la sección 9.7.3.b) La definición de región introducida en la sección 9.7.3.Ejercicio 9.7.5: Muestre que al aplicar la división de nodos a un grafo de flujo no reducible,y después realizar la reducción T1-T2 sobre el grafo dividido resultante, siempre terminamoscon menos nodos de los que había al empezar.Ejercicio 9.7.6: ¿Qué ocurre si aplica la división de nodos y la reducción T1-T2 de maneraalternativa, para reducir un grafo dirigido completo de n nodos?9.8 Análisis simbólicoVamos a usar el análisis simbólico en esta sección para ilustrar el uso del análisis basado enregiones. En este análisis, rastreamos los valores de las variables en programas de manerasimbólica, como expresiones de variables de entrada y otras variables, a las cuales llamaremosvariables de referencia. Al expresar las variables en términos del mismo conjunto de variables dereferencia se sacan sus relaciones. El análisis simbólico puede usarse para varios propósitos comola optimización, la paralelización y análisis para la comprensión de los programas.!!!!Maq. Cap_9_Bok.indd 686 11/10/07 1:04:02 AM1) x = input();2) y = x−1;3) z = y−1;4) A[x] = 10;5) A[y] = 11;6) if (z > x)7) z = x;Figura 9.54: Un programa de ejemplo que motiva el análisis simbólicoEjemplo 9.56: Considere el programa simple de ejemplo de la figura 9.54. Aquí, utilizamos xcomo la única variable de referencia. El análisis simbólico encontrará que y tiene el valor x − 1 yz tiene el valor x − 2 después de sus respectivas instrucciones de asignación en las líneas (2) y (3).Por ejemplo, esta información es útil para determinar que las dos asignaciones en las líneas (4) y(5) escriben en distintas ubicaciones de memoria y, por lo tanto, pueden ejecutarse en paralelo.Además, podemos determinar que la condición z > x nunca es verdadera, con lo cual el optimizador puede eliminar la instrucción condicional en las líneas (6) y (7). ✷9.8.1 Expresiones afines de las variables de referenciaComo no podemos crear expresiones simbólicas concisas y de forma cerrada para todos los valores calculados, elegimos un domino abstracto y aproximamos los cálculos con las expresionesmás precisas dentro del dominio. Ya hemos visto un ejemplo de esta estrategia antes: la propagación de constantes. En la propagación de constantes, nuestro dominio abstracto consiste enlas constantes, un símbolo UNDEF si no hemos determinado aún que el valor es una constante,y un símbolo especial NAC que se utiliza cada vez que se descubre que una variable no es unaconstante.El análisis simbólico que presentamos aquí expresa los valores como expresiones afines devariables de referencia siempre que sea posible. Una expresión es afín con respecto a las variables v1, v2,…, v n si puede expresarse como c0 + c 1v1 + … + cnvn, en donde c0, c1,…, c n sonconstantes. Dichas expresiones se conocen de manera informal como expresiones lineales. Hablando en sentido estricto, una expresión afín es lineal sólo si c 0 es cero. Estamos interesados enlas expresiones afines, ya que a menudo se utilizan para indexar los arreglos en los ciclos; dichainformación es útil para las optimizaciones y la paralelización. En el capítulo 11 hablaremosmucho más acerca de este tema.Variables de inducciónEn vez de utilizar variables del programa como variables de referencia, una expresión afín también puede escribirse en términos de la cuenta de iteraciones a través del ciclo. Las variablescuyos valores pueden expresarse como c 1i + c 0, en donde i es la cuenta de iteraciones a travésdel ciclo circundante más cercano, se conocen como variables de inducción.Ejemplo 9.57: Considere el siguiente fragmento de código:9.8 Análisis simbólico 687Maq. Cap_9_Bok.indd 687 11/10/07 1:04:02 AM688 Capítulo 9. Optimizaciones independientes de la máquinafor (m = 10; m < 20; m++){ x = m*3; A[x] = 0; }Suponga que introducimos una variable para el ciclo, por decir i, para representar el númerode iteraciones ejecutadas. El valor i es 0 en la primera iteración del ciclo, 1 en la segunda, yasí en lo sucesivo. Podemos expresar la variable m como una expresión afín de i, es decir m =i + 10. La variable x, que es 3m, toma los valores 30, 33,…, 57 durante iteraciones sucesivasdel ciclo. Por ende, x tiene la expresión afín x = 30 + 3i. Concluimos que tanto m como x sonvariables de inducción de este ciclo. ✷Al expresar las variables como expresiones afines de los índices de un ciclo, la serie de valores que se calculan se hace explícita y permite varias transformaciones. La serie de valores quetoma una variable de inducción puede calcularse con sumas, en vez de multiplicaciones. Estatransformación se conoce como “reducción de fuerza”, y se presentó en las secciones 8.7 y 9.1.Por ejemplo, podemos eliminar la multiplicación x=m*3 del ciclo del ejemplo 9.57, rescribiendoel ciclo como:x = 27;for (m = 10; m < 20; m++) { x = x+3; A[x] = 0; }Además, observe que las ubicaciones a las que se asigna 0 en ese ciclo, &A+30, &A+33,…,&A+57, son también expresiones afines del índice de ciclo. De hecho, esta serie de enteros essólo una que debe calcularse; no necesitamos a m o a x. El código anterior puede sustituirse tan sólo por:for (x = &A+30; x <= &A+57; x = x+3) *x = 0;Además de agilizar el cálculo, el análisis simbólico también es útil para la paralelización.Cuando los índices de un arreglo en un ciclo son expresiones afines de los índices del ciclo,podemos razonar acerca de las relaciones de los datos a los que se accede a través de las iteraciones. Por ejemplo, podemos saber que las ubicaciones escritas son distintas en cada iteracióny, por lo tanto, todas las iteraciones en el ciclo pueden ejecutarse en paralelo, en distintosprocesadores. Dicha información se utiliza en los capítulos 10 y 11 para extraer el paralelismode los programas secuenciales.Otras variables de referenciaSi una variable no es una función lineal de las variables de referencia que ya se han elegido,tenemos la opción de tratar su valor como referencia para operaciones futuras. Por ejemplo,considere el siguiente fragmento de código:a = f ();b = a + 10;c = a + 11;Maq. Cap_9_Bok.indd 688 11/10/07 1:04:03 AMAunque el valor que contiene a después de la llamada a la función no puede en sí expresarsecomo una función lineal de cualquier variable de referencia, puede usarse como referencia paralas instrucciones siguientes. Por ejemplo, si usamos a como variable de referencia, podemosdescubrir que c es un número más grande que b al final del programa.1) a = 0;2) for (f = 100; f < 200; f++) {3) a = a + 1;4) b = 10 * a;5) c = 0;6) for (g = 10; g < 20; g++) {7) d = b + c;8) c = c + 1; } }Figura 9.55: Código fuente para el ejemplo 9.58Ejemplo 9.58: Nuestro ejemplo para esta sección se basa en el código fuente que se muestraen la figura 9.55. Los ciclos interno y externo son fáciles de comprender, ya que f y g no semodifican, excepto cuando lo requieran los ciclos for. Por ende, es posible sustituir f y g porlas variables de referencia i y j, que cuentan el número de iteraciones de los ciclos externo einterno, respectivamente. Es decir, podemos dejar que f = i + 99 y g = j + 9, y sustituir paraf y g durante la ejecución. Al traducir a código intermedio, podemos aprovechar el hecho deque cada ciclo itera por lo menos una vez, para así posponer la prueba para i ≤ 100 y j ≤ 10hasta el final de los ciclos. La figura 9.56 muestra el grafo de flujo para el código de la figura9.55, después de introducir i y j y tratar los ciclos for como si fueran ciclos repeat.Resulta que a, b, c y d son todas variables de inducción. Las secuencias de valores asignados a las variables en cada línea del código se muestran en la figura 9.57. Como veremos,es posible descubrir las expresiones afines para esas variables, en términos de las variables dereferencia i y j. Es decir, en la línea (4) a = i, en la línea (7) d = 10i + j − 1, y en la línea (8),c = j. ✷9.8.2 Formulación del problema de flujo de datosEste análisis encuentra las expresiones afines de las variables de referencia introducidas (1)para contar el número de iteraciones ejecutadas en cada ciclo, y (2) para guardar valores ala entrada de las regiones, cuando sea necesario. Este análisis también encuentra variables deinducción, invariantes de ciclo, así como constantes, como expresiones afines degeneradas. Tenga en cuenta que este análisis no puede encontrar todas las constantes, ya que sólo rastrea lasexpresiones afines de las variables de referencia.9.8 Análisis simbólico 689Maq. Cap_9_Bok.indd 689 11/10/07 1:04:03 AM690 Capítulo 9. Optimizaciones independientes de la máquinaB 3B 2B 1B 2B 3B 4R 5R 7R 8R 6a = 0i = 1a = a + 1b = 10 * ac = 0j = 1d = b + cif j<10 gotoj = j + 1c = c + 1i = i + 1if i<100 gotoFigura 9.56: Grafo de flujo y su jerarquía de regiones para el ejemplo 9.58Valores de flujo de datos: mapas simbólicosEl dominio de los valores de flujo de datos para este análisis es el de los mapas simbólicos, queson funciones que asignan cada variable en el programa a un valor. El valor puede ser una función afín de valores de referencia, o el símbolo especial NAA para representar a una expresiónno afín. Si sólo hay una variable, el valor inferior del semi-lattice es un mapa que envía la variable a NAA. El semi-lattice para n variables es simplemente el producto de los semi-lattices individuales. Utilizamos mNAA para denotar la parte inferior del semi-lattice, que asigna todas lasvariables a NAA. Podemos definir el mapa simbólico que envía todas las variables a un valordesconocido para que sea el valor superior del flujo de datos, como hicimos con la propagación deconstantes. Sin embargo, no necesitamos valores superiores en el análisis basado en regiones.Ejemplo 9.59: Los mapas simbólicos asociados con cada bloque para el código del ejemplo9.58 se muestran en la figura 9.58. Más adelante veremos cómo se descubren estos mapas; sonel resultado de realizar un análisis de flujo de datos basado en regiones en el grafo de flujo de lafigura 9.56.Maq. Cap_9_Bok.indd 690 11/10/07 1:04:04 AMlíneaFigura 9.57: Secuencia de valores vistos en puntos del programa del ejemplo 9.58ENTSALENTSALENTSALENTSALFigura 9.58: Mapas simbólicos del programa en el ejemplo 9.58El mapa simbólico asociado con la entrada del programa es mNAA. A la salida de B 1, elvalor de a se establece a 0. Al entrar al bloque B 2, a tiene el valor 0 en la primera iteración yse incrementa en uno en cada iteración siguiente del ciclo externo. Por ende, a tiene el valori − 1 al entrar a la i-ésima iteración y el valor i al final. El mapa simbólico a la entrada de B 2asigna las variables b, c, d a NAA, ya que estas variables tiene valores desconocidos al entrar alciclo interno. Sus valores dependen del número de iteraciones del ciclo externo, hasta ahora. Elmapa simbólico al salir de B 2 refleja las instrucciones de asignación a a, b y c en ese bloque.El resto de los mapas simbólicos se puede deducir de una manera similar. Una vez establecida lavalidez de los mapas en la figura 9.58, podemos sustituir cada una de las asignaciones a a, b, cy d en la figura 9.55 por las expresiones afines apropiadas. Es decir, podemos sustituir la figura9.55 por el código de la figura 9.59. ✷Función de transferencia de una instrucciónLas funciones de transferencia en este problema de flujo de datos envían mapas simbólicos a losmapas simbólicos. Para calcular la función de transferencia de una instrucción de asignación,interpretamos la semántica de la instrucción y determinamos si las variables asignadas puedenexpresarse como una expresión afín de los valores a la derecha de la asignación. Los valores detodas las demás variables permanecen sin cambio.9.8 Análisis simbólico 691Maq. Cap_9_Bok.indd 691 11/10/07 1:04:05 AM692 Capítulo 9. Optimizaciones independientes de la máquinaFigura 9.59: El código de la figura 9.55, con las asignaciones sustituidas por expresiones afinesde las variables de referencia i y jPrecauciones para las funciones de transferenciaen los mapas de valoresUna sutileza en la forma que definimos las funciones de transferencia en los mapas simbólicos es que tenemos opciones en cuanto a la forma en que se expresan los efectos deun cálculo. Si m es el mapa para la entrada de una función de transferencia, m(x) es enrealidad sólo “cualquier valor que la variable x tenga al momento de entrar”. Nos esforzamos en expresar el resultado de la función de transferencia como una expresión afín delos valores que se describen mediante el mapa de entrada.Hay que observar la interpretación apropiada de expresiones como f(m)(x), en dondef es una función de transferencia, m un mapa, y x una variable. Al igual que la convención en matemáticas, aplicamos las funciones a partir de la izquierda, lo cual significaque primero calculamos f(m), que es un mapa. Como un mapa es una función, podemosentonces aplicarla a una variable x para producir un valor.La función de transferencia de la instrucción s, denotada por f s, se define de la siguientemanera:1. Si s no es una instrucción de asignación, entonces f s es la función de identidad.2. Si s es una instrucción de asignación para la variable x, entonces:para todas las variablessi x es asignadayen cualquier otro caso.Maq. Cap_9_Bok.indd 692 11/10/07 1:04:07 AMEl objetivo de la expresión c0 + c 1m(y ) + c2m(z ) es representar todas las posibles formas deexpresiones que involucren variables arbitrarias y y z, y que puedan aparecer en el lado derechode una asignación para x, y que proporcionen a x un valor que sea una transformación afínsobre los valores anteriores de las variables. Estas expresiones son: c 0, c 0 + y, c 0 − y, y + z,x − y, c 1 * y y y/(1/c 1). Observe que en muchos casos, uno o más de los valores de c 0, c 1 y c 2son 0.Ejemplo 9.60: Si la asignación es x=y+z, entonces c 0 = 0 y c 1 = c 2 = 1. Si la asignación esx=y/5, entonces c 0 = c 2 = 0, y c 1 = 1/5. ✷Composición de las funciones de transferenciaPara calcular f 2 ° f 1, en donde f 1 y f 2 se definen en términos del mapa de entrada m, sustituimos el valor de m(vi) en la definición de f 2 con la definición de f 1(m)(vi). Sustituimos todas lasoperaciones sobre los valores NAA con NAA. Es decir,1. Si f 2(m)(v) = NAA, entonces (f 2 ° f 1)(m)(v) = NAA.2. Si f 2(m)(v) = c 0 + ªicim(vi), entonces (f 2 ° f 1)(m)(v)en cualquier otro casosi para ciertaEjemplo 9.61: Las funciones de transferencia de los bloques en el ejemplo 9.58 puedencalcularse al componer las funciones de transferencia de sus instrucciones constituyentes. Estasfunciones de transferencia se definen en la figura 9.60. ✷Figura 9.60: Funciones de transferencia del ejemplo 9.58Solución al problema de flujo de datosUtilizamos la notación ENTi, j [B 3] y SALi, j [B 3] para referirnos a los valores del flujo de datosde entrada y salida del bloque B 3 en la iteración j del ciclo interno, y la iteración i del cicloexterno. Para los demás bloques, usamos ENTi[Bk] y SALi[Bk] para referirnos a estos valores en9.8 Análisis simbólico 693Maq. Cap_9_Bok.indd 693 11/10/07 1:04:08 AM694 Capítulo 9. Optimizaciones independientes de la máquina SAL[B k ] = f B (ENT[B k ]), para todos los valores de Bk SAL[B1 ] ≥ ENT1[B2 ] SALi[B2 ] ≥ ENTi, 1 [B 3 ], 1 ≤ i ≤ 10 SALi, j −1[B3 ] ≥ ENTi, j[B 3 ], 1 ≤ i ≤ 100, 2 ≤ j ≤ 10 SALi, 10 [B3 ] ≥ ENTi [B 4 ], 2 ≤ i ≤ 100 SALi −1[B4 ] ≥ ENTi [B 2 ], 1 ≤ i ≤ 100Figura 9.61: Restricciones que se satisfacen en cada iteración de los ciclos anidadosla i-ésima iteración del ciclo externo. Además, podemos ver que los mapas simbólicos que semuestran en la figura 9.58 satisfacen las restricciones impuestas por las funciones de transferencia, que se presentan en la figura 9.61. La primera restricción dice que el mapa de salida de un bloque básico se obtiene aplicandola función de transferencia del bloque al mapa de entrada. El resto de las restricciones dicenque el mapa de salida de un bloque básico debe ser mayor que, o igual al mapa de entrada deun bloque sucesor en la ejecución.Observe que nuestro algoritmo de flujo de datos iterativo no puede producir la soluciónanterior, ya que carece del concepto de expresar los valores del flujo de datos en términos delnúmero de iteraciones ejecutadas. El análisis basado en regiones puede utilizarse para encontrar dichas soluciones, como veremos en la siguiente sección.9.8.3 Análisis simbólico basado en regionesPodemos extender el análisis basado en regiones descrito en la sección 9.7 para encontrar expresiones de variables en la i-ésima iteración de un ciclo. Un análisis simbólico basado en regionestiene una pasada de abajo hacia arriba y una pasada de arriba hacia abajo, al igual que otrosalgoritmos basados en regiones. La pasada ascendente sintetiza el efecto de una región con unafunción de transferencia que envía un mapa simbólico a la entrada hacia un mapa simbólico desalida en la salida. En la pasada descendente, los valores de los mapas simbólicos se propaganhacia abajo, a las regiones más internas.La diferencia recae en la forma en que manejamos los ciclos. En la sección 9.7, el efecto deun ciclo se sintetiza con un operador de cerradura. Dado un ciclo con el cuerpo f, su cerraduraf * se define como una reunión infinita de todos los posibles números de aplicaciones de f. Sinembargo, para encontrar variables de inducción debemos determinar si un valor de una variablees una función afín del número de iteraciones ejecutadas hasta el momento. El mapa simbólicodebe parametrizarse en base al número de la iteración que se está ejecutando. Además, cadavez que conocemos el número total de iteraciones ejecutadas en un ciclo, podemos usar esenúmero para encontrar los valores de las variables de inducción después del ciclo. Por ejemplo,en el ejemplo 9.58 afirmamos que a tiene el valor de i después de ejecutar la i-ésima iteración.Como el ciclo tiene 100 iteraciones, el valor de a debe ser 100 al final del ciclo.En lo que sigue, primero vamos a definir los operadores primitivos: la reunión y la composición de funciones de transferencia para el análisis simbólico. Después mostraremos cómousarlos para realizar un análisis basado en regiones de las variables de inducción.Maq. Cap_9_Bok.indd 694 11/10/07 1:04:10 AMReunión de funciones de transferenciaAl calcular la reunión de dos funciones, el valor de una variable es NAA, a menos que las dosfunciones asignen la variable al mismo valor, y que el valor no sea NAA.Por ende,en cualquier otro casosiComposiciones de funciones parametrizadasPara expresar una variable como una función afín de un índice de ciclo, debemos calcular elefecto de componer una función cierto número de veces. Si el efecto de una iteración se sintetizamediante la función de transferencia f, entonces el efecto de ejecutar i iteraciones, para ciertai ¦ 0, se denota como f i. Observe que cuando i = 0, f i = f 0 = I, la función de identidad.Las variables en el programa se dividen en tres categorías:1. Si f (m)(x ) = m(x ) + c, en donde c es una constante, entonces f i(m)(x ) = m(x ) + cipara cada valor de i ¦ 0. Decimos que x es una variable de inducción básica del ciclo cuyocuerpo se representa mediante la función de transferencia f.2. Si f (m)(x ) = m(x ), entonces f i(m)(x ) = m(x ) para todas las i ¦ 0. La variable x no semodifica y permanece sin cambios al final de cualquier número de iteraciones a travésdel ciclo con la función de transferencia f. Decimos que x es una constante simbólica enel ciclo.3. Si f (m)(x ) = c 0+c 1m(x 1) + … + cnm(xn), en donde cada xk es una variable de inducciónbásica o una constante simbólica, entonces para i > 0, Decimos que x es también una variable de inducción, aunque no básica. Observe que lafórmula anterior no se aplica si i = 0.4. En todos los demás casos, f i(m)(x ) = NAA.Para encontrar el efecto de ejecutar un número fijo de iteraciones, simplemente sustituimos i en la ecuación anterior por ese número. En el caso en el que se desconoce el número deiteraciones, el valor al inicio de la última iteración se da mediante f *. En este caso, las únicasvariables cuyos valores pueden aún expresarse en la forma afín son las variables invariantes deciclo.en cualquier otro casosiEjemplo 9.62: Para el ciclo más interno en el ejemplo 9.58, el efecto de ejecutar i iteraciones,i > 0, se sintetiza mediante f iB3. De la definición de fB3 podemos ver que a y b son constantessimbólicas, c es una variable de inducción básica debido a que se incrementa por cada iteración,9.8 Análisis simbólico 695Maq. Cap_9_Bok.indd 695 11/10/07 1:04:10 AM696 Capítulo 9. Optimizaciones independientes de la máquinad es una variable de inducción ya que es una función afín la constante simbólica b y la variable deinducción básica c. Por ende,sisisisiSi no podemos saber cuántas veces iteró el ciclo del bloque B 3, entonces no podríamosusar f i y tendríamos que usar f * para expresar las condiciones al final del ciclo. En este caso,tendríamos:sisisisi✷Un algoritmo basado en regionesAlgoritmo 9.63: Análisis simbólico basado en regiones.ENTRADA: Un grafo de flujo reducible G.SALIDA: Los mapas simbólicos ENT[B ] para cada bloque B de G.MÉTODO: Realizamos las siguientes modificaciones al Algoritmo 9.53.1. Cambiamos la forma en que construimos la función de transferencia para una región deciclo. En el algoritmo original usamos la función de transferencia fR, ENT[S ] para asignar elmapa simbólico a la entrada de la región de ciclo R a un mapa simbólico en la entradadel cuerpo de ciclo S, después de ejecutar un número desconocido de iteraciones. Se define como el cierre de la función de transferencia que representa a todos los caminos queconducen de vuelta a la entrada del ciclo, como se muestra en la figura 9.50(b). Aquídefinimos fR, i,ENT[S ] para representar el efecto de la ejecución, desde el inicio de la región deciclo, hasta la entrada de la i-ésima iteración. Por lo tanto,ENT SALpredecesores B en R del encabezado de S2. Si el número de iteraciones de una región es conocido, el resumen de la región se calculasustituyendo i por la cuenta actual.3. En la pasada descendente, calculamos fR, i,ENT[B ] para encontrar el mapa simbólico asociado con la entrada de la i-ésima iteración de un ciclo.Maq. Cap_9_Bok.indd 696 11/10/07 1:04:12 AM4. En el caso en donde se utiliza el valor de entrada de una variable m(v ) del lado derechode un mapa simbólico en la región R, y m(v) = NAA al entrar a la región, introducimosuna nueva variable de referencia t, agregamos la asignación t = v al inicio de la regiónR, y todas las referencias de m(v) se sustituyen por t. Si no introdujéramos una variablede referencia en este punto, el valor NAA guardado por v penetraría en los ciclos de másadentro.✷ENTENTENTENTENTENTSALSAL SALSALSAL SALSALSALFigura 9.62: Relaciones de las funciones de transferencia en la pasada de abajo hacia arribapara el ejemplo 9.58Ejemplo 9.64: Para el ejemplo 9.58, mostramos cómo se calculan las funciones de transferencia para el programa en la pasada de abajo hacia arriba en la figura 9.62. La región R 5 esel ciclo interno, con el cuerpo B 5. La función de transferencia que representa el camino queproviene de la entrada de la región R 5 al inicio de la j-ésima iteración, j ¦ 1, es f jB3−1. La funciónde transferencia que representa el camino al final de la j-ésima iteración, j ¦ 1, es f jB3.La región R 6 consiste en los bloques B 2 y B 4, con la región de ciclo R 5 en medio. Las funciones de transferencia que provienen de la entrada de B 2 y R 5 pueden calcularse de la mismaforma que en el algoritmo original. La función de transferencia fR6, SAL[B3] representa a la composición del bloque B 2 y toda la ejecución del ciclo interior, ya que fB4 es la función de identidad.Como se sabe que el ciclo interno itera 10 veces, podemos sustituir j por 10 para sintetizar conprecisiónel efecto del ciclo interno. El resto de las funciones de transferencia puede calcularsede manera similar. Las funciones de transferencia actuales que se calculan se muestran en lafigura 9.63.9.8 Análisis simbólico 697Maq. Cap_9_Bok.indd 697 11/10/07 1:04:13 AM698 Capítulo 9. Optimizaciones independientes de la máquinaSALSALSALENTENTENTENTENTENTSALFigura 9.63: Funciones de transferencia calculadas en la pasada de abajo hacia arriba para elejemplo 9.58El mapa simbólico a la entrada del programa es simplemente mNAA. Utilizamos la pasada dearriba hacia abajo para calcular el mapa simbólico a la entrada hacia regiones anidadas en forma sucesiva, hasta encontrar todos los mapas simbólicos para cada bloque básico. Empezamospor calcular los valores del flujo de datos para el bloque B 1 en la región R 8:ENT[B 1] = mNAASAL[B 1] = fB1(ENT[B 1])Si descendemos hasta las regiones R 7 y R 6, obtenemos:ENTi[B 2] = fR7, i, ENT[R6](SAL[B 1])SALi[B 2] = fB2(ENTi[B 2])Por último, en la región R 5, obtenemos:ENTi, j[B 3] = fR5, ENT[B3](SALi[B 2])SALi, j [B 3] = fB3(ENTi, j[B 3])No es de sorprender que estas ecuaciones produzcan los resultados que mostramos en la figura9.58. ✷El ejemplo 9.58 muestra un programa simple, en el cual cada variable utilizada en el mapasimbólico tiene una expresión afín. Utilizamos el ejemplo 9.65 para ilustrar por qué y cómointroducimos las variables de referencia en el Algoritmo 9.63.Maq. Cap_9_Bok.indd 698 11/10/07 1:04:14 AM(b) Una variable de referencia t hace a una b una variable de inducción.(a) Un ciclo en donde a fluctúa.Figura 9.64: La necesidad de introducir variables de referenciaEjemplo 9.65: Considere el ejemplo simple en la figura 9.64(a). Hagamos que fj sea la funciónde transferencia que sintetiza el efecto de ejecutar j iteraciones del ciclo interno. Aun cuando elvalor de a puede fluctuar durante la ejecución del ciclo, podemos ver que b es una variable deinducción basada en el valor de a al entrar al ciclo; es decir, fj(m)(b) = m(a) − 1 + j. Comoa a se le asigna un valor de entrada, el mapa simbólico al entrara al ciclo interno asigna a aNAA. Introducimos una nueva variable de referencia t para guardar el valor de a al entrar, yrealizamos las sustituciones como en la figura 9.64(b). ✷9.8.4 Ejercicios para la sección 9.8Ejercicio 9.8.1: Para el grafo de flujo de la figura 9.10 (vea los ejercicios de la sección 9.1),proporcione las funciones de transferencia para:a) El bloque B 2 .b) El bloque B 4 .c) El bloque B 5 .9.8 Análisis simbólico 699Maq. Cap_9_Bok.indd 699 11/10/07 1:04:16 AM700 Capítulo 9. Optimizaciones independientes de la máquinaEjercicio 9.8.2: Considere el ciclo interno de la figura 9.10, que consiste en los bloques B 3 yB 4. Si i representa el número de veces que se recorre el ciclo, y f es la función de transferenciapara el cuerpo del ciclo (es decir, excluyendo la arista de B 4 a B 3) que proviene de la entradadel ciclo (es decir, el inicio de B 3) hasta la salida de B 4, entonces ¿qué es f i? Recuerde que ftoma como argumento un mapa m, y m asigna un valor a cada una de las variables a, b, d y e.Denotamos estos valores m(a), y así en lo sucesivo, aunque no conocemos sus valores.Ejercicio 9.8.3: Ahora considere el ciclo externo de la figura 9.10, que consiste en los bloquesB 2, B 3, B 4 y B 5. Hagamos que g sea la función de transferencia para el cuerpo del ciclo, desdela entrada del ciclo en B 2 hasta su salida en B 5. Hagamos que i mida el número de iteracionesdel ciclo interno de B 3 y B 4 (no podemos saber cuál cuenta de iteraciones), y hagamos que jmida el número de iteraciones del ciclo externo (que tampoco podemos conocer). ¿Qué es g j ?9.9 Resumen del capítulo 9♦ Subexpresiones comunes globales: Una de las optimizaciones importantes es la de encontrar cálculos de la misma expresión en dos bloques básicos distintos. Si una va antes quela otra, podemos almacenar el resultado la primera vez que se calcula y usar el resultadoalmacenado en las ocurrencias siguientes.♦ Propagación de copias: Una instrucción de copia, u = v, asigna una variable v a otravariable, u. En ciertas circunstancias, podemos sustituir todos los usos de u por v, con locual se elimina tanto la asignación como u.♦ Movimiento de código: Otra optimización es mover un cálculo fuera del ciclo en el queaparece. Este cambio sólo es correcto si el cálculo produce el mismo valor cada vez que serecorre el ciclo.♦ Variables de inducción: Muchos ciclos tienen variables de inducción, variables que tomanuna secuencia lineal de valores cada vez que se recorre el ciclo. Algunas de éstas se utilizan sólo para contar iteraciones, y a menudo pueden eliminarse, con lo cual se reduce eltiempo que se requiere para recorrer el ciclo.♦ Análisis del flujo de datos: Un esquema de análisis de flujo de datos define un valor encada punto en el programa. Las instrucciones del programa tienen funciones de transferencia asociadas que relacionan el valor antes de la instrucción con el valor después dela misma. Las instrucciones con más de un predecesor deben definir su valor mediante lacombinación de los valores en los predecesores, usando un operador de reunión (o confluencia).♦ Análisis del flujo de datos sobre bloques básicos: Debido a que la propagación de los valoresde un flujo de datos dentro de un bloque es en general bastante simple, es común que lasecuaciones de flujo de datos se preparen para tener dos variables para cada bloque, denombre ENT y SAL, que representan los valores del flujo de datos al inicio y al final delbloque, respectivamente. Las funciones de transferencia para las instrucciones en un blo-!Maq. Cap_9_Bok.indd 700 11/10/07 1:04:17 AMque son compuestas con el fin obtener la función de transferencia para el bloque como untodo.♦ Definiciones de alcance: El marco de trabajo del flujo de datos de las definiciones dealcance tiene valores que son conjuntos de instrucciones en el programa, los cuales definen valores para una o más variables. La función de transferencia para un bloque eliminalas definiciones de variables que en definitiva se definen de nuevo en el bloque, y agrega(“genera”) esas definiciones de variables que ocurren dentro del bloque. El operador deconfluencia es la unión, ya que las definiciones llegan a un punto si alcanzan cualquierpredecesor de ese punto.♦ Variables vivas: Otro marco de trabajo del flujo de datos importante calcula las variablesque están vivas (se utilizarán antes de la redefinición) en cada punto. El marco de trabajo es similar a las definiciones de alcance, sólo que la función de transferencia trabajaal revés. Una variable está viva al inicio de un bloque si se utiliza antes de la definiciónen el bloque, o si está viva al final del bloque y no se define de nuevo en el bloque.♦ Expresiones disponibles: Para descubrir las subexpresiones comunes globales, determinamos las expresiones disponibles en cada punto (las expresiones que se habían calculadoy ninguno de los argumentos de ellas se definieron de nuevo después del último cálculo).El marco de trabajo del flujo de datos es similar a las definiciones de alcance, pero eloperador de confluencia es la intersección, en vez de la unión.♦ Abstracción de los problemas de flujo de datos: Los problemas comunes de flujo de datos, como los que ya hemos mencionado, pueden expresarse en una estructura matemática común. Los valores son miembros de un semi-lattice, cuya reunión es el operador deconfluencia. Las funciones de transferencia asignan elementos del lattice a elementos dellattice. El conjunto de funciones de transferencia permitidas debe cerrarse bajo la composición e incluir la función de identidad.♦ Marcos de trabajo monótonos: Un semi-lattice tiene una relación ≤ definida por a ≤ b,si y sólo si a ∧ b = a. Los marcos de trabajo monótonos tienen la propiedad de que cadafunción de transferencia preserva la relación ≤; es decir, a ≤ b implica que f(a) ≤ f(b),para todos los elementos a y b del lattice y la función de transferencia f.♦ Marcos de trabajo distributivos: Estos marcos de trabajo cumplen con la condición de quef(a ∧ b) = f(a) ∧ f(b), para todos los elementos a y b del lattice y la función de transferencia f. Puede mostrarse que la condición distributiva implica la condición de monotonía.♦ Solución iterativa a los marcos de trabajo abstractos: Todos los marcos de flujo de datosmonótonos pueden resolverse mediante un algoritmo iterativo, en el que los valores ENTy SAL para cada bloque se inicializan de manera apropiada (dependiendo del marco detrabajo), y se calculan en forma repetida nuevos valores para estas variables, al aplicarlas operaciones de transferencia y confluencia. Esta solución siempre es segura (las optimizaciones que sugiere no cambiarán lo que hace el programa), pero sin duda la soluciónserá la mejor posible, sólo si el marco de trabajo es distributivo.9.9 Resumen del capítulo 9 701Maq. Cap_9_Bok.indd 701 11/10/07 1:04:18 AM702 Capítulo 9. Optimizaciones independientes de la máquina♦ El marco de trabajo de propagación de constantes: Mientras que los marcos de trabajobásicos, como las definiciones de alcance, son distributivos, hay también marcos de trabajo interesantes que son monótonos pero no distributivos. Uno implica la propagaciónde constantes mediante el uso de un semi-lattice, cuyos elementos son asignaciones de lasvariables del programa a constantes, más dos valores especiales que representan “no hayinformación” y “en definitiva no es constante”.♦ Eliminación de redundancia parcial: Muchas optimizaciones útiles, como el movimientode código y la eliminación de subexpresiones comunes globales, pueden generalizarse a unsolo problema, conocido como eliminación de redundancia parcial. Las expresiones queson necesarias, pero que están disponibles sólo a lo largo de algunos de los caminos haciaun punto, se calculan sólo a lo largo de los caminos en donde no están disponibles. Laaplicación correcta de esta idea requiere la solución a una secuencia de cuatro problemasde flujo de datos distintos, además de otras operaciones.♦ Dominadores: Un nodo en un grafo de flujo domina a otro nodo si cada camino que vaal último debe pasar a través del primero. Un dominador propio es uno distinto del nodoen sí. Cada nodo, excepto el nodo de entrada, tiene un dominador inmediato: uno de suspropios dominadores que todos los demás dominadores propios dominan.♦ Ordenamiento “primero en profundidad” de los grafos de flujo: Si realizamos una búsqueda en profundidad de un grafo de flujo, empezando en su entrada, producimos unárbol de expansión con búsqueda en profundidad. El orden “primero en profundidad”de los nodos es el inverso de un recorrido postorden de este árbol.♦ Clasificación de las aristas: Cuando construimos un árbol de expansión con búsqueda enprofundidad, todos las aristas del grafo de flujo pueden dividirse en tres grupos: aristasde avance (los que van del ancestro al descendiente apropiado), aristas de retirada (losque van del descendiente al ancestro), y las aristas de cruce (los demás). Una propiedadimportante es que todas las aristas de cruce van de derecha a izquierda en el árbol. Otrapropiedad importante es que de estas aristas, sólo los de retirada tienen una cabeza menor que su cola en el orden “primero en profundidad” (postorden inverso).♦ Aristas posteriores: En una arista posterior, su cabeza domina a su cola. Cada aristaposterior es una arista de retirada, sin importar cuál árbol de expansión con búsquedaen profundidad se elija para su grafo de flujo.♦ Grafos de flujo reducibles: Si cada arista de retirada es una arista posterior, sin importarqué árbol de expansión con búsqueda en profundidad se elija, entonces se dice que elgrafo de flujo es reducible. La gran mayoría de los grafos de flujo son reducibles; aquelloscuyas únicas instrucciones de flujo de control son las instrucciones usuales para formarciclos y bifurcar son sin duda reducibles.♦ Ciclos naturales: Un ciclo natural es un conjunto de nodos con un nodo de encabezado que domina a todos los nodos en el conjunto, y tiene por lo menos una arista posterior que entra a ese nodo. Dado cualquier arista posterior, podemos construir su cicloMaq. Cap_9_Bok.indd 702 11/10/07 1:04:18 AMnatural tomando la cabeza de la arista más todos los nodos que puedan llegar a la colade la arista, sin pasar a través de la cabeza. Dos ciclos naturales con distintos encabezados están separados, o uno de ellos está completamente dentro del otro; este hecho nospermite hablar sobre una jerarquía de ciclos anidados, siempre y cuando los “ciclos” seconsideren como ciclos naturales.♦ El orden “primero en profundidad” hace que el algoritmo iterativo sea eficiente: El algoritmo iterativo requiere pocas pasadas, siempre y cuando la propagación de informacióna lo largo de los caminos acíclicos sea suficiente; es decir, los ciclos no agregan nada. Sivisitamos los nodos en orden “primero en profundidad”, cualquier marco de trabajo delflujo de datos que propague la información hacia delante, por ejemplo, las definiciones dealcance, convergirá en no más de 2 más el número más grande de aristas de retirada encualquier camino acíclica. Lo mismo se aplica para los marcos de trabajo de propagaciónhacia atrás, como las variables vivas, si visitamos en el inverso del orden “primero enprofundidad” (es decir, en postorden).♦ Regiones: Las regiones son conjuntos de nodos y aristas con un encabezado h que dominaa todos los nodos en la región. Los predecesores de cualquier nodo distinto de h en laregión también deben encontrarse en la misma. Las aristas de la región son todos los que pasan de un nodo a otro de la región, con la posible excepción de algunos o todos los queentren al encabezado.♦ Regiones y grafos de flujo reducibles: Los grafos de flujo reducibles pueden analizarseen una jerarquía de regiones. Estas regiones son regiones de ciclo, que incluyen a todaslas aristas que van al encabezado, o regiones de cuerpo que no tienen aristas que van alencabezado.♦ Análisis del flujo de datos basado en regiones: Una alternativa para el método iterativodel análisis del flujo de datos es ir hacia arriba y hacia abajo de la jerarquía de regiones,calculando las funciones de transferencia desde el encabezado de cada región hasta cadanodo en esa región.♦ Detección de variables de inducción basada en regiones: Una aplicación importante delanálisis basado en regiones está en un marco de trabajo del flujo de datos que trata decalcular fórmulas para cada variable en una región de ciclo, cuyo valor sea una funciónafín (lineal) del número de veces que se recorre el ciclo.9.10 Referencias para el capítulo 9Dos de los primeros compiladores que realizaban una optimización de código extensa fueron Alpha [7] y Fortran H [16]. El tratado fundamental sobre las técnicas para la optimización de ciclos(por ejemplo, el movimiento de código) es [1], aunque aparecen versiones anteriores de algunas deestas ideas en [8]. Un libro distribuido de manera informal [4] fue una influencia para diseminarlas ideas de optimización del código.9.10 Referencias para el capítulo 9 703Maq. Cap_9_Bok.indd 703 11/10/07 1:04:19 AM704 Capítulo 9. Optimizaciones independientes de la máquinaLa primera descripción del algoritmo iterativo para el análisis de flujo de datos provienedel informe técnico no publicado de Vyssotsky y Wegner [20]. Se dice que el estudio científico delanálisis de flujo de datos empieza con un par de documentos realizados por Allen [2] y Cocke [3].La abstracción teórica del lattice que describimos aquí se basa en el trabajo de Kildall [13].Estos marcos de trabajo asumían la distributividad, que muchos marcos de trabajo no satisfacen. Después de que surgieron varios de esos marcos de trabajo, en [5] y [11] se incrustó lacondición de monotonía en el modelo.La eliminación de redundancia parcial se manejó por primera vez en [17]. El algoritmo demovimiento de código diferido descrito en este capítulo está basado en [14].Los dominadores se utilizaron por primera vez en el compilador descrito en [13]. Sin embargo, la idea se remonta a [18].La noción de los grafos de flujo reducibles proviene de [2]. La estructura de estos grafos deflujo, como se presentan aquí, proviene de [9] y [10]. [12] y [15] conectaron por primera vez lacapacidad de reducción de los grafos de flujo con las estructuras comunes anidadas de flujo decontrol, lo cual explica por qué esta clase de grafos de flujo es tan común.La definición de capacidad de reducción mediante la reducción T1-T2, como se utiliza enel análisis basado en regiones, proviene de [19]. El método basado en regiones se utilizó porprimera vez en un compilador descrito en [21].La forma de asignación individual estática (SSA) de la representación intermedia que sepresenta en la sección 6.1 incorpora tanto el flujo de datos como el flujo de control en su representación. SSA facilita la implementación de muchas transformaciones de optimización, a partirde un marco de trabajo común [6]. 1. Allen, F. E., “Program optimization”, Annual Review in Automatic Programming 5(1969), pp. 239-307. 2. Allen, F. E., “Control flow analysis”, ACM Sigplan Notices 5:7 (1970), pp. 1-19. 3. Cocke, J., “Global common subexpression elimination”, ACM SIGPLAN Notices 5:7(1970), pp. 20-24. 4. Cocke, J. y J. T. Schwartz, Programming Languages and Their Compilers: PreliminaryNotes, Courant Institute of Mathematical Sciences, Univ. de Nueva York, Nueva York,1970. 5. Cousot P. y R. Cousot, “Abstract interpretation: a unified lattice model for staticanalysis of programs by construction or approximation of fixpoints”, Fourth ACM Symposium on Principles of Programming Languages (1977), pp. 238-252. 6. Cytron, R., J. Ferrante, B. K. Rosen, M. N. Wegman y F. K. Zadeck, “Efficientlycomputing static single assignment form and the control dependence graph”, ACMTransactions on Programming Languages and Systems 13:4 (1991), pp. 451-490.Maq. Cap_9_Bok.indd 704 11/10/07 1:04:19 AM 7. Ershov, A. P., “Alpha – an automatic programming system of high efficiency”, J. ACM13:1 (1966), pp. 17-24. 8. Gear, C. W., “High speed compilation of efficient object code”, Comm. ACM 8:8(1965), pp. 483-488. 9. Hecht, M. S. y J. D. Ullman, “Flow graph reducibility”, SIAM J. Computing 1 (1972),pp. 188-202.10. Hecht, M. S. y J. D. Ullman, “Characterizations of reducible flow graphs”, J. ACM 21(1974), pp. 367-375.11. Kam, J. B. y J. D. Ullman, “Monotone data flow analysis frameworks”, Acta Informatica7:3 (1977), pp. 305-318.12. Kasami, T., W. W. Peterson y N. Tokura, “On the capabilities of while, repeat and exitstatements”, Comm. ACM 16:8 (1973), pp. 503-512.13. Kildall, G., “A unified approach to global program optimization”, ACM Symposium onPrinciples of Programming Languages (1973), pp. 194-206.14. Knoop, J., “Lazy code motion”, Proc. ACM SIGPLAN 1992 conference on ProgrammingLanguage Design and Implementation, pp. 224-234.15. Kosaraju, S. R., “Analysis of structured programs”, J. Computer and System Sciences9:3 (1974), pp. 232-255.16. Lowry, E. S. y C. W. Medlock, “Object code optimization”, Comm. ACM 12:1 (1969),pp. 13-22.17. Morel, E. y C. Renvoise, “Global optimization by supression of partial redundancies”,Comm. ACM 22 (1979), pp. 96-103.18. Prosser, R. T., “Application of boolean matrices to the analysis of flow diagrams”,AFIPS Eastern Joint Computer Conference (1959), Spartan Books, Baltimore MD,pp. 133-138.19. Ullman, J. D., “Fast algorithms for the elimination of common subexpressiones”, ActaInformatica 2 (1973), pp. 191-213.20. Vyssotsky, V. y P. Wegner, “A graph theoretical Fortran source language analyzer”,informe técnico sin publicar, Bell Laboratories, Murray Hill, NJ, 1963.21. Wulf, W. A., R. K. Johnson, C. B. Weinstock, S. O. Hobbs y C. M. Geschke, The Design of an Optimizing Compiler, Elsevier, Nueva York, 1975.9.10 Referencias para el capítulo 9 705Maq. Cap_9_Bok.indd 705 11/10/07 1:04:20 AMMaq. Cap_9_Bok.indd 706 11/10/07 1:04:20 AMTodo procesador moderno de alto rendimiento puede ejecutar varias operaciones en un solociclo de reloj. La “pregunta del millón de pesos” es: ¿qué tan rápido puede ejecutarse un programa en un procesador con paralelismo a nivel de instrucción? La respuesta depende de:1. El paralelismo potencial en el programa.2. El paralelismo disponible en el procesador.3. Nuestra capacidad para extraer el paralelismo del programa secuencial original.4. Nuestra capacidad para encontrar la mejor programación en paralelo, dadas las restricciones de programación.Si todas las operaciones en un programa dependen en gran parte unas de otras, entoncesno hay hardware ni técnicas de paralelización que puedan hacer que el programa se ejecute conrapidez en paralelo. Se ha investigado mucho para comprender los límites de la paralelización.Las aplicaciones no numéricas ordinarias tienen muchas dependencias inherentes. Por ejemplo, estos programas tienen muchas bifurcaciones dependientes de datos que no nos permitenpredecir qué instrucciones se van a ejecutar, y mucho menos decidir qué operaciones puedenejecutarse en paralelo. Por lo tanto, el trabajo en esta área se ha enfocado en la relajación delas restricciones de programación, incluyendo la introducción de nuevas características arquitectónicas, en vez de las mismas técnicas de programación.Las aplicaciones numéricas, como los cálculos científicos y el procesamiento de señales, tiendena presentar más paralelismo. Estas aplicaciones manejan grandes estructuras de datos en conjunto; a menudo, las operaciones sobre los distintos elementos de la estructura son independientesunas de otras y pueden ejecutarse en paralelo. Los recursos de hardware adicionales pueden aprovechar dicho paralelismo, y se proporcionan en máquinas de propósito general y procesadores deseñales digitales de alto rendimiento. Estos programas tienen, por lo regular, estructuras de controlsimples y patrones regulares de acceso a los datos; además se han desarrollado técnicas estáticas para extraer el paralelismo disponible de estos programas. La programación de código paraCapítulo 10Paralelismo a nivelde instrucción707Maq. Cap_10_AHO.indd 707 11/10/07 1:05:10 AM708 Capítulo 10. Paralelismo a nivel de instruccióndichas aplicaciones es interesante y considerable, ya que ofrecen un gran número de operacionesindependientes que se asignan a un gran número de recursos.Tanto la extracción del paralelismo como la programación para la ejecución en paralelo pueden realizarse en forma estática en el software, o en forma dinámica en el hardware. De hecho,hasta las máquinas con programación de hardware se pueden ayudar mediante la programaciónbasada en software. Este capítulo empieza por explicar las cuestiones fundamentales del uso delparalelismo a nivel de instrucción, que es igual sin importar si éste se maneja mediante softwareo hardware. Después motivamos los análisis de las dependencias de datos básicas necesariaspara la extracción del paralelismo. Estos análisis son útiles para muchas optimizaciones ademásdel paralelismo a nivel de instrucción, como veremos en el capítulo 11.Por último, presentamos las ideas básicas en la programación de código. Describimos unatécnica para programar bloques básicos, un método para manejar el flujo de control con altadependencia a los datos, que se encuentra en los programas de propósito general, y por últimouna técnica llamada canalizaciones de software, que se utiliza principalmente para la programación de programas numéricos.10.1 Arquitecturas de procesadoresAl pensar en el paralelismo a nivel de instrucción, por lo general, imaginamos un procesador queemite varias operaciones en un solo ciclo de reloj. De hecho, es posible para una máquina emitirsólo una operación por reloj1 y aún así lograr el paralelismo a nivel de instrucción, medianteel concepto de las canalizaciones. A continuación, primero explicaremos las canalizaciones ydespués hablaremos sobre la emisión de varias instrucciones.10.1.1 Canalizaciones de instrucciones y retrasos de bifurcaciónPrácticamente todos los procesadores, ya sean supercomputadoras de alto rendimiento o máquinas estándar, utilizan una canalización de instrucciones. Con una canalización de instrucciones, puede obtenerse una nueva instrucción en cada ciclo del reloj mientras que las siguientesinstrucciones siguen pasando a través de la canalización. En la figura 10.1 se muestra una canalización simple de instrucciones de 5 etapas: primero obtiene la instrucción (IF), la decodifica(ID), ejecuta la operación (EX), accede a la memoria (MEM) y escribe de vuelta el resultado(WB). La figura muestra cómo las instrucciones i, i + 1, i + 2, i + 3 e i + 4 pueden ejecutarseal mismo tiempo. Cada fila corresponde a un pulso del reloj, y cada columna en la figura especifica la etapa que ocupa cada instrucción en cada pulso del reloj.Si el resultado de una instrucción está disponible para cuando la instrucción que le sucedenecesita los datos, el procesador puede emitir una instrucción en cada ciclo del reloj. Las instrucciones de bifurcación son en especial problemáticas, ya que hasta que se obtienen, decodifican y ejecutan, el procesador no sabe qué instrucción se ejecutará a continuación. Muchosprocesadores obtienen y decodifican de manera especulativa las instrucciones que siguen justodespués, en caso de que no se tome una bifurcación. Cuando se toma una bifurcación, la canalización de instrucciones se vacía y se obtiene el destino de la bifurcación.1 Nos referiremos a un “pulso” de reloj o ciclo de reloj simplemente como un “ciclo”, cuando la intención estéclara.Maq. Cap_10_AHO.indd 708 11/10/07 1:05:13 AMFigura 10.1: Cinco instrucciones consecutivas en una canalización de instrucciones de 5 etapasPor ende, las bifurcaciones que se toman introducen un retraso en la obtención del destinode la bifurcación, e introducen “pequeños contratiempos” en la canalización de instrucciones.Los procesadores avanzados utilizan hardware para predecir los resultados de las bifurcaciones, con base en su historial de ejecución y para la preobtención de las ubicaciones de destinoque se predijeron. No obstante, se observan retrasos de bifurcación si se predicen mal lasbifurcaciones.10.1.2 Ejecución canalizadaAlgunas instrucciones requieren varios ciclos para ejecutarse. Un ejemplo común es la operación de carga en memoria. Aun cuando una memoria accede a las coincidencias en la caché,por lo general, se requieren varios ciclos para que la caché devuelva los datos. Decimos que laejecución de una instrucción está canalizada si las instrucciones que le suceden, que no sondependientes del resultado, pueden proceder. En consecuencia, aun cuando un procesadorpuede emitir sólo una operación por ciclo de reloj, varias operaciones podrían estar en susetapas de ejecución al mismo tiempo. Si la canalización de ejecución más profunda tiene n etapas, existe la posibilidad de que las n operaciones se encuentren en ejecución al mismo tiempo.Hay que tener en cuenta que no todas las instrucciones se canalizan por completo. Mientrasque, por lo general, las sumas y multiplicaciones de punto flotante se canalizan por completo,es común que las divisiones de punto flotante, que son más complejas y se ejecutan con menosfrecuencia, no se canalicen.La mayoría de los procesadores de propósito general detectan en forma dinámica las dependencias entre las instrucciones consecutivas y detienen de manera automática la ejecución delas instrucciones si sus operandos no están disponibles. Algunos procesadores, en especial losque están incrustados en los dispositivos portátiles, dejan la comprobación de dependencias alsoftware para poder mantener el hardware simple y el consumo de energía bajo. En este caso,el compilador es responsable de insertar instrucciones “no-operation” en el código, en caso deser necesario, para asegurar que los resultados estén disponibles cuando se requieran.10.1 Arquitecturas de procesadores 709Maq. Cap_10_AHO.indd 709 11/10/07 1:05:14 AM710 Capítulo 10. Paralelismo a nivel de instrucción10.1.3 Emisión de varias instruccionesAl emitir varias operaciones por cada ciclo de reloj, los procesadores pueden mantener aún másoperaciones en ejecución. El mayor número de operaciones que pueden ejecutarse en forma simultánea puede calcularse mediante la multiplicación del tamaño de emisión de instrucciones por elnúmero promedio de etapas en la canalización de ejecución.Al igual que la canalización, el paralelismo en máquinas que emiten varias instruccionespuede administrarse, ya sea mediante software o hardware. Las máquinas que dependen delsoftware para administrar su paralelismo se conocen como máquinas VLIW (Very-Long-Instruction-Word, Palabra de instrucción muy larga), mientras que las que administran su paralelismo con el hardware se conocen como máquinas superescalares. Las máquinas VLIW, comosu nombre lo implica, tienen palabras de instrucciones más grandes de lo normal, que codificanlas operaciones a emitir en un solo ciclo de reloj. El compilador decide qué operaciones se vana emitir en paralelo y codifica la información en el código máquina de manera explícita. Porotro lado, las máquinas superescalares tienen un conjunto de instrucciones regular, con unasemántica de ejecución secuencial ordinaria. Las máquinas superescalares detectan en formaautomática las dependencias entre las instrucciones y las emiten a medida que sus operandos sevuelven disponibles. Algunos procesadores incluyen ambas funcionalidades, VLIW y superescalar.Los programadores simples de hardware ejecutan las instrucciones en el orden en el que seobtienen. Si un programador se encuentra con una instrucción dependiente, ésta y todas las instrucciones que le siguen deben esperar hasta que se resuelvan las dependencias (es decir, que losresultados necesarios estén disponibles). Es obvio que dichas máquinas pueden beneficiarse detener un programador estático que coloque las operaciones independientes una enseguida de laotra, en el orden de ejecución.Los programadores más sofisticados pueden ejecutar las instrucciones “fuera de orden”. Lasoperaciones se detienen de manera independiente y no se les permite ejecutarse sino hasta quese hayan producido todos los valores de los cuales dependen. Inclusive, hasta estos programadores se benefician de la programación estática, ya que los programadores de hardware sólo tienenun espacio limitado en el cual pueden colocar en un búfer las operaciones que deben detenerse.La programación estática puede colocar las operaciones independientes cerca unas de otras,para lograr una mejor utilización del hardware. Además, sin importar qué tan sofisticado seaun programador dinámico, no puede ejecutar las instrucciones que no haya obtenido. Cuandoel procesador tiene que tomar una bifurcación inesperada, sólo puede encontrar paralelismoentre las instrucciones recién obtenidas. El compilador puede mejorar el rendimiento del programador dinámico, al asegurar que estas instrucciones recién obtenidas puedan ejecutarse enparalelo.10.2 Restricciones de la programación del códigoLa programación del código es una forma de optimización de los programas, que se aplica alcódigo máquina que produce el generador de código. La programación de código está sujeta atres tipos de restricciones:1. Restricciones de dependencia de control. Todas las operaciones que se ejecutan en elprograma original deben ejecutarse en el programa optimizado.Maq. Cap_10_AHO.indd 710 11/10/07 1:05:15 AM2. Restricciones de dependencia de datos. Las operaciones en el programa optimizado deben producir los mismos resultados que las operaciones correspondientes en el programaoriginal.3. Restricciones de recursos. El programador no debe excederse en las solicitudes de recursos de la máquina.Estas restricciones de programación garantizan que el programa optimizado producirá losmismos resultados que el original. Sin embargo, como la programación de código modifica elorden en el que se ejecutan las operaciones, el estado de la memoria en cualquier punto dadopuede no coincidir con ninguno de los estados de la memoria en una ejecución secuencial. Estasituación es un problema si la ejecución de un programa se interrumpe, por ejemplo, debido al lanzamiento de una excepción o un punto de interrupción insertado por el usuario. Por lotanto, los programas optimizados son más difíciles de depurar. Observe que este problemano es específico para la programación de código, sino que se aplica a todas las demás optimizaciones, incluyendo la eliminación de redundancia parcial (sección 9.5) y la asignación deregistros (sección 8.8).10.2.1 Dependencia de datosEs fácil ver que si modificamos el orden de ejecución de dos operaciones que no toquen ninguna de las mismas variables, no es posible que se puedan afectar sus resultados. De hecho, auncuando estas dos operaciones leen la misma variable, de todas formas podemos permutar suejecución. Sólo si una operación escribe a una variable leída o escrita por otra, se pueden veralterados sus resultados al modificar su orden de ejecución. Se dice que dichos pares de operaciones comparten una dependencia de datos, y debe preservarse su orden de ejecución relativo.Hay tres tipos de dependencia de datos:1. Dependencia verdadera: lectura después de escritura. Si una escritura va seguida de unalectura de la misma ubicación, la lectura depende del valor escrito; a dicha dependenciase le conoce como dependencia verdadera.2. Antidependencia: escritura después de lectura. Si una lectura va seguida de una escritura a la misma ubicación, decimos que hay una antidependencia de la lectura a la escritura. La escritura no depende de la lectura en sí, pero si la escritura ocurre antes de lalectura, entonces la operación de lectura elegirá el valor incorrecto. La antidependenciaes un derivado de la programación imperativa, en donde las mismas ubicaciones de memoria se utilizan para almacenar distintos valores. No es una “verdadera” dependenciay es posible eliminarla si almacenamos los valores en ubicaciones distintas.3. Dependencia de salida: escritura después de escritura. Dos escrituras a la misma ubicación comparten una dependencia de salida. Si se viola esta dependencia, el valor de laubicación de memoria escrita tendrá el valor incorrecto después de realizar ambas operaciones.A la antidependencia y a las dependencias de salida se les conoce como dependencias relacionadas con el almacenamiento. No son “verdaderas” dependencias, además de que pueden eliminarse10.2 Restricciones de la programación del código 711Maq. Cap_10_AHO.indd 711 11/10/07 1:05:15 AM712 Capítulo 10. Paralelismo a nivel de instrucciónmediante el uso de distintas ubicaciones para almacenar valores distintos. Observe que las dependencias de datos se aplican tanto a los accesos de memoria como a los accesos de registros.10.2.2 Búsqueda de dependencias entre accesos a memoriaPara comprobar si dos accesos a memoria comparten una dependencia de datos, sólo debemossaber si pueden referirse a la misma ubicación; no es necesario saber a qué ubicación se está accediendo. Por ejemplo, podemos saber que los dos accesos *p y (*p)+4 no pueden referirse a lamisma ubicación, aun cuando no podemos saber hacia dónde apunta p. Por lo general, la dependencia de datos es indecidible en tiempo de compilación., El compilador debe suponer que las operaciones pueden referirse a la misma ubicación, a menos que se pueda demostrar lo contrario.Ejemplo 10.1: Dada la siguiente secuencia de código:1) a = 1;2) *p = 2;3) x = a;a menos que el compilador sepa que no es posible que p apunte a a, debe concluir que las tresoperaciones tienen que ejecutarse en serie. Hay una dependencia de salida que fluye de la instrucción (1) hacia la instrucción (2), y hay dos dependencias verdaderas que fluyen de las instrucciones (1) y (2) a la instrucción (3). ✷El análisis de dependencias de datos es altamente sensible al lenguaje de programación quese utiliza en el programa. Para los lenguajes sin seguridad de tipos como C y C++, en dondeun apuntador puede convertirse para apuntar a cualquier tipo de objeto, es necesario el análisissofisticado para probar la independencia entre cualquier par de accesos a memoria basados enapuntadores. Hasta podemos acceder de manera indirecta a las variables escalares locales oglobales, a menos que podamos demostrar que ninguna de las instrucciones del programa haalmacenado sus direcciones en ninguna parte. En los lenguajes con seguridad de tipos, comoJava, los objetos de tipos diferentes son necesariamente distintos unos de otros. De manerasimilar, las variables primitivas locales en la pila no pueden tener alias con accesos a través deotros nombres.Un descubrimiento correcto de las dependencias de datos requiere una variedad de formasde análisis. Nos enfocaremos en las preguntas principales que deben resolverse si el compilador va a detectar todas las dependencias que existan en un programa, y en cómo utilizar estainformación en la programación de código. En los siguientes capítulos mostraremos cómo serealizan estos análisis.Análisis de dependencia de datos en arreglosLa dependencia de los datos en arreglos es el problema de eliminar la ambigüedad entre los valores de los índices en los accesos a los elementos de un arreglo. Por ejemplo, el siguiente ciclo: for (i = 0; i < n; i++) A[2*i] = A[2*i+1];Maq. Cap_10_AHO.indd 712 11/10/07 1:05:16 AMcopia los elementos impares en el arreglo A a los elementos pares que están justo después deellos. Como todas las ubicaciones leídas y escritas en el ciclo son distintas unas de otras, no haydependencias entre los accesos y todas las iteraciones en el ciclo pueden ejecutarse en paralelo.El análisis de dependencias de datos en arreglos, que a menudo se le conoce simplemente comoanálisis de dependencias de datos, es muy importante para la optimización de aplicaciones numéricas. En la sección 11.6 hablaremos sobre este tema con detalle.Análisis de alias de apuntadoresDecimos que dos apuntadores tienen el mismo alias si pueden hacer referencia al mismo objeto.El análisis de alias de apuntadores es difícil, ya que existen muchos apuntadores que puedentener el mismo alias potencialmente en un programa, y cada uno de ellos puede apuntar a unnúmero ilimitado de objetos dinámicos a través del tiempo. Para obtener alguna precisión, elanálisis de alias de apuntadores debe aplicarse a través de todas las funciones en un programa.En la sección 12.4 empezaremos a hablar sobre este tema.Análisis entre procedimientosPara los lenguajes que pasan parámetros por referencia, es necesario el análisis entre procedimientos para determinar si la misma variable se pasa como dos o más argumentos distintos.Dichos alias pueden crear dependencias entre parámetros aparentemente distintos. De manerasimilar, pueden usarse las variables globales como parámetros y, por ende, crear dependenciasentre los accesos a los parámetros y los accesos a las variables globales. El análisis entre procedimientos, que veremos en el capítulo 12, es necesario para determinar estos alias.10.2.3 Concesiones entre el uso de registros y el paralelismoEn este capítulo vamos a suponer que la representación intermedia independiente de la máquinadel programa fuente utiliza un número ilimitado de seudorregistros para representar variables quepueden asignarse a los registros. Estas variables incluyen a las variables escalares en el programa fuente a las que no se puede hacer referencia mediante otros nombres, así como las variablestemporales que el compilador genera para guardar los resultados parciales en las expresiones.A diferencia de las ubicaciones de memoria, los registros tienen nombres únicos. En consecuencia,pueden generarse con facilidad restricciones de dependencia de datos precisas para los accesosa los registros.El número ilimitado de seudorregistros utilizados en la representación intermedia debe, en unmomento dado, asignarse al pequeño número de registros físicos disponibles en la máquina dedestino. La asignación de varios seudorregistros al mismo registro físico tiene el desafortunadoefecto adicional de crear dependencias de almacenamiento artificiales, que restringen el paralelismo a nivel de instrucción. Por el contrario, la ejecución de las instrucciones en paralelo creala necesidad de más almacenamiento para guardar los valores que se calculan al mismo tiempo.Por ende, el objetivo de minimizar el número de registros utilizados entra en conflicto directocon el objetivo de incrementar al máximo el paralelismo a nivel de instrucción. Los ejemplos 10.2y 10.3 que se muestran a continuación, ilustran esta concesión clásica entre el almacenamiento yel paralelismo.10.2 Restricciones de la programación del código 713Maq. Cap_10_AHO.indd 713 11/10/07 1:05:16 AM714 Capítulo 10. Paralelismo a nivel de instrucciónRenombramiento de registros de hardwareEl paralelismo a nivel de instrucción se utilizó por primera vez en las arquitecturas computacionales como un medio para agilizar el código de máquina secuencial ordinario. Loscompiladores de ese tiempo no estaban al tanto del paralelismo a nivel de instrucción enla máquina y estaban diseñados para optimizar el uso de los registros. Reordenaban en forma deliberada las instrucciones para minimizar el número de registros utilizados y, comoresultado, también disminuían la cantidad de paralelismo disponible. El ejemplo 10.3ilustra cómo la minimización del uso de registros en el cálculo de árboles de expresionestambién limita su paralelismo.Quedaba tan poco paralelismo en el código secuencial que los arquitectos de computadoras inventaron el concepto de renombramiento de registros de hardware para deshacerlos efectos de la optimización de los registros en los compiladores. El renombramiento de losregistros de hardware cambia en forma dinámica la asignación de registros, a medida quese ejecuta el programa. Interpreta el código máquina, almacena los valores destinados parael mismo registro en distintos registros internos, y actualiza todos sus usos para que haganreferencia a los registros correctos, de manera acorde.Como el compilador introdujo primero las restricciones de dependencia de registros artificiales, pueden eliminarse mediante el uso de un algoritmo de asignación de registros queestá al corriente del paralelismo a nivel de instrucción. El renombramiento de registrosde hardware sigue siendo útil en el caso en el que el conjunto de instrucciones de unamáquina sólo pueda hacer referencia a un pequeño número de registros. Esta capacidadpermite una implementación de la arquitectura para asignar el pequeño número de registros arquitectónicos en el código, a un número mucho mayor de registros internos, enforma dinámica.Ejemplo 10.2: El siguiente código copia los valores de las variables en las ubicaciones a y ca las variables en las ubicaciones b y d, respectivamente, usando los seudorregistros t1 y t2.LD t1, a // t1 = aST b, t1 // b = t1LD t2, c // t2 = cST d, t2 // d = t2Si sabemos que todas las ubicaciones de memoria a las que se accedió son distintas, entonceslas copias pueden proceder en paralelo. No obstante, si a t1 y t2 se les asigna el mismo registrode forma que minimice el número de registros usados, es necesario serializar las copias. ✷Ejemplo 10.3: Las técnicas tradicionales de repartición de registros están orientadas a minimizar el número de registros utilizados cuando se realiza un cálculo. Considere la siguienteexpresión:Maq. Cap_10_AHO.indd 714 11/10/07 1:05:17 AM++++a bcdeFigura 10.2: Árbol de la expresión del ejemplo 10.3(a + b) + c + (d + e)que se muestra como un árbol sintáctico en la figura 10.2. Es posible realizar este cálculo usando tres registros, como se ilustra mediante el código máquina de la figura 10.3.Figura 10.3: Código máquina para la expresión de la figura 10.2Sin embargo, la reutilización de registros serializa el cálculo. Las únicas operaciones a lasque se les permite ejecutarse en paralelo son las cargas de los valores en las ubicaciones a y b,y las cargas de los valores en las ubicaciones d y e. Por ende, se requiere un total de 7 pasospara completar el cálculo en paralelo.Si hubiéramos usado distintos registros para cada suma parcial, la expresión podría evaluarse en 4 pasos, lo cual constituye la altura del árbol de expresión de la figura 10.2. La figura10.4 sugiere los cálculos en paralelo. ✷Figura 10.4: Evaluación en paralelo de la expresión de la figura 10.210.2 Restricciones de la programación del código 715Maq. Cap_10_AHO.indd 715 11/10/07 1:05:17 AM716 Capítulo 10. Paralelismo a nivel de instrucción10.2.4 Ordenamiento de fases entre la asignación de registrosy la programación de códigoSi los registros se asignan antes de la programación, el código resultante tiene por lo regularmuchas dependencias de almacenamiento que limitan la programación del código. Por otrolado, si el código se programa antes de la asignación de los registros, el programa creado puede requerir tantos registros que un derrame de registros (almacenamiento del contenido de unregistro en una ubicación de memoria, de manera que el registro pueda usarse para otro fin)puede negar las ventajas del paralelismo a nivel de instrucción. ¿Debe un compilador asignarlos registros primero, antes de programar el código?, ¿o debe ser al revés? ¿O tenemos quelidiar con estos dos problemas al mismo tiempo?Para responder a las preguntas anteriores, debemos considerar las características de losprogramas que se van a compilar. Muchas aplicaciones no numéricas no tienen tanto paralelismo disponible. Basta con dedicar un pequeño número de registros para guardar los resultadostemporales en las expresiones. Podemos aplicar primero un algoritmo de coloración, como enla sección 8.8.4, para asignar registros a todas las variables no temporales, después programarel código y por último asignar registros a las variables temporales.Este método no funciona para las aplicaciones numéricas, en las que hay expresiones muchomás extensas. Podemos usar un método jerárquico, en el cual el código se optimiza desde elinterior hacia fuera, empezando con los ciclos más internos. Las instrucciones se programan primero, suponiendo que a cada seudorregistro se le asigna su propio registro físico. La asignaciónde registros se aplica después de la programación y el código de derrame se agrega en donde seanecesario, y después el código se reprograma. Este proceso se repite para el código en los ciclosexternos. Cuando se consideran varios ciclos internos en conjunto en un ciclo externo común,a la misma variable se le pueden haber asignado distintos registros. Podemos cambiar la asignación de registros para evitar tener que copiar los valores de un registro a otro. En la sección10.5, vamos a hablar sobre la interacción entre la repartición de registros y la programación conmás detalle, en el contexto de un algoritmo de programación específico.10.2.5 Dependencia del controlLa programación de las operaciones dentro de un bloque básico es relativamente simple, debidoa que se garantiza que todas las instrucciones se ejecutan una vez que el flujo de control llega alinicio del bloque. Las instrucciones en un bloque básico pueden reordenarse en forma arbitraria, siempre y cuando se satisfagan todas las dependencias de datos. Por desgracia, los bloquesbásicos, en especial en los programas no numéricos, son en general muy pequeños; en promedio,hay aproximadamente sólo cinco instrucciones en un bloque básico. Además, a menudo las operaciones en el mismo bloque están muy relacionadas y, por ende, tienen poco paralelismo. Porlo tanto, la explotación del paralelismo entre los bloques básicos es crucial.Un programa optimizado debe ejecutar todas las operaciones en el programa original. Puedeejecutar más instrucciones que el original, siempre y cuando las instrucciones adicionales nocambien lo que hace el programa. ¿Por qué la ejecución de instrucciones adicionales agiliza laejecución de un programa? Si sabemos que hay la probabilidad de que se vaya a ejecutar unainstrucción, y hay un recurso inactivo disponible para realizar la operación “sin costo”, podemosejecutar la instrucción en forma especulativa. El programa se ejecuta con más rapidez cuando laespeculación resulta ser correcta.Maq. Cap_10_AHO.indd 716 11/10/07 1:05:19 AMSe dice que una instrucción i 1 es dependiente del control en la instrucción i 2 si el resultadode i 2 determina si se va a ejecutar i 1. La noción de dependencia del control corresponde alconcepto de anidar niveles en los programas estructurados por bloques. En específico, en lasiguiente instrucción if-else:if (c) s1; else s2;s1 y s2 son dependientes del control en c. De manera similar, en la siguiente instrucción while:while (c) s;el cuerpo es dependiente del control en c.Ejemplo 10.4: En el siguiente fragmento de código: if (a > t) b = a*a;d = a+c;las instrucciones b = a*a y d = a+c no tienen dependencia de datos con ninguna otra parte del fragmento. La instrucción b = a*a depende de la comparación a > t. Sin embargo, lainstrucción d = a+c no depende de la comparación y puede ejecutarse en cualquier momento.Suponiendo que la multiplicación a ∗ a no produzca efectos colaterales, puede realizarse enforma especulativa, siempre y cuando b se escriba sólo después de encontrar que a es mayorque t. ✷10.2.6 Soporte de ejecución especulativaLas cargas de memoria son un tipo de instrucción que puede beneficiarse en forma considerablede la ejecución especulativa. Desde luego, las cargas en memoria son bastante comunes. Tienenlatencias de ejecución bastante extensas, las direcciones que se utilizan en las cargas estándisponibles comúnmente desde antes, y el resultado puede almacenarse en una nueva variabletemporal sin destruir el valor de cualquier otra variable. Por desgracia, las cargas en memoriapueden producir excepciones si sus direcciones son ilegales, por lo que si accedemos ilegalmentea las direcciones en forma especulativa, podemos provocar que un programa correcto se detenga de manera inesperada. Además, las cargas de memoria mal pronosticadas pueden provocarfallas adicionales en la caché y fallas de página, que son en extremo costosas.Ejemplo 10.5: En el siguiente fragmento: if (p != null) q = *p;al desreferenciar p en forma especulativa, este programa correcto se detendrá con un error sip es null. ✷Muchos procesadores de alto rendimiento proporcionan características especiales para soportar los accesos especulativos a memoria. A continuación mencionaremos los más importantes.10.2 Restricciones de la programación del código 717Maq. Cap_10_AHO.indd 717 11/10/07 1:05:20 AM718 Capítulo 10. Paralelismo a nivel de instrucciónPreobtenciónLa instrucción de preobtención (prefetch) se inventó para llevar los datos de la memoria a lacaché antes de usarlos. Una instrucción preobtención indica al procesador que es probable queel programa utilice una palabra de memoria específica en el futuro cercano. Si la ubicación especificada es inválida o si al acceder a ésta se produce una falla de página, el procesador sóloignora la operación. En cualquier otro caso, el procesador lleva los datos de la memoria a lacaché, si no se encuentran ya ahí.Bits venenososHay otra característica de arquitectura, conocida como bits venenosos, que se inventó parapermitir la carga especulativa de datos, de la memoria al archivo de registro. Cada registro enla máquina se aumenta con un bit venenoso. Si se accede a la memoria ilegal o la página a laque se accedió no se encuentra en memoria, el procesador no produce la excepción de inmediato, sino que sólo establece el bit venenoso del registro de destino. Una excepción se producesólo si se utiliza el contenido del registro con un bit venenoso marcado.Ejecución predicadaComo las bifurcaciones son costosas, y las bifurcaciones mal pronosticadas lo son aún más (veala sección 10.1), se inventaron las instrucciones predicadas para reducir el número de bifurcaciones en un programa. Una instrucción predicada es como una instrucción normal, sólo quetiene un operando predicado adicional para proteger su ejecución; la instrucción se ejecuta sólosi el predicado es verdadero.Como ejemplo, una instrucción de movimiento condicional CMOVZ R2, R3, R1 tiene la semántica de que el contenido del registro R3 se mueve hacia el registro R2 sólo si el registro R1es cero. El código como:if (a == 0) b = c+d;puede implementarse con dos instrucciones de máquina, suponiendo que a, b, c y d se asignana los registros R1, R2, R4, R5 respectivamente, como se muestra a continuación: ADD R3, R4, R5CMOVZ R2, R3, R1Esta conversión sustituye una serie de instrucciones que comparten una dependencia de control con las instrucciones que comparten sólo dependencias de datos. Así, estas instruccionespueden combinarse con bloques básicos adyacentes para crear un bloque básico más grande.Lo más importante es que con este código el procesador no tiene oportunidad de dar un malpronóstico, con lo cual se garantiza que la canalización de instrucciones funcionará de manerauniforme.La ejecución predicada tiene un costo. Las instrucciones predicadas se obtienen y se decodifican, aun cuando tal vez no se ejecuten al final. Los programadores estáticos debenreservar todos los recursos necesarios para su ejecución y asegurar que se cumpla con todas lasMaq. Cap_10_AHO.indd 718 11/10/07 1:05:20 AMMáquinas con programación dinámicaEl conjunto de instrucciones de una máquina con programación estática define en formaexplícita lo que se puede ejecutar en paralelo. Sin embargo, en la sección 10.1.2 vimos queciertas arquitecturas de máquinas permiten realizar la decisión en tiempo de ejecución,acerca de lo que puede ejecutarse en paralelo. Con la programación dinámica, el mismocódigo de máquina puede ejecutarse en distintos miembros de la misma familia (máquinas que implementan el mismo conjunto de instrucciones) que tienen cantidades variadasde soporte de ejecución en paralelo. De hecho, la compatibilidad del código de máquinaes una de las principales ventajas de las máquinas con programación dinámica.Los programadores estáticos, que se implementan en el compilador mediante software, pueden ayudar a los programadores dinámicos (que se implementan en el hardwarede la máquina) a utilizar mejor los recursos de la máquina. Para construir un programador estático para una máquina con programación dinámica, podemos usar casi el mismoalgoritmo de programación que para las máquinas con programación estática, sólo que nose deben generar de manera explícita las instrucciones no−op que quedan en el programa.En la sección 10.4.7 hablaremos con más detalle sobre esta cuestión.dependencias de datos potenciales. La ejecución predicada no debe usarse en forma agresiva,a menos que la máquina tenga muchos recursos más de los que podrían utilizarse en cualquierotro caso.10.2.7 Un modelo de máquina básicoMuchas máquinas pueden representarse mediante el siguiente modelo simple. Una máquina M =R, T , consiste en:1. Un conjunto de tipos de operación T, como cargas, almacenamientos, operaciones aritméticas, etcétera.2. Un vector R = [r 1, r 2,…] que representa a los recursos de hardware, en donde ri es elnúmero de unidades disponibles del i-ésimo tipo de recurso. Algunos ejemplos de tiposde recursos comunes son: unidades de acceso a memoria, ALUs y unidades funcionales depunto flotante.Cada operación tiene un conjunto de operandos de entrada, un conjunto de operandosde salida y un requerimiento de recursos. Con cada operando de entrada hay una latencia deentrada asociada, la cual indica cuándo debe estar disponible el valor de entrada (relativo alinicio de la operación). Los operandos de entrada comunes tienen cero latencia, lo cual significaque los valores se necesitan de inmediato, en el ciclo de reloj en el que se emite la operación. Demanera similar, con cada operando de salida hay una latencia de salida asociada, la cual indicacuándo debe estar disponible el resultado, relativo al inicio de la operación.El uso de recursos para cada tipo de operación t de la máquina se modela mediante unatabla de reservación de recursos bidimensional, RTt. La. anchura de la tabla es el número de10.2 Restricciones de la programación del código 719Maq. Cap_10_AHO.indd 719 11/10/07 1:05:21 AM720 Capítulo 10. Paralelismo a nivel de instruccióntipos de recursos en la máquina, y su longitud es la duración a través de la cual la operaciónutiliza los recursos. La entrada RTt[i, j] es el número de unidades del j-ésimo recurso utilizadopor una operación de tipo t, i ciclos de reloj después de emitirse. Por cuestión de simplicidaden la notación, vamos a suponer que RTt[i, j] = 0, si i se refiere a una entrada inexistente enla tabla (es decir, si i es mayor que el número de ciclos que se requieren para ejecutar la operación). Desde luego que, para cualquier t, i y j, RTt[i, j] debe ser menor o igual que R[j], elnúmero de recursos de tipo j que tiene la máquina.Las operaciones comunes de la máquina sólo ocupan una unidad de recurso al momentoen que se emite una operación. Algunas operaciones pueden usar más de una unidad funcional. Por ejemplo, una operación de multiplicación y suma puede usar un multiplicador en elprimer ciclo de reloj, y un sumador en el segundo. Algunas operaciones, como la división, talvez necesiten ocupar un recurso durante varios ciclos de reloj. Las operaciones canalizadas porcompleto son aquellas que pueden emitirse en cada ciclo de reloj, aun cuando sus resultadosno estén disponibles sino hasta cierto número de ciclos después. No tenemos que modelar los recursos de cada etapa de canalización en forma explícita; basta con una sola unidad pararepresentar la primera etapa. Cualquier operación que ocupe la primera etapa de una canalización tiene garantizado el derecho de proceder a las etapas siguientes en los ciclos de relojposteriores.Figura 10.5: Una secuencia de asignaciones que exhibe dependencias de datos10.2.8 Ejercicios para la sección 10.2Ejercicio 10.2.1: Las asignaciones en la figura 10.5 tienen ciertas dependencias. Para cadauno de los siguientes pares de instrucciones, clasifique la dependencia como (i) dependenciaverdadera, (ii) antidependencia, (iii) dependencia de salida, o (iv ) sin dependencia (es decir,las instrucciones pueden aparecer en cualquier orden):a) Instrucciones (1) y (4).b) Instrucciones (3) y (5).c) Instrucciones (1) y (6).d) Instrucciones (3) y (6).e) Instrucciones (4) y (6).Maq. Cap_10_AHO.indd 720 11/10/07 1:05:21 AMEjercicio 10.2.2: Evalúe la expresión ((u+v ) + (w+x )) + (y+z ) tal como están los paréntesis (es decir, no utilice las leyes conmutativas o asociativas para reordenar las sumas). Proporcione el código de máquina a nivel de registro para obtener el máximo paralelismo posible.Ejercicio 10.2.3: Repita el ejercicio 10.2.2 para las siguientes expresiones:Si en vez de maximizar el paralelismo, minimizamos el número de registros, ¿cuántos pasos requeriría el cálculo? ¿Cuántos pasos nos ahorramos al utilizar el paralelismo máximo?Ejercicio 10.2.4: La expresión del ejercicio 10.2.2 puede ejecutarse mediante la secuencia deinstrucciones que se muestran en la figura 10.6. Si tenemos todo el paralelismo que necesitamos, ¿cuántos pasos se necesitan para ejecutar las instrucciones?Figura 10.6: Implementación de una expresión aritmética con el número mínimo de registrosEjercicio 10.2.5: Traduzca el fragmento de código que vimos en el ejemplo 10.4, usando lainstrucción de copia condicional CMOVZ de la sección 10.2.6. ¿Cuáles son las dependencias dedatos en su código de máquina?10.3 Programación de bloques básicosAhora estamos listos para empezar a hablar sobre los algoritmos de programación de código.Empezaremos con el problema más sencillo: programar las operaciones en un bloque básico queconsiste en instrucciones de máquina. La solución óptima para este problema es NP-completo.Pero en la práctica, un bloque básico ordinario sólo tiene un pequeño número de operacionesaltamente restringidas, por lo que basta con las técnicas simples de programación. Para esteproblema vamos a presentar un algoritmo simple pero muy efectivo, conocido como programación por lista.10.3 Programación de bloques básicos 721Maq. Cap_10_AHO.indd 721 11/10/07 1:05:22 AM722 Capítulo 10. Paralelismo a nivel de instrucción10.3.1 Grafos de dependencia de datosRepresentamos cada bloque básico de instrucciones de máquina mediante un grafo de dependencia de datos, G = (N, E), el cual tiene un conjunto de nodos N que representan las operaciones en las instrucciones de máquina en el bloque, y un conjunto de aristas E dirigidos querepresentan las restricciones de dependencia de datos entre las operaciones. Los nodos y lasaristas de G se construyen de la siguiente manera:1. Cada operación n en N tiene una tabla de reservación de recursos RTn, cuyo valor essimplemente la tabla de reservación de recursos asociada con el tipo de operación de n.2. Cada arista e en E se etiqueta con el retraso de, lo cual indica que el nodo de destinono debe emitirse antes de de ciclos después de emitir el nodo de origen. Suponga que laoperación n 1 va seguida de la operación n 2, y que ambas acceden a la misma ubicación,con las latencias l 1 y l 2, respectivamente. Es decir, el valor de la ubicación se produce l 1ciclos después de que empieza la primera instrucción, y la segunda instrucción necesitaese valor l 2 ciclos después de iniciar (observe que es común tener l 1 = 1 y l 2 = 0). Entonces, hay una arista n 1 → n 2 en E etiquetada con el retraso l 1 − l 2.Ejemplo 10.6: Considere una máquina simple que puede ejecutar dos operaciones en cadaciclo de reloj. La primera debe ser una operación de bifurcación, o una operación de la ALU,de la siguiente forma:OP dst, orig1, orig2La segunda debe ser una operación de carga o almacenamiento, de la siguiente forma:LD dst, direcST direc, origLa operación de carga (LD) está canalizada por completo y requiere dos ciclos de reloj. Sinembargo, justo después de una carga puede ir un almacenamiento ST que escriba a la ubicaciónde memoria que se leyó. Todas las demás operaciones se completan en un ciclo.En la figura 10.7 se muestra el grafo de dependencia de un ejemplo de un bloque básico y surequerimiento de recursos. Podríamos imaginar que R1 es un apuntador de pila, que se utilizapara acceder a los datos en la pila con desplazamientos como 0 o 12. La primera instruccióncarga el registro R2, y el valor cargado no está disponible sino hasta dos ciclos de reloj después. Esta observación explica la etiqueta 2 en las aristas que van de la primera a la segunday quinta instrucciones, cada una de las cuales necesita el valor de R2. De manera similar, hayun retraso de 2 en la arista que va de la tercera instrucción a la cuarta; el valor que se carga enR3 lo necesita la cuarta instrucción, y no está disponible sino hasta dos ciclos después de queempieza la tercera instrucción.Como no sabemos cómo se relacionan los valores de R1 y R7, tenemos que considerar la posibilidad de que una dirección como 8(R1) sea igual que la dirección 0(R7). Es decir, la últimaMaq. Cap_10_AHO.indd 722 11/10/07 1:05:23 AMreservaciónalu memdependenciasde datosi1i2i3i4i5i6i7tablas dede recursosFigura 10.7: Grafo de dependencia de datos para el ejemplo 10.6instrucción puede estar almacenando datos en la misma dirección de la que la tercera instrucción carga datos. El modelo de máquina que estamos usando nos permite almacenar datos enuna ubicación un ciclo después de cargar datos de esa ubicación, aun cuando el valor que seva a cargar no aparecerá en un registro sino hasta un ciclo de reloj después. Esta observaciónexplica la etiqueta 1 en la arista que va de la tercera instrucción a la última. El mismo razonamiento explica las aristas y las etiquetas que van de la primera instrucción a la última. Lasotras aristas con la etiqueta 1 se explican mediante una dependencia, o posible dependencia,condicionada en el valor de R7. ✷10.3.2 Programación por lista de bloques básicosEl método más simple para programar bloques básicos implica la acción de visitar cada nododel grafo de dependencia de datos en “orden topológico priorizado”. Como no puede haberciclos en un grafo de dependencia de datos, siempre hay por lo menos un orden topológico paralos nodos. Sin embargo, entre los posibles órdenes topológicos, algunos pueden ser preferiblesa otros. En la sección 10.3.3 hablaremos sobre algunas de las estrategias para elegir un orden10.3 Programación de bloques básicos 723LD R2,0(R1)ST 4(R1),R2LD R3,8(R1)ADD R3,R3,R4ADD R3,R3,R2ST 12(R1),R3ST 0(R7),R71 11111222Maq. Cap_10_AHO.indd 723 11/10/07 1:05:24 AM724 Capítulo 10. Paralelismo a nivel de instrucciónTablas de reservación de recursos ilustradasCon frecuencia, es útil visualizar una tabla de reservación de recursos para una operación,mediante una rejilla de cuadros rellenos y vacíos. Cada columna corresponde a uno delos recursos de la máquina, y cada fila corresponde a uno de los ciclos de reloj durante loscuales se ejecuta la operación. Suponiendo que la operación nunca necesita más de unaunidad de cualquier recurso, podemos representar los 1s mediante cuadros rellenos ylos 0s mediante cuadros vacíos. Además, si la operación está canalizada por completo,entonces sólo debemos indicar los recursos utilizados en la primera fila, y la tabla dereservación de recursos se convierte en una sola fila.Por ejemplo, esta representación se utiliza en el ejemplo 10.6. En la figura 10.7 podemos ver las tablas de reservación de recursos como filas. Las dos operaciones de sumarequieren el recurso “alu”, mientras que las operaciones de carga y almacenamiento requieren el recurso “mem”.topológico, pero por el momento sólo supondremos que existe un algoritmo para elegir un ordenpreferido.El algoritmo de programación por lista que vamos a describir a continuación visita losnodos en el orden topológico priorizado que hayamos elegido. Los nodos pueden o no terminarsiendo programados en el mismo orden en el que se visitan. Pero las instrucciones se colocanen el programa lo más pronto posible, por lo que hay una tendencia para que las instruccionesse programen aproximadamente en el orden visitado.Visto con más detalle, el algoritmo calcula la primera ranura de tiempo en la que puedeejecutarse cada nodo, de acuerdo con sus restricciones de dependencia de datos con los nodosque se programaron previamente. A continuación, se comprueban los recursos que el nodo necesita contra una tabla de reservación de recursos que recolecta todos los recursos asignadoshasta ese momento. El nodo se programa en la primera ranura de tiempo que tenga suficientesrecursos.Algoritmo 10.7: Programación por lista de un bloque básico.ENTRADA: Un vector de recursos de máquina R = [r 1, r 2,…], en donde ri es el número de unidades disponibles del i-ésimo tipo de recurso, y un grafo de dependencia de datos G = (N, E).Cada operación n en N se etiqueta con su tabla de reservación de recursos RTn; cada aristae = n 1 → n 2 en E se etiqueta con de, lo cual indica que n 2 no debe ejecutarse antes de de ciclosde reloj después que n 1.SALIDA: Un programa S que asigna las operaciones en N en ranuras de tiempo, en donde lasoperaciones pueden iniciarse y cumplir con todas las restricciones de recursos y de datos.MÉTODO: Ejecute el programa de la figura 10.8. En la sección 10.3.3 veremos una explicaciónde lo que podría ser el “orden topológico priorizado”. ✷Maq. Cap_10_AHO.indd 724 11/10/07 1:05:25 AMRT = una tabla de reservación vacía;for (cada n en N en orden topológico priorizado) {s = máxe=p→n en E (S(p) + de ); /* Busca el momento más temprano en el que podría empezar esta instrucción,ya cuando empezaron sus predecesores. */while (exista una i tal que RT [s + i] + RTn[i] > R)s = s + 1; /* Retrasa aún más la instrucción, hasta que estén disponibleslos recursos necesarios. */S(n) = s ;for (todas las i)RT [s + i] = RT [s + i] + RTn[i]}Figura 10.8: Un algoritmo de programación por lista10.3.3 Órdenes topológicos priorizadosLa programación por lista no da marcha atrás; programa cada nodo sólo una vez. Utiliza unafunción de prioridad heurística para elegir de entre los nodos que están listos para programarsea continuación. He aquí algunas observaciones acerca de los posibles ordenamientos priorizadosde los nodos:• Sin restricciones de recursos, el programa más corto se da mediante la ruta crítica, la rutamás larga a través del grafo de dependencia de datos. Una medida útil como función deprioridad es la altura del nodo, que es la longitud de una ruta más larga en el grafo, quese origina desde el nodo.• Por otro lado, si todas las operaciones son independientes, entonces la longitud del programa se ve restringida por lo recursos disponibles. El recurso crítico es el que tiene laproporción más grande de usos, para el número de unidades disponibles de ese recurso.Las operaciones que utilizan recursos más críticos pueden recibir una prioridad másalta.• Por último, podemos usar el ordenamiento de origen para romper los vínculos entre lasoperaciones; la operación que aparezca primero en el programa fuente debe programarseprimero.Ejemplo 10.8: Para el grafo de dependencia de datos de la figura 10.7, la ruta crítica, incluyendo el tiempo para ejecutar la última instrucción, es de 6 ciclos. Es decir, la ruta críticaconsta de los últimos cinco nodos, desde la carga de R3 hasta el almacenamiento en R7. El totalde los retrasos en las aristas a lo largo de esta ruta es de 5, a lo cual sumamos 1 por el ciclo dereloj requerido para la última instrucción.Usando la altura como la función de prioridad, el Algoritmo 10.7 encuentra un programaóptimo, como se muestra en la figura 10.9. Observe que programamos primero la carga de R3,ya que tiene la mayor altura. La suma de R3 y R4 tiene los recursos para programarse en el10.3 Programación de bloques básicos 725Maq. Cap_10_AHO.indd 725 11/10/07 1:05:25 AM726 Capítulo 10. Paralelismo a nivel de instruccióntablas dereservaciónde recursosalu memprogramaFigura 10.9: Resultado de aplicar la programación por lista al ejemplo de la figura 10.7segundo ciclo de reloj, pero el retraso de 2 para una carga nos obliga a esperar hasta el tercerciclo de reloj para programar esta suma. Es decir, no podemos estar seguros de que R3 tendrásu valor necesario, sino hasta el inicio del ciclo 3. ✷Figura 10.10: Código máquina para el ejercicio 10.3.110.3.4 Ejercicios para la sección 10.3Ejercicio 10.3.1: Para cada uno de los fragmentos de código de la figura 10.10, dibuje el grafode dependencia de datos.Ejercicio 10.3.2: Suponga que tenemos una máquina con un recurso ALU (para las operaciones ADD y SUB) y un recurso MEM (para las operaciones LD y ST). Suponga que todas lasoperaciones requieren un ciclo de reloj, excepto LD, la cual requiere dos. Sin embargo, como enel ejemplo 10.6, una operación ST en la misma ubicación de memoria puede comenzar un ciclode reloj después del inicio de una operación LD sobre esa ubicación. Busque la programaciónmás corta para cada uno de los fragmentos de la figura 10.10. LD R2,0(R1) ST 4(R1),R2 LD R3,8(R1) ADD R3,R3,R4 ADD R3,R3,R2 ST 12(R1),R3 ST 0(R7),R7Maq. Cap_10_AHO.indd 726 11/10/07 1:05:26 AMEjercicio 10.3.3: Repita el ejercicio 10.3.2, suponiendo que: i. La máquina tiene un recurso ALU y dos recursos MEM. ii. La máquina tiene dos recursos ALU y un recurso MEM.iii. La máquina tiene dos recursos ALU y dos recursos MEM.Figura 10.11: Código máquina para el ejercicio 10.3.4Ejercicio 10.3.4: Tomando el modelo de máquina del ejemplo 10.6 (como en el ejercicio10.3.2): a) Dibuje el grafo de dependencia de datos para el código de la figura 10.11. b) ¿Cuáles son todas las rutas críticas en su grafo de la parte (a)? ! c) Si consideramos recursos MEM ilimitados, ¿cuáles son todas las programaciones posibles para las siete instrucciones?10.4 Programación de código globalPara una máquina con una cantidad moderada de paralelismo a nivel de instrucción, los programas creados por la compactación de bloques básicos individuales tienden a dejar muchosrecursos inactivos. Para poder utilizar mejor los recursos de la máquina, es necesario considerarestrategias de generación de código que muevan las instrucciones de un bloque básico a otro.Las estrategias que consideran más de un bloque básico a la vez se conocen como algoritmosde programación global. Para realizar la programación global de manera correcta, debemos considerar no sólo las dependencias de datos, sino también las dependencias de control. Debemosasegurarnos de que:1. Todas las instrucciones en el programa original se ejecuten en el programa optimizado, y2. Aunque el programa optimizado puede ejecutar instrucciones adicionales en forma especulativa, estas instrucciones no deben tener efectos adicionales no deseados.10.4 Programación de código global 727Maq. Cap_10_AHO.indd 727 11/10/07 1:05:27 AM728 Capítulo 10. Paralelismo a nivel de instrucción10.4.1 Movimiento de código primitivoVamos a estudiar primero las cuestiones que tienen que ver con el movimiento de las operaciones, mediante un ejemplo simple.Ejemplo 10.9: Suponga que tenemos una máquina que puede ejecutar dos operaciones cualesquiera en un solo ciclo de reloj. Cada operación se ejecuta con un retraso de un ciclo dereloj, excepto la operación de carga, que tiene una latencia de dos ciclos. Por cuestión de simplicidad, suponemos que todos los accesos a memoria en el ejemplo son válidos, y que habrácoincidencias en la caché. La figura 10.12(a) muestra un grafo de flujo simple con tres bloquesbásicos. El código se expande en las operaciones de máquina de la figura 10.12(b). Todas las instrucciones en cada bloque básico deben ejecutarse en serie, debido a las dependencias de datos;de hecho, hay que insertar una instrucción “no-op” en cada bloque básico.Suponga que las direcciones de las variables a, b, c, d y e son distintas y que estas direcciones se almacenan en los registros del R1 al R5, respectivamente. Por lo tanto, los cálculos delos distintos bloques básicos no comparten dependencias de datos. Observamos que se ejecutantodas las operaciones en el bloque B 3, sin importar si se toma la bifurcación y, por lo tanto,pueden ejecutarse en paralelo con las operaciones del bloque B 1. No podemos mover las operaciones de B 1 hacia B 3, ya que se necesitan para determinar los resultados de la bifurcación.Las operaciones en el bloque B 2 son dependientes del control en la prueba del bloque B 1.Podemos realizar la carga de B 2 de manera especulativa en el bloque B 1 sin costo, y ahorrarnosdos ciclos del tiempo de ejecución cada vez que se tome la bifurcación.Las operaciones de almacenamiento no deben realizarse en forma especulativa, ya quesobrescriben el antiguo valor en una ubicación en memoria. Sin embargo, es posible retrasaruna operación de almacenamiento. No podemos simplemente colocar la operación de almacenamiento del bloque B 2 en el bloque B 3, ya que sólo debería ejecutarse si el flujo de controlpasa a través del bloque B 2. Sin embargo, podemos colocar la operación de almacenamiento enuna copia duplicada de B 3. La figura 10.12(c) muestra un programa optimizado de este tipo.El código optimizado se ejecuta en 4 ciclos de reloj, que es lo mismo que el tiempo requeridopara ejecutar B 3 por sí solo. ✷El ejemplo 10.9 muestra que es posible mover las operaciones a lo largo de una ruta deejecución. Cada par de bloques básicos en este ejemplo tiene una “relación de predominio”distinta, y por ende son distintas las consideraciones de cuándo y cómo deben moverse las instrucciones entre cada par. Como vimos en la sección 9.6.1, se dice que un bloque B domina albloque B si cada ruta proveniente de la entrada del grafo de flujo de control hacia B pasa através de B. De manera similar, un bloque B posdomina al bloque B si cada ruta que va de Ba la salida del grafo pasa a través de B. Cuando B domina a B y B posdomina a B, decimosque B y B son de control equivalente, lo cual significa que uno se ejecuta sólo cuando el otrotambién se ejecuta. Para el ejemplo de la figura 10.12, suponiendo que B 1 sea la entrada y B 3la salida:1. B 1 y B 3 son de control equivalente: B 1 domina a B 3 y B 3 posdomina a B 1.2. B 1 domina a B 2, pero B 2 no posdomina a B 1.Maq. Cap_10_AHO.indd 728 11/10/07 1:05:28 AM(c) Código máquina programado en forma global(a) Programa fuente(b) Código máquina programado en forma localFigura 10.12: Grafos de flujo antes y después de la programación global en el ejemplo 10.910.4 Programación de código global 729 LD R7,0(R2) nop ST 0(R3),(R7)B1 LD R6,0(R1) nop BEQZ R6,L LD R8,0(R4) nop ADD R8,R8,R8 ST 0(R5),R8B2L: B3e = d+dc = bif (a==0) goto LL:L: LD R6,0(R1), LD R8,0 (R4) LD R7,0(R2) ADD R8,R8,R8 BEQZ R6,LST 0(R5),R8B1B3 B3ST 0(R5),R8, ST 0(R3),R7Maq. Cap_10_AHO.indd 729 11/10/07 1:05:29 AM730 Capítulo 10. Paralelismo a nivel de instrucción3. B 2 no domina a B 3, pero B 3 posdomina a B 2.Es posible también que un par de bloques a lo largo de una ruta no compartan una relación dedominio ni de posdominio.10.4.2 Movimiento de código hacia arribaAhora vamos a examinar con cuidado lo que significa mover una operación hacia arriba en uncamino. Suponga que deseamos mover una operación del bloque org hacia arriba, por una rutade flujo de control hasta el bloque dst. Asumimos que dicho movimiento no viola ninguna dependencia de datos, y que hace que las rutas a través de dst y org se ejecuten con más rapidez.Si dst domina a org, y org posdomina a dst, entonces la operación que se movió se ejecuta sólouna vez, cuando es debido.Si org no posdomina a dstEntonces, existe un camino que pasa a través de dst y que no llega a org. En este caso se habría ejecutado una operación adicional. Este movimiento de código es ilegal, a menos que laoperación que se movió no tenga efectos adicionales no deseables. Si la operación que se movióse ejecuta “sin costo” (es decir, si utiliza sólo recursos que de otra forma estarían inactivos),entonces este movimiento no tiene costo. Es benéfico sólo si el flujo de control llega a org.Si dst no domina a orgEntonces, existe un camino que llega a org sin pasar primero a través de dst. Debemos insertarcopias de la operación que se movió a lo largo de dichas rutas. Sabemos cómo lograr exactamente eso debido a nuestra explicación sobre la eliminación de redundancia parcial de lasección 9.5. Colocamos copias de la operación a lo largo de los bloques básicos que forman unconjunto de corte, separando el bloque de entrada de org. En cada lugar en el que se inserta laoperación, se deben cumplir las siguientes restricciones:1. Los operandos de la operación deben contener los mismos valores que en la operaciónoriginal.2. El resultado no sobrescribe un valor que todavía se necesita.3. La misma operación no se sobrescribe más adelante, antes de llegar a org.Estas copias hacen que la instrucción original en org sea totalmente redundante y, por lo tanto,puede eliminarse.Nos referimos a las copias adicionales de la operación como código de compensación. Comovimos en la sección 9.5, pueden insertarse bloques básicos a lo largo de las aristas críticas conel fin de crear lugares para almacenar dichas copias. El código de compensación puede llegara provocar que algunas rutas se ejecuten con más lentitud. Por ende, el movimiento de códigomejora la ejecución del programa sólo si los caminos optimizados se ejecutan con más frecuencia que las no optimizadas.Maq. Cap_10_AHO.indd 730 11/10/07 1:05:30 AM10.4.3 Movimiento de código hacia abajoSuponga que estamos interesados en mover una operación del bloque org, hacia abajo por uncamino de flujo de control hasta el bloque dst. Podemos razonar acerca de dicho movimientode código en la misma forma que la sección anterior.Si org no domina a dstEntonces, existe un camino que llega a dst sin visitar primero a org. De nuevo, en este caso seejecutará una operación adicional. Por desgracia, el movimiento de código hacia abajo se aplicacon frecuencia a las operaciones de escritura, que tienen los efectos adicionales de sobrescribirlos valores antiguos. Podemos lidiar con este problema mediante la replicación de los bloquesbásicos a lo largo de las rutas de org a dst, y colocando la operación sólo en la nueva copia de dst.Otro método, si está disponible, es usar instrucciones predicadas. Protegemos la operación que semovió con el predicado que protege al bloque org. Tenga en cuenta que la instrucción predicadadebe programarse sólo en un bloque dominado por el cálculo del predicado, ya que éste no estará disponible en cualquier otro caso.Si dst no posdomina a orgComo en la explicación anterior, hay que insertar código de compensación para que la operaciónque se movió se ejecute en todas las rutas que no visitan a dst. De nuevo, esta transformación esanáloga a la eliminación de redundancia parcial, sólo que las copias se colocan debajo del bloqueorg en un conjunto de corte que separa a org de la salida.Resumen del movimiento de código hacia arriba y hacia abajoDe esta explicación podemos ver que hay un rango de posibles movimientos de código globales,que varían en términos de beneficio, costo y complejidad de implementación. La figura 10.13muestra un resumen de estos diversos movimientos de código; las líneas corresponden a loscuatro casos siguientes:arriba: org posdomina a dst dst domina a org especulación código deabajo: org domina a dst dst posdomina a org dup. de código compensación1 sí sí no no2 no sí sí no3 sí no no sí4 no no sí síFigura 10.13: Resumen de movimientos de código1. Es más simple y efectivo en costo mover las instrucciones entre los bloques de controlequivalente. Nunca se ejecutan operaciones adicionales y no se necesita código de compensación.10.4 Programación de código global 731Maq. Cap_10_AHO.indd 731 11/10/07 1:05:30 AM732 Capítulo 10. Paralelismo a nivel de instrucción2. Pueden ejecutarse operaciones adicionales si el origen no posdomina (domina) al destinoen el movimiento de código hacia arriba (abajo). Este movimiento de código es benéficosi las operaciones adicionales pueden ejecutarse sin costo, y se ejecuta la ruta que pasa através del bloque de origen.3. Se requiere código de compensación si el destino no domina (posdomina) al origen enel movimiento de código hacia arriba (abajo). Las rutas con el código de compensaciónpueden reducir su velocidad, por lo que es importante que las rutas optimizadas se ejecuten con más frecuencia.4. El último caso combina las desventajas de los casos segundo y tercero: pueden ejecutarseoperaciones adicionales y se requiere código de compensación.10.4.4 Actualización de las dependencias de datosComo se ilustra mediante el ejemplo 10.10 a continuación, el movimiento de código puede cambiar las relaciones de dependencia de datos entre las operaciones. Por ende, las dependenciasde datos se deben actualizar después de cada movimiento de código.Ejemplo 10.10: Para el grafo de flujo que se muestra en la figura 10.14, cualquiera de lasasignaciones a x puede moverse hacia arriba hasta el bloque superior, ya que todas las dependencias en el programa original se preservan con esta transformación. No obstante, una vezque hemos movido una asignación hacia arriba, no podemos mover la otra. Dicho en formamás específica, podemos ver que la variable x no está viva al salir en el bloque superior antesdel movimiento de código, pero lo está después del movimiento. Si una variable está viva enun punto del programa, entonces no podemos mover las definiciones especulativas a la variablepor encima de ese punto del programa. ✷Figura 10.14: Ejemplo que ilustra el cambio en las dependencias de datos, debido al movimiento de código10.4.5 Algoritmos de programación globalEn la última sección vimos que el movimiento de código puede beneficiar a ciertos caminos,mientras que afecta al rendimiento de otras. Lo bueno es que no todas las instrucciones se creande la misma forma. De hecho, está bien establecido que más del 90% del tiempo de ejecución dex = 2 x = 1Maq. Cap_10_AHO.indd 732 11/10/07 1:05:31 AMun programa se invierte en menos del 10% del código. Por ende, debemos lograr que los caminos que se ejecutan con frecuencia lo hagan con más rapidez, mientras que existe la posibilidadde que los caminos menos frecuentes se ejecuten con menos velocidad.Hay una variedad de técnicas que un compilador puede usar para estimar las frecuencias deejecución. Es razonable suponer que las instrucciones en los ciclos más internos se ejecutan mása menudo que el código en los ciclos externos, y que es más probable tomar las bifurcacionesque van hacia atrás que no hacerlo. Además, es poco probable que se tomen las instrucciones debifurcación que protegen las salidas del programa o las rutinas de manejo de excepciones. Sinembargo, las mejores estimaciones de frecuencia provienen de los perfiles dinámicos. En estatécnica, los programas se instrumentan para registrar los resultados de las bifurcaciones condicionales, a medida que se ejecutan. Después, los programas se ejecutan con entradas representativas, para determinar cómo se van a comportar en general. Los resultados que se obtienen deesta técnica han demostrado ser bastante precisos. Dicha información puede retroalimentarseal compilador, para que la utilice en sus optimizaciones.Programación basada en regionesAhora describiremos un programador global simple, que soporta las dos formas más sencillasde movimiento de código:1. Mover las operaciones hacia arriba, a los bloques básicos de control equivalente.2. Mover las operaciones en forma especulativa una bifurcación hacia arriba, hasta un predecesor dominante.En la sección 9.7.1 vimos que una región es un subconjunto de un grafo de flujo de control,al cual puede llegarse sólo a través de un bloque de entrada. Podemos representar cualquierprocedimiento como una jerarquía de regiones. El procedimiento completo constituye la regiónde nivel superior; en éste se anidan las subregiones que representan a los ciclos naturales en lafunción. Suponemos que el grafo de control de flujo es reducible.Algoritmo 10.11: Programación basada en regiones.ENTRADA: Un grafo de flujo de control y una descripción de los recursos de una máquina.SALIDA: Un programa S que asigna cada instrucción a un bloque básico y una ranura detiempo.MÉTODO: Ejecute el programa de la figura 10.15. Debe ser aparente cierta terminología deabreviación: ControlEquiv (B) es el conjunto de bloques que tiene control equivalente para elbloque B, y SucDominados, que se aplica a un conjunto de bloques, es el conjunto de bloquesque son sucesores de por lo menos un bloque en el conjunto, y están dominados por todos.La programación de código en el Algoritmo 10.11 procede de las regiones más internas alas más externas. Al programar una región, cada subregión anidada se trata como una cajanegra; no se permite a las instrucciones moverse hacia dentro o hacia fuera de una subregión.Sin embargo, pueden moverse alrededor de una subregión, siempre y cuando se cumpla con susdependencias de datos y de control.10.4 Programación de código global 733Maq. Cap_10_AHO.indd 733 11/10/07 1:05:31 AM734 Capítulo 10. Paralelismo a nivel de instrucciónfor (cada región R en orden topológico, de manera que las regiones internas se procesen antes que las regiones externas) { calcular las dependencias de datos;for (cada bloque básico B de R en orden topológico priorizado) {BloquesCand = ControlEquiv (B) ∪ SucDominados (ControlEquiv (B));InstCand = instrucciones listas en BloquesCand;for (t = 0, 1,… hasta que se programen todas las instrucciones de B) { for (cada instrucción n en InstCand en orden de prioridad) if (n no tiene conflictos de recursos en el tiempo t) { S(n) = B, t; actualizar asignaciones de recursos; actualizar dependencias de datos; } actualizar InstCand; } }}Figura 10.15: Un algoritmo de programación global basado en regionesSe ignoran todos las aristas de control y de dependencia que fluyen de regreso al encabezadode la región, por lo que los grafos resultantes de flujo de control y dependencia de datos sonacíclicos. Los bloques básicos en cada región se visitan en orden topológico. Este ordenamientogarantiza que un bloque básico no se programe sino hasta que se hayan programado todas lasinstrucciones de las que depende. Las instrucciones que se van a programar en un bloque básico Bse arrastran de todos los bloques que son de control equivalente para B (incluyendo a B), asícomo de todos sus sucesores inmediatos dominados por B.Se utiliza un algoritmo de programación por lista para crear el programa para cada bloquebásico. El algoritmo mantiene una lista de instrucciones candidatas, InstCand, que contienetodas las instrucciones en los bloques candidatos cuyos predecesores hayan sido todos programados. Crea el programa de ciclo en ciclo del reloj. Para cada ciclo de reloj, comprueba cadainstrucción de InstCand en orden de prioridad y la programa en ese ciclo de reloj, si los recursoslo permiten. Después, el Algoritmo 10.11 actualiza InstCand y repite el proceso, hasta que seprograman todas las instrucciones de B.El orden de prioridad en InstCand utiliza una función de prioridad similar a la que vimosen la sección 10.3. No obstante, hicimos una modificación importante. Dimos a las instrucciones de los bloques que tienen control equivalente para B una mayor prioridad que a las de losbloques sucesores. La razón es que las instrucciones en esta última categoría sólo se ejecutanen forma especulativa en el bloque B. ✷Maq. Cap_10_AHO.indd 734 11/10/07 1:05:32 AMDesenrollamiento de ciclosEn la programación basada en regiones, el límite de una iteración de un ciclo es una barrerapara el movimiento de código. Las operaciones de una iteración no pueden traslaparse con lasde otra. Una técnica simple, pero muy efectiva para mitigar este problema, es la de desenrollarel ciclo un pequeño número de veces antes de la programación del código. Un ciclo for como:for (i = 0; i < N; i++) { S(i);}puede escribirse como en la figura 10.16(a). De manera similar, un ciclo repeat como: repeat S;until C;puede escribirse como en la figura 10.16(b). El desenrollamiento crea más instrucciones en el cuerpo del ciclo, lo cual permite a los algoritmos de programación global encontrar más paralelismo.Desenrollamiento de un ciclo for.Desenrollamiento de un ciclo repeat.Figura 10.16: Ciclos desenrollados10.4 Programación de código global 735Maq. Cap_10_AHO.indd 735 11/10/07 1:05:32 AM736 Capítulo 10. Paralelismo a nivel de instrucciónCompactación de las proximidadesEl Algoritmo 10.11 sólo soporta las primeras dos formas de movimiento de código descritas en lasección 10.4.1. Los movimientos de código que requieren la introducción de código de compensación pueden ser útiles algunas veces. Una manera de soportar dichos movimientos de códigoes colocar una pasada simple después de la programación basada en regiones. En esta pasada,podemos examinar cada par de bloques básicos que se ejecutan uno después del otro, y comprobar si alguna operación puede moverse hacia arriba o hacia abajo entre ellos, para mejorarel tiempo de ejecución de estos bloques. Si se encuentra un par de este tipo, comprobamos si lainstrucción que se va a mover necesita duplicarse a lo largo de otras rutas. El movimiento decódigo se realiza si se produce la ganancia neta esperada.Esta extensión simple puede ser bastante efectiva para mejorar el rendimiento de los ciclos. Por ejemplo, puede mover una operación al principio de una iteración hasta el final dela siguiente iteración, al tiempo que mueve la operación de la primera iteración hacia fueradel ciclo. Esta optimización es muy atractiva para los ciclos estrechos, que son ciclos que ejecutan sólo algunas instrucciones por cada iteración. Sin embargo, el impacto de esta técnicase limita por el hecho de que cada decisión de movimiento de código se realiza en forma locale independiente.10.4.6 Técnicas avanzadas de movimiento de códigoSi nuestra máquina destino se programa en forma estática y tiene mucho paralelismo a nivelde instrucción, tal vez necesitemos un algoritmo más agresivo. He aquí una descripción de altonivel de extensiones adicionales:1. Para facilitar las siguientes extensiones, podemos agregar nuevos bloques básicos a lolargo de los flancos de flujo de control que se originan de bloques con más de un predecesor. Estos bloques básicos se eliminarán al final de la programación de código, si estánvacíos. Una heurística útil es mover las instrucciones fuera de un bloque básico que estácasi vacío, para que el bloque pueda eliminarse por completo.2. En el Algoritmo 10.11, el código a ejecutar en cada bloque básico se programa de unavez por todas, a medida que se visita cada bloque. Este método simple es suficiente, yaque el algoritmo sólo puede mover las operaciones hacia arriba, a los bloques dominantes. Para permitir movimientos que requieran la adición de código de compensación,tomamos un método ligeramente distinto. Al visitar el bloque B, sólo programamos lasinstrucciones provenientes de B y todos sus bloques de control equivalente. Primerotratamos de colocar estas instrucciones en bloques predecesores, que ya se han visitadoy para los cuales ya existe un programa parcial. Tratamos de encontrar un bloque dedestino que nos conduzca a una mejora en una ruta de ejecución frecuente y despuéscolocamos copias de la instrucción en otras rutas, para garantizar que sea lo correcto.Si las instrucciones no pueden moverse hacia arriba, se programan en el bloque básicoactual, como antes.3. Es más difícil implementar el movimiento de código hacia abajo en un algoritmo quevisite los bloques básicos en orden topológico, ya que todavía no se han programado losMaq. Cap_10_AHO.indd 736 11/10/07 1:05:33 AMbloques de destino. Sin embargo, de cualquier forma hay relativamente menos oportunidades para dicho movimiento de código. Movemos todas las operaciones que: (a) puedan moverse, y (b) no puedan ejecutarse sin costo en su bloque nativo.Esta estrategia simple funciona bien si la máquina de destino contiene muchos recursosde hardware sin uso.10.4.7 Interacción con los programadores dinámicosUn programador dinámico tiene la ventaja de que puede crear nuevos programas de acuerdocon las condiciones en tiempo de ejecución, sin tener que codificar todos estos posibles programas antes de tiempo. Si una máquina destino tiene un programador dinámico, la función principal del programador estático es asegurar que las instrucciones con alta latencia se obtenganantes de tiempo, para que el programador dinámico pueda emitirlas lo antes posible.Las fallas en caché son una clase de eventos impredecibles que pueden hacer una gran diferencia en el rendimiento de un programa. Si hay instrucciones de preobtención de datos disponibles, el programador estático puede ayudar al programador dinámico de manera considerable, alcolocar estas instrucciones de preobtención con la suficiente anticipación como para que los datosse encuentren en la caché, para cuando se les requiere. Si no hay instrucciones de preobtencióndisponibles, es útil para un compilador estimar qué operaciones tienen probabilidad de fallar, ytratar de emitirlas antes de tiempo.Si la programación dinámica no está disponible en la máquina destino, el programador estático debe ser conservador y separar cada par dependiente de datos de operaciones, en base alretraso mínimo. No obstante, si la programación dinámica está disponible, el compilador sólodebe colocar las operaciones dependientes de datos en el orden correcto, para asegurar que elprograma esté correcto. Para un mejor rendimiento, el compilador debe asignar retrasos extensos a las dependencias que tienen probabilidad de ocurrir, y retrasos cortos a las dependenciasque no tengan probabilidad de ocurrir.El mal pronóstico de las bifurcaciones es una causa importante de pérdida en el rendimiento. Debido al extenso castigo por mal pronóstico, las instrucciones en rutas que se ejecutanraras veces aún pueden tener un efecto considerable sobre el tiempo total de ejecución. Se debedar una mayor prioridad a dichas instrucciones, para reducir el costo del mal pronóstico.10.4.8 Ejercicios para la sección 10.4Ejercicio 10.4.1: Muestre cómo desenrollar el siguiente ciclo while genérico:while (C) S;Ejercicio 10.4.2: Considere el siguiente fragmento de código: if (x == 0) a = b;else a = c;d = a;10.4 Programación de código global 737!Maq. Cap_10_AHO.indd 737 11/10/07 1:05:34 AM738 Capítulo 10. Paralelismo a nivel de instrucciónSuponga que tiene una máquina que utiliza el modelo de retraso del ejemplo 10.6 (las cargasrequieren dos ciclos de reloj, y todas las demás instrucciones requieren un ciclo). Supongaademás que la máquina puede ejecutar dos instrucciones cualesquiera a la vez. Encuentre unaejecución que sea lo más corta posible para este fragmento. No olvide considerar qué registrose utiliza mejor para cada uno de los pasos de copia. Además, recuerde explotar la informaciónque proporcionan los descriptores de los registros, como se describió en la sección 8.6, paraevitar operaciones innecesarias de carga y almacenamiento.10.5 Canalización por softwareComo vimos en la introducción de este capítulo, las aplicaciones numéricas tienen, por lo regular, mucho paralelismo. En especial, a menudo tienen ciclos cuyas iteraciones son por completoindependientes unas de otras. Estos ciclos, conocidos como ciclos de ejecución total (do-all ),son bastante atractivos desde la perspectiva de la paralelización, ya que sus iteraciones puedenejecutarse en paralelo para lograr una agilización lineal en cuanto al número de iteraciones en elciclo. Los ciclos de ejecución total con muchas iteraciones tienen suficiente paralelismo parasaturar a todos los recursos en un procesador. Depende del programador si aprovecha o no porcompleto el paralelismo disponible. Esta sección describe un algoritmo, conocido como canalización por software, que programa un ciclo completo a la vez, aprovechando por completo elparalelismo entre las iteraciones.10.5.1 IntroducciónVamos a usar el ciclo de ejecución total en el ejemplo 10.12 a lo largo de esta sección, paraexplicar la canalización por software. Primero mostraremos que la programación entre iteraciones es de gran importancia, ya que hay relativamente poco paralelismo entre las operacionesen una sola iteración. A continuación, vamos a mostrar que el desenrollamiento de los ciclosmejora el rendimiento, al traslapar el cálculo de las iteraciones desenrolladas. Sin embargo,el límite del ciclo desenrollado sigue actuando como barrera para el movimiento de código, y eldesenrollamiento aún deja mucho rendimiento “en la mesa”. Por otro lado, la técnica de canalización por software traslapa un número de iteraciones consecutivas en forma continua, hastaque se agotan las iteraciones. Esta técnica permite que la canalización por software produzcacódigo compacto y muy eficiente.Ejemplo 10.12: He aquí un típico ciclo de ejecución total:for (i = 0; i < n; i++) D[i] = A[i]*B[i] + c;Las iteraciones en el ciclo anterior escriben en distintas ubicaciones de memoria, que por sí solasson diferentes de cualquiera de las ubicaciones leídas. Por lo tanto, no hay dependencias de memoria entre las iteraciones, y todas pueden proceder en paralelo.Adoptaremos el siguiente modelo como nuestra máquina destino a lo largo de esta sección.En este modelo:Maq. Cap_10_AHO.indd 738 11/10/07 1:05:34 AM10.5 Canalización por software 739• La máquina puede emitir en un solo ciclo de reloj: una operación de carga, una de almacenamiento, una aritmética y una bifurcación.• La máquina tiene una operación de regreso al ciclo de la siguiente forma: BL R, Lla cual decrementa al registro R y, a menos que el resultado sea 0, bifurca hacia la ubicación L.• Las operaciones de memoria tienen un modo de direccionamiento con autoincremento,denotado por ++ después del registro. El registro se incrementa de manera automáticapara apuntar a la siguiente dirección consecutiva después de cada acceso.• Las operaciones aritméticas están canalizadas por completo; pueden iniciarse en cada ciclo del reloj, pero sus resultados no están disponibles sino hasta 2 ciclos de reloj después.Todas las demás instrucciones tienen una latencia de un solo ciclo de reloj.Si las iteraciones se programan una a la vez, el mejor programa que podemos obtener ennuestro modelo de máquina se muestra en la figura 10.17. Algunas suposiciones acerca de ladistribución de los datos también se indican en esa figura: los registros R1, R2 y R3 contienenlas direcciones de los inicios de los arreglos A, B y D, el registro R4 contiene la constante c,y el registro R10 contiene el valor n − 1, que se ha calculado fuera del ciclo. El cálculo es ensu mayor parte serial, y requiere un total de 7 ciclos; sólo la instrucción de regreso al ciclo setraslapa con la última operación en la iteración.Figura 10.17: Código programado en forma local para el ejemplo 10.12En general, obtenemos un mejor uso del hardware al desenrollar varias iteraciones de unciclo. Sin embargo, al hacer esto se incrementa el tamaño del código, lo que a su vez puedetener un impacto negativo sobre el rendimiento en general. Por ende, tenemos que comprometernos, elegir un número de veces para desenrollar un ciclo que obtenga la mayoría de lamejora en el rendimiento, y que no expanda el código demasiado. El siguiente ejemplo ilustraesta concesión.Maq. Cap_10_AHO.indd 739 11/10/07 1:05:35 AM740 Capítulo 10. Paralelismo a nivel de instrucciónEjemplo 10.13: Aunque es difícil encontrar paralelismo en cada iteración del ciclo del ejemplo10.12, hay mucho paralelismo a través de las iteraciones. El desenrollamiento del ciclo coloca varias iteraciones del mismo en un bloque básico grande, y puede utilizarse un algoritmo simple deprogramación por lista para programar las operaciones a ejecutar en paralelo. Si desenrollamos elciclo en nuestro ejemplo cuatro veces, y aplicamos el Algoritmo 10.7 al código, podemos obtenerel programa que se muestra en la figura 10.18. (Por cuestión de simplicidad, ignoramos los detallesde la asignación de registros por ahora.) El ciclo se ejecuta en 13 ciclos de reloj, o una iteracióncada 3.25 ciclos.Un ciclo que se desenrolla k veces requiere por lo menos 2k + 5 ciclos de reloj, con lo cualse logra una tasa de transferencia de una iteración por cada 2 + 5/k ciclos de reloj. Por ende,entre más iteraciones desenrollemos, más rápido se ejecutará el ciclo. A medida que n → £,un ciclo desenrollado por completo se puede ejecutar en promedio una iteración cada dos ciclosde reloj. Sin embargo, entre más iteraciones desenrollemos, más grande se hará el código. Esevidente que no podemos darnos el lujo de desenrollar todas las iteraciones en un ciclo. Si desenrollamos el ciclo 4 veces se produce código con 13 instrucciones, o un 163% del valor óptimo; sidesenrollamos el ciclo 8 veces se produce código con 21 instrucciones, o un 131% del valor óptimo. Por el contrario, si deseamos operar, por decir a sólo un 110% del valor óptimo, debemosdesenrollar el ciclo 25 veces, lo cual produciría un código con 55 instrucciones. ✷10.5.2 Canalización de los ciclos mediante softwareLa canalización por software proporciona una manera conveniente de obtener un uso óptimode los recursos y un código compacto al mismo tiempo. Vamos a ilustrar esta idea con nuestroejemplo abierto.Ejemplo 10.14: En la figura 10.19 está el código del ejemplo 10.12, desenrollado cinco veces.De nuevo, omitiremos la consideración del uso de los registros. En la fila i se muestran todas lasoperaciones que se emiten en el ciclo de reloj i; en la columna j se muestran todas las operaciones de la iteración j. Observe que cada iteración tiene el mismo programa relativo a su inicio, yobserve también que cada iteración se inicia dos ciclos de reloj después de la que le precede.Es fácil ver que este programa cumple con todas las restricciones de recursos y dependenciade datos.Podemos observar que las operaciones que se ejecutan en los ciclos 7 y 8 son las mismas quese ejecutan en los ciclos 9 y 10. Los ciclos de reloj 7 y 8 ejecutan las operaciones de las primeras cuatro iteraciones en el programa original. Los relojes 9 y 10 también ejecutan operacionesde cuatro iteraciones, esta vez de las iteraciones 2 a la 5. De hecho, podemos seguir ejecutandoel mismo par de instrucciones con múltiples operaciones para obtener el efecto de retirar laiteración más antigua y agregar una nueva, hasta que se agoten las iteraciones.Dicho comportamiento dinámico puede codificarse brevemente con el código que se muestra en la figura 10.20, si asumimos que el ciclo tiene por lo menos 4 iteraciones. Cada fila enla figura corresponde a una instrucción de máquina. Las líneas 7 y 8 forman un ciclo de dosciclos de reloj, el cual se ejecuta n − 3 veces, en donde n es el número de iteraciones en el ciclooriginal. ✷Maq. Cap_10_AHO.indd 740 11/10/07 1:05:36 AM10.5 Canalización por software 741Figura 10.18: Código desenrollado para el ejemplo 10.12Figura 10.19: Cinco iteraciones desenrolladas del código del ejemplo 10.12Maq. Cap_10_AHO.indd 741 11/10/07 1:05:36 AM742 Capítulo 10. Paralelismo a nivel de instrucciónFigura 10.20: Código con canalización por software para el ejemplo 10.12La técnica antes descrita se conoce como canalización por software, ya que es la analogíade software de una técnica que se utiliza para la programación de canalizaciones por hardware. Podemos considerar el programa ejecutado por cada iteración en este ejemplo como unacanalización de 8 etapas. Se puede iniciar una nueva iteración en la canalización cada 2 ciclosde reloj. Al principio, sólo hay una iteración en la canalización. Cuando la primera iteraciónprocede a la etapa tres, la segunda iteración empieza a ejecutarse en la primera etapa de lacanalización.Para el ciclo de reloj 7, la canalización está llena por completo con las primeras cuatro iteraciones. En el estado estable, se ejecutan cuatro iteraciones consecutivas al mismo tiempo. Seinicia una nueva iteración cuando se retira la iteración más antigua en la canalización. Cuandose agotan las iteraciones, la canalización se drena y todas las iteraciones en la canalización seejecutan hasta completarse. La secuencia de instrucciones que se utiliza para llenar la canalización, las líneas 1 a 6 en nuestro ejemplo, se conoce como el prólogo; las líneas 7 y 8 son elestado estable; y la secuencia de instrucciones que se utiliza para drenar la canalización, laslíneas de la 9 a 14, es el epílogo.Para este ejemplo, sabemos que el ciclo no puede ejecutarse a una velocidad mayor de 2ciclos de reloj por iteración, ya que la máquina sólo puede emitir una lectura por cada ciclo dereloj, y hay dos lecturas en cada iteración. El ciclo anterior canalizado por software se ejecutaen 2n + 6 ciclos de reloj, en donde n es el número de iteraciones en el ciclo original. A medidaque n → £, la tasa de transferencia se aproxima a la velocidad de una iteración por cada dosciclos de reloj. Por ende, a diferencia del desenrollamiento, la programación por software tieneel potencial de codificar el programa óptimo con una secuencia de código muy compacta.Observe que el programa adoptado por cada iteración individual no es el más corto posible.La comparación con el programa optimizado en forma local que se presenta en la figura 10.17muestra que se introduce un retraso antes de la operación ADD. El retraso se coloca de maneraestratégica, para que el programa pueda iniciarse cada dos ciclos de reloj sin conflictos en losrecursos. Si nos hubiéramos apegado al programa compactado en forma local, el intervalo deMaq. Cap_10_AHO.indd 742 11/10/07 1:05:37 AM10.5 Canalización por software 743iniciación tendría que extenderse hasta 4 ciclos de reloj para evitar conflictos en los recursos,y la velocidad de la tasa de transferencia se reduciría a la mitad. Este ejemplo ilustra un principio importante en la programación por canalización: el programa debe elegirse con cuidadopara poder optimizar la tasa de transferencia. Un programa compactado en forma local, aunque disminuye al mínimo el tiempo para completar una iteración, puede producir una tasa detransferencia por debajo de la óptima a la hora de canalizarse.10.5.3 Asignación de recursos y generación de códigoVamos a empezar por hablar sobre la asignación de registros para el ciclo canalizado por software del ejemplo 10.14.Ejemplo 10.15: En el ejemplo 10.14, el resultado de la operación de multiplicación en la primera iteración se produce en el ciclo de reloj 3 y se utiliza en el ciclo 6. Entre estos ciclos de relojse genera un nuevo resultado debido a la operación de multiplicación en la segunda iteración,en el ciclo de reloj 5; este valor se utiliza en el ciclo 8. Los resultados de estas dos iteracionesdeben guardarse en registros diferentes para evitar que interfieran uno con el otro. Como lainterferencia ocurre sólo entre pares adyacentes de iteraciones, puede evitarse con el uso de dosregistros, uno para las iteraciones impares y otro para las iteraciones pares. Como el código paralas iteraciones impares es distinto del de las pares, se duplica el tamaño del ciclo en el estadoestable. Este código puede usarse para ejecutar cualquier ciclo que tenga un número impar deiteraciones mayor o igual a 5.Figura 10.21: Desenrollamiento a nivel de código fuente del ciclo del ejemplo 10.12Para manejar ciclos que tengan menos de 5 iteraciones y ciclos con un número par de iteraciones, generamos el código cuyo equivalente a nivel de código fuente se muestra en la figura10.21. El primer ciclo está canalizado, como puede verse en el equivalente a nivel de máquinade la figura 10.22. El segundo ciclo de la figura 10.21 no necesita optimizarse, ya que puedeiterar cuando mucho cuatro veces. ✷10.5.4 Ciclos de ejecución cruzadaLa canalización por software también puede aplicarse a los ciclos cuyas iteraciones compartendependencias de datos. Dichos ciclos se conocen como ciclos de ejecución cruzada.Maq. Cap_10_AHO.indd 743 11/10/07 1:05:38 AM744 Capítulo 10. Paralelismo a nivel de instrucciónFigura 10.22: El código después de la canalización por software y la asignación de recursos enel ejemplo 10.15Ejemplo 10.16: El siguiente código:for (i = 0; i < n; i++) { suma = suma + A[i]; B[i] = A[i] * b;}tiene una dependencia de datos entre iteraciones consecutivas, ya que el valor anterior de sumase suma a A[i ] para crear un nuevo valor de suma. Es posible ejecutar la suma en un tiempodeterminado por O(log n) si la máquina puede producir el paralelismo suficiente, pero por elbien de esta explicación, sólo asumiremos que se deben obedecer todas las dependencias secuenciales, y que todas las sumas deben realizarse en el orden secuencial original. Como nuestrosupuesto modelo de máquina requiere dos ciclos de reloj para completar una operación ADD, elciclo no se puede ejecutar con más rapidez que la de una iteración por cada dos ciclos de reloj.Si proporcionamos a la máquina más sumadores o multiplicadores, el ciclo no se ejecutará másrápido. La tasa de transferencia de los ciclos de ejecución cruzada como éste se limita en basea la cadena de dependencias a través de las iteraciones.En la figura 10.23(a) se muestra el mejor programa compactado en forma local para cadaiteración, y en la figura 10.23(b) el código con canalización por software. Este ciclo canalizado por software empieza una iteración cada dos ciclos, y por ende opera a la velocidad óptima. ✷Maq. Cap_10_AHO.indd 744 11/10/07 1:05:39 AM10.5 Canalización por software 745sumaEl mejor programa compactado en forma local.sumaLa versión canalizada por software.Figura 10.23: Canalización por software de un ciclo de ejecución cruzada10.5.5 Objetivos y restricciones de la canalización por softwareEl principal objetivo de la canalización por software es incrementar al máximo la tasa detransferencia de un ciclo de larga duración. Un objetivo secundario es mantener el códigogenerado de un tamaño razonablemente pequeño. En otras palabras, el ciclo canalizado porsoftware debe tener un pequeño estado estable de la canalización. Podemos lograr un pequeñoestado estable al requerir que el programa relativo de cada iteración sea el mismo, y que lasiteraciones se inicien en un intervalo constante. Como la tasa de transferencia del ciclo es simplemente el inverso del intervalo de iniciación, el objetivo de la canalización por software esminimizar este intervalo.Un programa de canalización por software para un grafo de dependencia de datos G = (N, E)puede especificarse mediante:1. Un intervalo de iniciación T.2. Un programa S relativo que especifique, para cada operación, cuándo se va a ejecutar esaoperación, en forma relativa al inicio de la iteración a la cual pertenece.Maq. Cap_10_AHO.indd 745 11/10/07 1:05:40 AM746 Capítulo 10. Paralelismo a nivel de instrucciónAsí, una operación n en la i-ésima iteración, contando a partir de 0, se ejecuta en el ciclo i ×T+ S(n). Al igual que todos los demás problemas de programación, la canalización por softwaretiene dos tipos de restricciones: recursos y dependencias de datos. A continuación hablaremossobre cada una de ellas con detalle.Reservación modular de recursosHagamos que los recursos de una máquina se representen mediante R = [r 1, r 2,… ], en donderi es el número de unidades del i-ésimo tipo de recurso disponible. Si una iteración de un ciclorequiere ni unidades del recurso i, entonces el intervalo de iniciación promedio de un ciclo canalizado es, por lo menos, de máxi(ni/ri) ciclos de reloj. La canalización por software requiereque los intervalos de iniciación entre cualquier par de iteraciones tengan un valor constante.Por ende, el intervalo de iniciación debe tener por lo menos máxi[ni/ri ] ciclos de reloj. Simáxi(ni/ri ) es menor que 1, es útil desenrollar el código fuente un pequeño número de veces.Ejemplo 10.17: Vamos a regresar a nuestro ciclo canalizado por software, que se muestra enla figura 10.20. Recuerde que la máquina destino puede emitir una operación de carga, unaoperación aritmética, una de almacenamiento y una bifurcación de regreso de ciclo, por cadaciclo de reloj. Como el ciclo tiene dos cargas, dos operaciones aritméticas y una operación dealmacenamiento, el intervalo mínimo de iniciación basado en las restricciones de los recursoses de 2 ciclos de reloj.Ld Alu StLd Alu StLd Alu StLd Alu St Ld Alu StEstado estableIteración 1Iteración 2Iteración 3Iteración 4TiempoFigura 10.24: Requerimientos de recursos de cuatro iteraciones consecutivas del ejemplo en lafigura 10.13La figura 10.24 muestra los requerimientos de recursos de cuatro iteraciones consecutivas através del tiempo. Se utilizan más recursos a medida que se inician más iteraciones, culminandoMaq. Cap_10_AHO.indd 746 11/10/07 1:05:42 AM10.5 Canalización por software 747en la asignación máxima de recursos en el estado estable. Suponga que RT es la tabla de reservación de recursos que representa la asignación de una iteración, y suponga que RTS representala asignación del estado estable. RTS combina la asignación de cuatro iteraciones consecutivasque empiezan T ciclos de reloj aparte. La asignación de la fila 0 en la tabla RTS correspondea la suma de los recursos asignados en RT [0], RT [2], RT [4] y RT [6]. De manera similar, laasignación de la fila 1 en la tabla corresponde a la suma de los recursos asignados en RT [1],RT [3], RT [5] y RT [7]. Es decir, los recursos asignados en la i-ésima fila en el estado estable sedan mediante:Nos referimos a la tabla de reservación de recursos que representan el estado estable como latabla de reservación de recursos modular del ciclo canalizado.Para comprobar si el programa de canalización por software tiene conflictos de recursos, podemos simplemente comprobar la asignación de la tabla de reservación de recursos. Sin duda, sipuede cumplirse la asignación en el estado estable, también se pueden cumplir las asignaciones en el prólogo y el epílogo, las porciones de código antes y después del ciclo en estado estable. ✷En general, dado un intervalo de iniciación T y una tabla de reservación de recursos de unaiteración RT, el programa canalizado no tiene conflictos de recursos en una máquina con elvector de recursos R, si y sólo si RTS[i] ¥ R para todas las i = 0, 1,…, T − 1.Restricciones de dependencia de datosLas dependencias de datos en la canalización por software son distintas de las que nos hemosencontrado hasta ahora, debido a que pueden formar ciclos. Una operación puede depender en elresultado de la misma operación de una iteración anterior. Ya no es adecuado etiquetar una arista de dependencia sólo con base en el retraso; también debemos diferenciar entre las instanciasde la misma operación en distintas iteraciones. Etiquetamos una arista de dependencia n1 → n2con la etiqueta δ, d  si la operación n2 en la iteración i debe retrasarse por lo menos d ciclos dereloj después de la ejecución de la operación n1 en la iteración i − δ. Suponga que S, una funciónque va de los nodos del grafo de dependencia de datos a enteros, es el programa de canalización por software, y suponga que T es el destino del intervalo de iniciación. Entonces:La diferencia de la iteración, δ, debe ser no negativa. Además. Dado un ciclo de aristas dedependencia de datos, por lo menos una de las aristas tiene una diferencia de iteración positiva.Ejemplo 10.18: Considere el siguiente ciclo, y suponga que no conocemos los valores de p y q:Maq. Cap_10_AHO.indd 747 11/10/07 1:05:43 AM748 Capítulo 10. Paralelismo a nivel de instrucciónDebemos suponer que cualquier par de accesos *(p++) y *(q++) puede referirse a la misma ubicación de memoria. Por ende, todas las lecturas y escrituras deben ejecutarse en el mismo ordensecuencial original. Suponiendo que la máquina destino tiene las mismas características que lasdescritas en el ejemplo 10.13, los flancos de dependencia de datos para este código son como semuestra en la figura 10.25. Sin embargo, observe que ignoramos las instrucciones de controlde ciclo que tendrían que estar presentes, ya sea calculando y evaluando i, o realizando la pruebacon base en el valor de R1 o R2. ✷i1i2i3Figura 10.25: Grafo de dependencia de datos para el ejemplo 10.18La diferencia de iteración entre las operaciones relacionadas puede ser mayor que uno, comose muestra en el siguiente ejemplo: for (i = 2; i < n; i++) A[i] = B[i] + A[i−2]Aquí, el valor escrito en la iteración i se utiliza dos iteraciones después. La arista de dependencia entre la operación de almacenamiento de A[i] y la carga de A[i − 2] tiene, por lo tanto,una diferencia de 2 iteraciones.La presencia de ciclos de dependencia de datos en un ciclo impone otro límite más encuanto a su tasa de transferencia de ejecución. Por ejemplo, el ciclo de dependencia de datosen la figura 10.25 impone un retraso de 4 ciclos de reloj entre las operaciones de carga de lasiteraciones consecutivas. Es decir, los ciclos no pueden ejecutarse a una velocidad mayor queuna iteración cada 4 ciclos de reloj.El intervalo de iniciación de un ciclo canalizado no es menor que:c es un ciclo en enen áEn resumen, el intervalo de iniciación de cada ciclo canalizado por software está delimitadopor el uso de recursos en cada iteración. Es decir, el intervalo de iniciación no debe ser menorque la proporción de unidades necesarias de cada recurso y las unidades disponibles en lamáquina. Además, si los ciclos tienen ciclos de dependencia de datos, entonces el intervalo deLD R4,0(R1++)ADD R5,R4,R3ST 0(R2++),R5<0,1><0,2><1,1>Maq. Cap_10_AHO.indd 748 11/10/07 1:05:44 AM10.5 Canalización por software 749iniciación se restringe aún más debido a la suma de los retrasos en el ciclo, dividida por la sumade las diferencias de iteración. La mayor de estas cantidades define un límite inferior sobre elintervalo de iniciación.10.5.6 Un algoritmo de canalización por softwareEl objetivo de la canalización por software es encontrar un programa con el menor intervalode iniciación posible. El problema es NP-completo, y puede formularse como un problema deprogramación lineal de enteros. Hemos mostrado que si conocemos el intervalo de iniciaciónmínimo, el algoritmo de programación puede evitar conflictos de recursos mediante el uso de latabla de reservación de recursos modular para colocar cada operación. Pero no sabremos cuáles el intervalo de iniciación mínimo sino hasta que podamos encontrar un programa. ¿Cómoresolvemos esta circularidad?Sabemos que el intervalo de iniciación debe ser mayor que el límite calculado a partir del requerimiento de recursos de un ciclo y los ciclos de dependencia, como vimos antes. Si podemosencontrar un programa que cumpla con este límite, hemos encontrado el programa óptimo. Sino podemos encontrar un programa así, podemos intentar de nuevo con intervalos de iniciaciónmás largos hasta encontrar un programa. Observe que si se utiliza la heurística en vez de labúsqueda exhaustiva, tal vez este proceso no encuentre el programa óptimo.La probabilidad de encontrar un programa cerca del límite inferior depende de las propiedades del grafo de dependencia de datos y de la arquitectura de la máquina de destino. Podemosencontrar con facilidad el programa óptimo si el grafo de dependencia es acíclico y si cadainstrucción de máquina sólo necesita una unidad de un recurso. También es fácil encontrar unprograma cerca del límite inferior si hay más recursos de hardware de los que pueden usarsepor los grafos con ciclos de dependencia. Para tales casos, es aconsejable empezar con el límiteinferior como el destino del intervalo de iniciación inicial, y después seguir incrementando eldestino mediante sólo un ciclo de reloj con cada intento de programación. Otra posibilidades buscar el intervalo de iniciación usando una búsqueda binaria. Podemos emplear como unlímite superior sobre el intervalo de iniciación la longitud del programa para una iteraciónproducida mediante la programación por listas.10.5.7 Programación de grafos de dependencia de datos acíclicosPor cuestión de simplicidad, vamos a suponer por ahora que el ciclo que se va a canalizar porsoftware contiene sólo un bloque básico. Haremos esta suposición más flexible en la sección10.5.11.Algoritmo 10.19: Canalización por software de un grafo de dependencia acíclico.ENTRADA: Un vector de recursos de máquina R = [r 1, r 2,…], en donde ri es el número de unidades disponibles del i-ésimo tipo de recurso, y un grafo de dependencia de datos G = (N, E).Cada operación n en N se etiqueta con su tabla de reservación de recursos R Tn ; cada flancoe = n 1 → n 2 en E se etiqueta con δe, de , lo cual indica que n 2 no se debe ejecutar antes de deciclos después del nodo n 1, de la δ e-ésima iteración siguiente.SALIDA: Un programa S canalizado por software y un intervalo de iniciación T.Maq. Cap_10_AHO.indd 749 11/10/07 1:05:46 AM750 Capítulo 10. Paralelismo a nivel de instrucciónMÉTODO: Ejecute el programa de la figura 10.26. ✷main() {áfor (T = T 0, T 0 + 1,…, hasta que se programen todos los nodos en N) {RT = una tabla de reservación vacía con T filas;for (cada n en N, en orden topológico priorizado) { s 0 = máxe = p→n en E (S(p) + de); for (s = s 0, s 0 + 1,…, s 0 + T − 1) if (NodoProgramado(RT, T, n, s) break; if (n no puede programarse en RT ) break; } }}NodoProgramado (RT, T, n, s) {RT  = RT;for (cada fila i en RTn)RT [(s + i) mod T] = RT [(s + i) mod T] + RTn[i];if (para todas las i, RT (i) ¥ R) {RT = RT ;S(n) = s;return true; }else return false;}Figura 10.26: Algoritmo de canalización por software para los grafos acíclicosEl Algoritmo 10.19 canaliza por software los grafos de dependencia de datos acíclicos. El algoritmo busca primero un límite en el intervalo de iniciación, T 0, con base en los requerimientosde recursos de las operaciones en el grafo. Después trata de encontrar un programa canalizadopor software, empezando con T 0 como el intervalo de iniciación de destino. El algoritmo se repite con intervalos de iniciación cada vez más grandes si no puede encontrar un programa.El algoritmo utiliza un método de programación por lista en cada intento. Utiliza una tablaRT de reservación de recursos modular para llevar la cuenta de la asignación de recursos en elestado estable. Las operaciones se programan en orden topológico, para que las dependencias dedatos siempre puedan satisfacerse mediante las operaciones de retraso. Para programar unaoperación, primero busca un límite inferior s 0 de acuerdo con las restricciones de dependencia de datos. Después invoca a NodoProgramado para comprobar posibles conflictos de recursosen el estado estable. Si hay un conflicto de recursos, el algoritmo trata de programar la operación en el siguiente ciclo de reloj. Si se descubre que la operación tiene conflictos duranteMaq. Cap_10_AHO.indd 750 11/10/07 1:05:46 AM10.5 Canalización por software 751T ciclos de reloj consecutivos, debido a la naturaleza modular de la detección de conflictosde recursos, se garantiza que los siguientes intentos serán en vano. En ese punto, el algoritmoconsidera el intento como una falla y se prueba otro intervalo de iniciación.La heurística de las operaciones de programación tiende lo más pronto posible a disminuiral mínimo la longitud del programa para una iteración. Sin embargo, programar una instrucción lo más pronto posible, puede alargar los tiempos de vida de algunas variables. Por ejemplo,las cargas de datos tienden a programarse antes, algunas veces mucho antes de utilizarse. Unaheurística simple es la de programar el grafo de dependencia al revés, ya que, por lo general, haymás cargas que almacenamientos.10.5.8 Programación de grafos de dependencia cíclicosLos ciclos de dependencia complican la canalización por software de manera considerable.Cuando se programan operaciones en un grafo acíclico en orden topológico, las dependenciasde datos con las operaciones programadas pueden imponer sólo un límite inferior en cuanto a lacolocación de cada operación. Como resultado, siempre es posible satisfacer las restriccionesde dependencia de datos al retrasar las operaciones. El concepto de “orden topológico” no seaplica a los grafos acíclicos. De hecho, dado un par de operaciones que comparten un ciclo, alcolocar una operación se impondrá tanto un límite inferior como superior en la colocación dela segunda operación.Hagamos que n 1 y n 2 sean dos operaciones en un ciclo de dependencia, que S sea un programa de canalización por software, y que T sea el intervalo de iniciación para el programa.Una arista de dependencia n 1 → n 2 con la etiqueta δ 1, δ 1  impone la siguiente restricción sobreS(n 1) y S(n 2):De manera similar, un flanco de dependencia (n 1, n 2) con la etiqueta δ 2, d 2 impone la siguienterestricción:Así,Un componente fuertemente conectado (Strongly Connected Component, SCC) en un grafoes un conjunto de nodos, en donde se puede llegar a cada uno en el componente desde cualquierotro nodo en el mismo. La programación de un nodo en un SCC limitará el tiempo de los demásnodos en el componente, tanto de arriba como de abajo. Por transición, si existe una ruta pque conduce de n 1 a n 2, entonces:en(10.1)Maq. Cap_10_AHO.indd 751 11/10/07 1:05:47 AM752 Capítulo 10. Paralelismo a nivel de instrucciónObserve que:• Alrededor de cualquier ciclo, la suma de las δ debe ser positiva. Si fuera 0 o negativa,entonces indicaría que una operación en el ciclo tendría que precederse a sí misma o ejecutarse en el mismo ciclo de reloj para todas las iteraciones.• El programa de operaciones dentro de una iteración es igual para todas las iteraciones;en esencia, ese requerimiento es el significado de una “canalización por software”. Comoresultado, la suma de los retrasos (segundos componentes de las etiquetas de las aristasen un grafo de dependencia de datos) alrededor de un ciclo es un límite inferior sobre elintervalo de iniciación T.Cuando combinamos estos dos puntos, podemos ver que para cualquier intervalo de iniciaciónT viable, el valor del lado derecho de la ecuación (10.1) debe ser negativo o cero. Como resultado, las restricciones más fuertes en la colocación de los nodos se obtienen de los caminossimples; aquellas rutas que no contienen ciclos.Así, para cada T viable, calcular el efecto transitivo de las dependencias de datos en cadapar de nodos equivale a encontrar la longitud de la ruta simple más larga, desde el primernodo hasta el segundo. Además, como los ciclos no pueden aumentar la longitud de un camino,podemos usar un algoritmo simple de programación dinámica para encontrar los caminos máslargos sin el requerimiento del “camino simple”, y estar seguros de que las longitudes resultantes también serán las longitudes de los caminos simples más largos (vea el ejercicio 10.5.7).<0,2>adb<0,1>c<0,2><0,1><1,1>Figura 10.27: Grafo de dependencia y requerimiento de recursos del ejemplo 10.20Ejemplo 10.20: La figura 10.27 muestra un grafo de dependencia de datos con cuatro nodosa, b, c, d. A cada nodo se le adjunta su tabla de reservación de recursos; a cada arista se le adjunta su diferencia de iteración y su retraso. Para este ejemplo, suponga que la máquina dedestino tiene una unidad de cada tipo de recurso. Como hay tres usos del primer recurso y dosdel segundo, el intervalo de iniciación no debe ser menor de 3 ciclos de reloj. Hay dos SCCs eneste grafo: el primero es un componente trivial que consiste en el nodo a por sí solo, y el segundoconsiste en los nodos b, c y d. El ciclo más largo, b, c, d, b, tiene un retraso total de 3 ciclos dereloj que conecta a los nodos que están una iteración aparte. Por ende, el límite inferior en el intervalo de iniciación producido por las restricciones del ciclo de dependencia de datos es tambiénde 3 ciclos de reloj.Maq. Cap_10_AHO.indd 752 11/10/07 1:05:48 AM10.5 Canalización por software 753Al colocar uno de los nodos b, c o d en un programa, se restringe a todos los demás nodosen el componente. Suponga que T es el intervalo de iniciación. La figura 10.28 muestra lasdependencias transitivas. La parte (a) muestra el retraso y la diferencia de iteración δ, paracada arista. El retraso se representa directamente, pero δ se representa “sumando” al retrasoel valor −δT.La figura 10.28(b) muestra la longitud del camino simple más largo entre dos nodos, si es queexiste dicho camino; sus entradas son las sumas de las expresiones dadas por la figura 10.28(a),para cada flanco a lo largo del camino. Por ende, en (c) y (d) podemos ver las expresiones de (b)con los dos valores relevantes de T, es decir, 3 y 4, que se sustituyen por T. La diferencia entreel programa de dos nodos S(n 2) − S(n 1) no debe ser menor que el valor que se proporciona enla entrada (n 1, n 2) en cada una de las tablas (c) o (d), dependiendo del valor elegido de T.Por ejemplo, considere la entrada en la figura 10.28 para el camino (simple) más largo dec a b, que es 2 − T. El camino simple más largo de c a b es c → d → b. El retraso total es de 2a lo largo de este camino, y la suma de las δ es 1, lo cual representa el hecho de que el númerode la iteración debe incrementarse en 1. Como T es el tiempo durante el cual cada iteraciónsigue a la anterior, el ciclo de reloj en el cual b debe programarse es por lo menos de 2 − Tciclos de reloj después del ciclo de reloj en el que se programa c. Como T es por lo menos de 3,en realidad estamos diciendo que b puede programarse T − 2 ciclos de reloj antes que c, odespués de ese ciclo de reloj, pero no antes.Observe que si consideramos rutas no simples de c a b, no se produce una restricción másfuerte. Podemos agregar al camino c → d → b cualquier número de iteraciones del ciclo queinvolucren a d y b. Si agregamos k ciclos de ese tipo, obtenemos una longitud de 2 − T + k (3 −T), ya que el retraso total a lo largo del camino es de 3, y la suma de las δ es 1. Como T ¦ 3,esta longitud no puede exceder a 2 − T; es decir, el límite inferior más fuerte en el reloj de brelativo al ciclo de reloj de c es de 2 − T, el límite que obtenemos al considerar el camino simplemás largo.Por ejemplo, de las entradas (b, c) y (c, b ), podemos ver que: S (c ) − S (b ) ¦ 1S(b ) − S (c ) ¦ 2 − T.Es decir,S(b ) + 1 ¥ S (c ) ¥ S(b ) − 2 + T.Si T = 3,S(b ) + 1 ¥ S (c ) ¥ S(b ) + 1.De manera equivalente, c debe programarse un ciclo de reloj después de b. No obstante, si T = 4,S(b ) + 1 ¥ S (c ) ¥ S(b ) + 2.Es decir, c se programa uno o dos ciclos de reloj después de b.Dada toda la información del camino más largo de todos los puntos, podemos calcularcon facilidad el rango en donde es válido colocar un nodo debido a las dependencias de datos.Podemos ver que no hay espacio cuando T = 3, y que el espacio aumenta a medida que T seincrementa. ✷Maq. Cap_10_AHO.indd 753 11/10/07 1:05:50 AM754 Capítulo 10. Paralelismo a nivel de instrucciónacdabcdbacdabcdbacdabcdbacdabcdb1ï T2ï T1ï T 2ï T213 4ï122 221111111ï1ï2ï223 4 3 4(b) Caminos simples más largos.(c) Caminos simples más largos (T=3). (d) Caminos simples más largos (T=3).2 22 3(a) Aristas originales.Figura 10.28: Dependencias transitivas del ejemplo 10.20Algoritmo 10.21: Canalización por software.ENTRADA: Un vector de recursos de máquina R = [r 1, r 2,…], en donde ri es el número deunidades disponibles en el i-ésimo tipo de recurso, y un grafo de dependencia de datos G =(N, E). Cada operación n en N se etiqueta con su tabla de reservación de recursos RTn; cadaarista e = n 1 → n 2 en E se etiqueta con (δe, de), indicando que n 2 no debe ejecutarse antes dede ciclos después del nodo n 1, de la δe-ésima iteración siguiente.SALIDA: Un programa S canalizado por software y un intervalo de iniciación T.MÉTODO: Ejecute el programa de la figura 10.29. ✷El Algoritmo 10.21 tiene una estructura de alto nivel similar a la del Algoritmo 10.19, quesólo maneja grafos acíclicos. El intervalo de iniciación mínimo en este caso se delimita no sólopor los requerimientos de recursos, sino también por los ciclos de dependencia de datos en elgrafo, el cual se programa un componente fuertemente conectado a la vez. Al tratar a cadacomponente como una unidad, las aristas entre estos componentes forman necesariamente ungrafo acíclico. Mientras que el ciclo de nivel superior en el Algoritmo 10.19 programa los nodosen el grafo en orden topológico, el ciclo de nivel superior en el Algoritmo 10.21 programa loscomponentes fuertemente conectados en orden topológico. Como antes, si el algoritmo no puede programar todos los componentes, entonces se prueba con un intervalo de iniciación máslargo. Observe que el Algoritmo 10.21 se comporta en forma idéntica al Algoritmo 10.19, sirecibe un grafo de dependencia de datos acíclico.El Algoritmo 10.21 calcula dos conjuntos más de aristas: E es el conjunto de todos las aristas cuya diferencia de iteración es 0, E* representa a todos los flancos de ruta más larga de todosMaq. Cap_10_AHO.indd 754 11/10/07 1:05:50 AM10.5 Canalización por software 755main() {E = {e |e en E, δe = 0};á á á c acíclico en G enen for (T = T0, T0 + 1, … o hasta que se programen todos los SCCs en G) {RT = una tabla de reservación vacía con T filas;E* = RutaMasLargaTodosLosPares(G,T);for (cada SCC C en G, en orden topológico priorizado) { for (todas las n en C) s0(n) = máxe= p →n en E *.p programado (S(p) + de); primero = cierta n tal que s 0(n) sea un mínimo; s 0 = s 0(primero); for (s = s 0; s < s 0 + T; s = s + 1) if (SccProgramado (RT, T, C, primero, s)) break; if (C no puede programarse en RT ) break; } }}SccProgramado(RT, T, c, primero, s) {RT  = RT;if (not NodoProgramado (RT , T, primero, s)) return false;for (cada n restante en c, en orden topológico priorizado de aristas en E ) {s t = máxe= n →n en E*, n en c, n programado S(n ) + d e − (δe × T);s u = míne= n →n en E*, n en c, n programado S(n ) − d e + (δe × T);for (s = s l; ¥ mín(s u, s l + T − 1); s = s + 1) if NodoProgramado(RT , T, n, s) break;if (n no puede programarse en RT  ) return false; }RT = RT ;return true;}Figura 10.29: Un algoritmo de canalización por software para las grafos de dependencia cíclicosMaq. Cap_10_AHO.indd 755 11/10/07 1:05:51 AM756 Capítulo 10. Paralelismo a nivel de instrucciónlos puntos. Es decir, para cada par de nodos (p, n), hay una arista e en E* cuya distanciade asociada es la longitud del camino simple más largo de p a n, siempre y cuando haya porlo menos un camino de p a n. E* se calcula para cada valor de T, el destino del intervalo deiniciación. También es posible realizar este cálculo sólo una vez con un valor simbólico de T ydespués sustituir para T en cada iteración, como hicimos en el ejemplo 10.20.El Algoritmo 10.20 utiliza el rastreo hacia atrás. Si no puede programar un SCC, trata dereprogramar todo el SCC un ciclo de reloj después. Estos intentos de programación continúanhasta T ciclos de reloj. El rastreo hacia atrás es importante ya que, como se muestra en elejemplo 10.20, la colocación del primer nodo en un SCC puede dictar por completo el programa de todos los demás nodos. Si el programa no se ajusta al programa creado hasta ahora, elintento falla.Para programar un SCC, el algoritmo determina el primer tiempo en el que pueda programarse cada nodo en el componente, que cumpla con las dependencias de datos transitivas enE*. Después elige el que tenga el tiempo de inicio más anticipado como el primer nodo paraprogramar. Después, el algoritmo invoca a SccProgramado para tratar de programar el componente en el tiempo inicial más anticipado. El algoritmo realiza cuando mucho T intentos contiempos iniciales cada vez mayores. Si falla, entonces el algoritmo prueba con otro intervalo deiniciación.El algoritmo SccProgramado se asemeja al Algoritmo 10.19, pero tiene tres diferencias importantes.1. El objetivo de SccProgramado es programar el componente fuertemente conectado en laranura de tiempo s dada. Si el primer nodo del componente no puede programarse en s,SccProgramado devuelve falso. La función main puede invocar a SccProgramado otra vezcon una ranura de tiempo posterior, si se desea.2. Los nodos en el componente fuertemente conectado se programan en orden topológico,con base en las aristas en E. Como las diferencias de iteración en todas las aristas enE son 0, estas aristas no cruzan ningún límite de iteración y no pueden formar ciclos.(Las aristas que cruzan los límites de iteración se conocen como acarreados por ciclo.)Sólo las dependencias acarreadas por ciclo colocan límites superiores en donde puedenprogramarse las operaciones. Así, este orden de programación, junto con la estrategiade programar cada operación lo antes posible, maximiza los rangos en los que puedenprogramarse los nodos siguientes.3. Para los componentes fuertemente conectados, las dependencias imponen un límite tantoinferior como superior en el rango en el que puede programarse un nodo. SccProgramadocalcula estos rangos y los utiliza para limitar más los intentos de programación.Ejemplo 10.22: Vamos a aplicar el Algoritmo 10.21 al grafo de dependencia de datos cíclicodel ejemplo 10.20. El algoritmo calcula primero que el límite sobre el intervalo de iniciaciónpara este ejemplo sea de 3 ciclos de reloj. Observamos que no es posible cumplir con estelímite inferior. Cuando el intervalo de iniciación T es de 3, las dependencias transitivas enla figura 10.28 dictan que S(d ) − S(b ) = 2. Al programar los nodos b y d dos ciclos de relojMaq. Cap_10_AHO.indd 756 11/10/07 1:05:53 AM10.5 Canalización por software 757aparte se producirá un conflicto en una tabla de reservación de recursos modular, con unalongitud de 3.Intento Intervalode iniciación Nodo Rango ProgramaReservaciónde recursosmodularFigura 10.30: Comportamiento del Algoritmo 10.21 en el ejemplo 10.20La figura 10.30 muestra cómo se comporta el Algoritmo 10.21 con este ejemplo. Primerotrata de encontrar un programa con un intervalo de iniciación de 3 ciclos de reloj. El intentoempieza por programar los nodos a y b lo antes posible. Sin embargo, una vez que el nodo bse coloca en el ciclo de reloj 2, el nodo c sólo puede colocarse en el ciclo 3, lo cual entra enconflicto con el uso de recursos del nodo a. Es decir, tanto a como c necesitan el primer recursoen los ciclos de reloj que tienen un residuo de 0 módulo 3.El algoritmo rastrea hacia atrás y trata de programar el componente fuertemente conectado{b, c, d} un ciclo de reloj más tarde. Esta vez el nodo b se programa en el ciclo 3, y el nodo c seprograma con éxito en el ciclo de reloj 4. Sin embargo, el nodo d no puede programarse en elMaq. Cap_10_AHO.indd 757 11/10/07 1:05:53 AM758 Capítulo 10. Paralelismo a nivel de instrucciónciclo de reloj 5. Es decir, tanto b como d necesitan el segundo recurso en los ciclos de relojque tienen un residuo de 0 módulo 3. Observe que es sólo una coincidencia que los dos conflictos descubiertos hasta ahora se encuentren en ciclos de reloj con un residuo de 0 módulo 3;el conflicto podría haber ocurrido en los ciclos de reloj con un residuo de 1 o 2 en otroejemplo.El algoritmo se repite retrasando el inicio del SCC {b, c,d} un ciclo más. Pero, como vimosantes, este SCC nunca puede programarse con un intervalo de iniciación de 3 ciclos de reloj, porlo que el intento está propenso a fallar. En este punto, el algoritmo se da por vencido y tratade encontrar un programa con un intervalo de iniciación de 4 ciclos de reloj. En un momentodado, el algoritmo encuentra el programa óptimo en su sexto intento. ✷10.5.9 Mejoras a los algoritmos de canalizaciónEl Algoritmo 10.21 es un algoritmo bastante simple, aunque se ha encontrado que funcionabien en los destinos de máquinas actuales. Los elementos importantes en este algoritmo son:1. El uso de una tabla de reservación de recursos modular para comprobar conflictos derecursos en el estado estable.2. La necesidad de calcular las relaciones de dependencia transitivas para encontrar el rangoválido en el que puede programarse un nodo, en presencia de los ciclos de dependencia.3. El rastreo hacia atrás es útil, y los nodos en ciclos críticos (ciclos que colocan el límiteinferior más alto en el intervalo de iniciación T) deben reprogramarse en conjunto, yaque no hay espacio entre ellos.Hay muchas formas de mejorar el Algoritmo 10.21. Por ejemplo, el algoritmo tarda ciertotiempo en darse cuenta de que un intervalo de iniciación de 3 ciclos de reloj no es factible parael ejemplo 10.22 simple. Podemos programar primero los componentes fuertemente conectadosde manera independiente, con el fin de determinar si el intervalo de iniciación es factible paracada componente.También podemos modificar el orden en el que se programan los nodos. El orden utilizado en elAlgoritmo 10.21 tiene algunas desventajas. En primer lugar, como los SCCs no triviales sonmás difíciles de programar, es conveniente programarlos primero. En segundo lugar, algunosde los registros pueden tener tiempos de vida innecesariamente largos. Es conveniente acercarmás las definiciones a los usos. Una posibilidad es programar primero los componentes fuertemente conectados con los ciclos críticos y después extender el programa en ambos extremos.10.5.10 Expansión modular de variablesSe dice que una variable escalar es privatizable en un ciclo si su rango de vida está dentro deuna iteración del ciclo. En otras palabras, una variable privatizable no debe vivir a la entrada oa la salda de cualquier iteración. Estas variables se llaman así debido a que los distintos procesadores que ejecutan distintas iteraciones en un ciclo pueden tener sus propias copias privadas,y, por lo tanto, no interfieren unas con otras.La expansión de variables se refiere a la transformación de convertir una variable escalarprivatizable en un arreglo, y hacer que la i-ésima iteración del ciclo lea y escriba el i-ésimoMaq. Cap_10_AHO.indd 758 11/10/07 1:05:55 AM10.5 Canalización por software 759elemento. Esta transformación elimina las restricciones antidependientes entre las lecturas enuna iteración y las escrituras en las iteraciones siguientes, así como las dependencias de salidaentre las escrituras de distintas iteraciones. Si se eliminan todas las dependencias acarreadaspor ciclo, todas las iteraciones en el ciclo pueden ejecutarse en paralelo.La eliminación de las dependencias acarreadas por ciclo, y por ende la eliminación de ciclosen el grafo de dependencia de datos, puede mejorar bastante la efectividad de la canalizaciónpor software. Como se ilustra mediante el ejemplo 10.15, no necesitamos expandir una variable privatizable por completo mediante el número de iteraciones en el ciclo. Sólo un pequeñonúmero de iteraciones puede estar ejecutándose a la vez, y las variables privatizables puedenestar vivas en forma simultánea, en un número aún más pequeño de iteraciones. Por ende,el mismo almacenamiento se reutiliza para guardar variables con tiempos de vida que no setraslapen. Dicho en forma más específica, si el tiempo de vida de un registro es de l ciclos, yel intervalo de iniciación es T, entonces sólo puede haber variables vivas en un puntodado. Podemos asignar q registros a la variable, en donde la variable en la i-ésima iteraciónutiliza el (i mod q )-ésimo registro. A esta transformación se le conoce como expansión modularde variables.Algoritmo 10.23: Canalización por software con expansión modular de variables.ENTRADA: Un grafo de dependencia de datos y una descripción de recursos de la máquina.SALIDA: Dos ciclos, uno canalizado por software y uno sin canalización.¿Hay alternativas para la heurística?Podemos formular el problema de encontrar al mismo tiempo un programa de canalización por software óptimo y la asignación de registros como un problema de programaciónlineal de enteros. Aunque muchos programas lineales enteros pueden resolverse con rapidez, algunos de ellos pueden requerir una cantidad de tiempo exorbitante. Para usar unsolucionador de programación lineal entera en un compilador, debemos tener la capacidadde abortar el procedimiento, si éste no se completa dentro de un límite preestablecido.Dicho método se ha probado en una máquina de destino (la SGI R8000) en forma empírica, y se descubrió que el solucionador podía encontrar la solución óptimapara un gran porcentaje de los programas en el experimento, dentro de un rango detiempo razonable. El resultado fue que los programas producidos mediante el uso de unmétodo heurístico también estuvieron cerca de lo óptimo. Los resultados sugieren que,por lo menos para esa máquina, no tiene sentido utilizar el método de programaciónlineal entera, en especial desde la perspectiva de la ingeniería de software. Como elsolucionador lineal entero tal vez no termine, aún es necesario implementar cierto tipode solucionador heurístico en el compilador. Una vez establecido dicho solucionadorheurístico, hay poco incentivo para implementar también un programador basado en lastécnicas de programación entera.Maq. Cap_10_AHO.indd 759 11/10/07 1:05:55 AM760 Capítulo 10. Paralelismo a nivel de instrucciónMÉTODO:1. Elimine las antidependencias acarreadas por ciclo y las dependencias de salida asociadascon las variables privatizables del grafo de dependencia de datos.2. Canalice por software el grafo de dependencia resultante, usando el Algoritmo 10.21.Haga que T sea el intervalo de iniciación para el cual se encuentra un programa, y queL sea la longitud del programa para una iteración.3. Del programa resultante, calcule qv , el mínimo número de registros necesarios para cadavariable privatizable v. Haga que Q = máxv q v .4. Genere dos ciclos: un ciclo canalizado por software y uno sin canalización. El ciclo canalizado por software tienecopias de las iteraciones, colocadas T ciclos aparte. Tiene un prólogo coninstrucciones, un estado estable con QT instrucciones, y un epílogo de L − T instrucciones. Inserte una instrucción de regreso de ciclo que bifurque desde la parte inferiordel estado estable hasta la parte superior del mismo.El número de registros asignados a la variable privatizable v essien cualquier otro casoLa variable v en la iteración i utiliza el (i mod q i)-ésimo registro asignado.Haga que n sea la variable que representa el número de iteraciones en el ciclo de origen.El ciclo canalizado por software se ejecuta siEl número de veces que se toma la bifurcación de regreso de ciclo esPor ende, el número de iteraciones de origen ejecutadas por el ciclo canalizado porsoftware es:en cualquier otro casosiEl número de iteraciones ejecutadas por el ciclo sin canalización es n 3 = n − n 2. ✷Maq. Cap_10_AHO.indd 760 11/10/07 1:05:56 AM10.5 Canalización por software 761Ejemplo 10.24: Para el ciclo canalizado por software de la figura 10.22, L = 8, T = 2 yQ = 2. El ciclo canalizado por software tiene 7 copias de las iteraciones, en donde el prólogo,el estado estable y el epílogo tienen 6, 4 y 6 instrucciones, respectivamente. Haga que n sea elnúmero de iteraciones en el ciclo de origen. El ciclo canalizado por software se ejecuta si n ¦ 5,en cuyo caso la bifurcación de regreso de ciclo se tomaveces, y el ciclo canalizado por software es responsable dede las iteraciones en el ciclo de origen. ✷La expansión modular incrementa el tamaño del estado estable por un factor de Q. A pesarde este incremento, el código generado por el Algoritmo 10.23 sigue siendo bastante compacto.En el peor de los casos, el ciclo canalizado por software requeriría hasta tres veces más instrucciones que el del programa para una iteración. Aproximadamente, junto con el ciclo adicionalgenerado para manejar las iteraciones restantes, el tamaño total del código es de casi 4 vecesel del original. Por lo general, esta técnica se aplica a los ciclos internos estrechos, por lo queeste incremento es razonable.El Algoritmo 10.23 minimiza la expansión de código a expensas de usar más registros. Podemos reducir el uso de registros al generar más código. Podemos usar el mínimo de qv registrospara cada variable v si utilizamos un estado estable conT × LCMvq vinstrucciones. Aquí, LCMv representa la operación de tomar el mínimo múltiplo común detodas las qv, a medida que v varía sobre todas las variables privatizables (es decir, el menorentero que sea un múltiplo entero de todas las qv). Por desgracia, el mínimo múltiplo comúnpuede ser bastante grande, incluso para unas cuantas qv pequeñas.10.5.11 Instrucciones condicionalesSi las instrucciones predicadas están disponibles, podemos convertir las instrucciones dependientes de control en predicadas. Las instrucciones predicadas pueden canalizarse por software,al igual que cualquier otra operación. No obstante, si hay una gran cantidad de flujo de controldependiente de datos dentro del cuerpo del ciclo, tal vez sean más apropiadas las técnicas deprogramación descritas en la sección 10.4.Si una máquina no tiene instrucciones predicadas, podemos usar el concepto de reducciónjerárquica, descrito a continuación, para manejar una pequeña cantidad de flujo de control dependiente de datos. Al igual que el Algoritmo 10.11, en la reducción jerárquica las construcciones de control en el ciclo se programan de adentro hacia fuera, empezando con las estructurasMaq. Cap_10_AHO.indd 761 11/10/07 1:05:57 AM762 Capítulo 10. Paralelismo a nivel de instrucciónmás profundamente anidadas. A medida que se programa cada construcción, toda la construcción se reduce a un solo nodo, el cual representa todas las restricciones de programación desus componentes con respecto a las otras partes del programa. Así, este nodo se puede programar como si fuera un nodo simple dentro de la construcción de control circundante. El procesode programación está completo cuando todo el programa se reduce a un solo nodo.En el caso de una instrucción condicional con bifurcaciones “then” y “else”, programamoscada una de las bifurcaciones de manera independiente. Entonces:1. Las restricciones de toda la instrucción condicional se consideran, en forma conservadora, como la unión de las restricciones de ambas bifurcaciones.2. Su utilización de los recursos es el máximo de los recursos utilizados en cada bifurcación.3. Sus restricciones de precedencia son la unión de las de cada bifurcación, que se obtienenpretendiendo que ambas bifurcaciones se ejecutan.Por ende, este nodo puede programarse como cualquier otro. Se generan dos conjuntos de código, que corresponden a las dos bifurcaciones. Cualquier código programado en paralelo con lainstrucción condicional se duplica en ambas bifurcaciones. Si se traslapan varias instruccionescondicionales, debe generarse un código separado para cada combinación de bifurcaciones ejecutadas en paralelo.10.5.12 Soporte de hardware para la canalización por softwareSe ha propuesto el soporte para hardware especializado para minimizar el tamaño del códigocanalizado por software. El archivo de registro giratorio en la arquitectura Itanium es uno deesos ejemplos. Un archivo de registro giratorio tiene un registro base, que se agrega al númerode registro especificado en el código para derivar el registro actual al que se va a acceder. Podemos obtener distintas iteraciones en un ciclo para usar distintos registros, con sólo cambiar elcontenido del registro base en el límite de cada iteración. La arquitectura Itanium tambiéntiene un soporte extenso para las instrucciones predicadas. La predicación no sólo puede usarsepara convertir la dependencia de control a la dependencia de datos, sino también para evitargenerar los prólogos y epílogos. El cuerpo de un ciclo canalizado por software contiene un superconjunto de las instrucciones emitidas en el prólogo y epílogo. Podemos tan sólo generar elcódigo para el estado estable y la predicación de manera apropiada, con el fin de eliminar lasoperaciones adicionales para obtener los efectos de tener un prólogo y un epílogo.Aunque el soporte de hardware del Itanium mejora la densidad del código canalizado porsoftware, también debemos tener en cuenta que el soporte no es económico. Como la canalización por software es una técnica orientada a los ciclos estrechos más internos, los ciclos canalizados tienden a ser pequeños de todas formas. El soporte especializado para la canalizaciónpor software se garantiza sobre todo para las máquinas destinadas a ejecutar muchos cicloscanalizados por software, y en situaciones en las que es muy importante disminuir al mínimoel tamaño del código.Maq. Cap_10_AHO.indd 762 11/10/07 1:05:58 AM10.5 Canalización por software 763Figura 10.31: Código máquina para el ejercicio 10.5.210.5.13 Ejercicios para la sección 10.5Ejercicio 10.5.1: En el ejemplo 10.20 mostramos cómo establecer los límites en los ciclos dereloj relativos en los cuales se programan b y c. Calcule los límites para cada uno de los otroscinco pares de nodos (i) para T general (ii) para T = 3 (iii) para T = 4.Ejercicio 10.5.2: En la figura 10.31 está el cuerpo de un ciclo. Las direcciones como a(R9)están diseñadas para ser ubicaciones de memoria, en donde a es una constante, y R9 es el registro que cuenta las iteraciones a través del ciclo. Puede suponer que cada iteración del cicloaccede a distintas ubicaciones, ya que R9 tiene un valor distinto. Usando el modelo de máquinadel ejemplo 10.12, programe el ciclo de la figura 10.31 de las siguientes maneras: a) Manteniendo cada iteración lo más estrecha posible (es decir, sólo introduzca una nopdespués de cada operación aritmética), desenrolle el ciclo dos veces. Programe la segunda iteración para que comience lo más pronto posible, sin violar la restricción de que lamáquina sólo puede realizar una carga, un almacenamiento, una operación aritmética,y una bifurcación en cualquier ciclo de reloj. b) Repita la parte (a), pero desenrolle el ciclo tres veces. De nuevo, empiece cada iteraciónlo más pronto que pueda, sujeta a las restricciones de la máquina. ! c) Construya código completamente canalizado, sujeto a las restricciones de la máquina.En esta parte, puede introducir instrucciones nop si es necesario, pero debe empezar unanueva iteración cada dos ciclos de reloj.Ejercicio 10.5.3: Cierto ciclo requiere 5 cargas, 7 almacenamientos y 8 operaciones aritméticas. ¿Cuál es el intervalo de iniciación mínimo para una canalización por software de este ciclo,en una máquina que ejecuta cada operación en un ciclo de reloj, y tiene suficientes recursospara hacerlo, en un ciclo de reloj?:a) 3 cargas, 4 almacenamientos y 5 operaciones aritméticas.b) 3 cargas, 3 almacenamientos y 3 operaciones aritméticas.Maq. Cap_10_AHO.indd 763 11/10/07 1:05:58 AM764 Capítulo 10. Paralelismo a nivel de instrucciónEjercicio 10.5.4: Usando el modelo de máquina del ejemplo 10.12, encuentre el intervalo deiniciación mínimo y un programa uniforme para las iteraciones, para el siguiente ciclo: for (i = 1; 1 < n; i++) { A[i] = B[i−1] + 1; B[i] = A[i−1] + 2;}Recuerde que la cuenta de iteraciones se maneja mediante el autoincremento de los registros,y no se necesitan operaciones que sean sólo para el conteo asociado con el ciclo for.Ejercicio 10.5.5: Demuestre que el Algoritmo 10.19, en el caso especial en donde cada operación requiere sólo una unidad de un recurso, siempre puede encontrar un programa de canalización por software que cumpla con el límite inferior.Ejercicio 10.5.6: Suponga que tenemos un grafo de dependencia de datos cíclico con losnodos a, b, c y d. Hay flancos que van de a hacia b y de c hacia d con la etiqueta 0, 1, y hayflancos que van de b hacia c y de d hacia a con la etiqueta 1, 1. No hay otros flancos.a) Dibuje el grafo de dependencia cíclico.b) Calcule la tabla de caminos simples más largos entre los nodos.c) Muestre las longitudes de los caminos simples más largos, si el intervalo de iniciación Tes 2.d) Repita (c) si T = 3.e) Para T = 3, ¿cuáles son las restricciones en los tiempos relativos en los que puede programarse cada una de las instrucciones representadas por a, b, c y d?Ejercicio 10.5.7: Proporcione un algoritmo O(n)3 para encontrar la longitud del caminosimple más largo en un grafo con n nodos, suponiendo que ningún ciclo tenga una longitud positiva. Sugerencia: Adapte el algoritmo de Floyd para los caminos más cortos (vea, por ejemplo,A. V. Aho y J. D. Ullman, Foundations of Computer Science, Computer Science Press, NuevaYork, 1992).Ejercicio 10.5.8: Suponga que tenemos una máquina con tres tipos de instrucciones, a lascuales llamaremos A, B y C. Todas las instrucciones requieren un ciclo de reloj, y la máquinapuede ejecutar una instrucción de cada tipo en cada ciclo de reloj. Suponga que un ciclo consisteen seis instrucciones, dos de cada tipo. Entonces, es posible ejecutar el ciclo en una canalizaciónpor software con un intervalo de iniciación de dos. No obstante, algunas secuencias de las seisinstrucciones requieren la inserción de un retraso, y algunas requieren la inserción de dos retrasos. De las 90 secuencias posibles de dos As, dos Bs y dos Cs, ¿cuántas no requieren retraso?¿Cuántas requieren un retraso? Sugerencia: Hay simetría entre los tres tipos de instrucciones,por lo que dos secuencias que pueden transformarse entre sí mediante la permutación de losnombres A, B y C deben requerir el mismo número de retrasos. Por ejemplo, ABBCAC debeser igual que BCCABA.!!!!!!Maq. Cap_10_AHO.indd 764 11/10/07 1:05:59 AM10.6 Resumen del capítulo 10♦ Cuestiones de arquitectura: La programación de código optimizado aprovecha las características de las arquitecturas de las computadoras modernas. A menudo, dichas máquinas permiten la ejecución canalizada, en donde varias instrucciones están en distintasetapas de ejecución al mismo tiempo. Algunas máquinas también permiten que varias instrucciones empiecen a ejecutarse al mismo tiempo.♦ Dependencias de datos: Al programar instrucciones, debemos tener en cuenta el efecto quelas instrucciones tienen sobre cada ubicación de memoria y registro. Las verdaderas dependencias de datos ocurren cuando una instrucción debe leer una ubicación, después de queotra ha escrito en ella. Las antidependencias ocurren cuando hay una escritura después deuna lectura, y las dependencias de salida ocurren cuando hay dos escrituras en la mismaubicación.♦ Eliminación de dependencias: Al utilizar ubicaciones adicionales para almacenar los datos, se pueden eliminar las antidependencias y las dependencias de salida. Sólo las verdaderas dependencias no pueden eliminarse y deben sin duda respetarse cuando el códigose programa.♦ Grafos de dependencia de datos para bloques básicos: Estos grafos representan las restricciones de sincronización entre las instrucciones de un bloque básico. Los nodoscorresponden a las instrucciones. Un flanco de n a m etiquetado como d indica que lainstrucción m debe empezar por lo menos d ciclos de reloj después de que empiezala instrucción n.♦ Órdenes topológicos priorizados: El grafo de dependencia de datos para un bloque básicosiempre es acíclico y, por lo general, hay muchos órdenes topológicos consistentes con elgrafo. Se puede utilizar una de varias heurísticas para seleccionar un orden topológicopreferido para un grafo dado; por ejemplo, elegir primero los nodos con el camino crítico más largo.♦ Programación por lista: Dado un orden topológico priorizado para un grafo de dependencia de datos, podemos considerar los nodos en ese orden. Programe cada nodo en el ciclode reloj más anticipado que sea consistente con las restricciones de tiempo implicadas porlas aristas del grafo, los programas de todos los nodos programados con anterioridad, y lasrestricciones de recurso de la máquina.♦ Movimiento de código entre bloques: Bajo ciertas circunstancias, es posible mover instrucciones del bloque en el que aparecen, a un bloque predecesor o sucesor. La ventaja es quepuede haber oportunidades para ejecutar instrucciones en paralelo en la nueva ubicación,que no existen en la ubicación original. Si no hay una relación de predominio entre lasubicaciones antigua y nueva, tal vez sea necesario insertar código de compensación a lolargo de ciertas rutas, para poder asegurar que se ejecute exactamente la misma secuencia de instrucciones, sin importar el flujo del control.♦ Ciclos de ejecución total (do-all): Un ciclo de ejecución total no tiene dependencias entreiteraciones, por lo que cualquier iteración puede ejecutarse en paralelo.10.6 Resumen del capítulo 10 765Maq. Cap_10_AHO.indd 765 11/10/07 1:06:00 AM766 Capítulo 10. Paralelismo a nivel de instrucción♦ Canalización por software de los ciclos de ejecución total: La canalización por softwarees una técnica que explota la habilidad de una máquina para ejecutar varias instruccionesa la vez. Programamos las iteraciones del ciclo para empezar en pequeños intervalos, talvez colocando instrucciones no-op en las iteraciones para evitar conflictos entre iteraciones de los recursos de la máquina. El resultado es que el ciclo puede ejecutarse conrapidez, con un preámbulo, un coda, y (por lo general) un ciclo interno diminuto.♦ Ciclos de ejecución cruzada: La mayoría de los ciclos tienen dependencias de datos quevan de cada iteración a iteraciones posteriores. A éstos se les conoce como ciclos de ejecución cruzada.♦ Grafos de dependencia de datos para ciclos de ejecución cruzada: Para representar las dependencias entre instrucciones de un ciclo de ejecución cruzada, se requiere que las aristasse etiqueten mediante un par de valores: el retraso requerido (como para los grafos querepresentan bloques básicos) y el número de iteraciones que transcurren entre las dos instrucciones que tienen una dependencia.♦ Programación por lista de los ciclos: Para programar un ciclo, debemos elegir el programapara todas las iteraciones, y también elegir el intervalo de iniciación en el cual deben comenzar las iteraciones sucesivas. El algoritmo implica derivar las restricciones en los programas relativos de las diversas instrucciones en el ciclo, buscando la longitud de las rutasacíclicas más largas entre los dos nodos. Estas longitudes tienen el intervalo de iniciacióncomo un parámetro, y por ende imponen un límite menor en el intervalo de iniciación.10.7 Referencias para el capítulo 10Para una discusión más detallada sobre la arquitectura y el diseño de procesadores, recomendamos a Hennessy y Patterson [5].El concepto de la dependencia de datos se discutió por primera vez en Kuck, Muraoka yChen [6], y en Lamport [8] dentro del contexto de la compilación de código para multiprocesadores y máquinas de vectores.La programación de instrucciones se utilizó por primera vez en la programación de microcódigo horizontal ([2, 3, 11 y 12]). El trabajo de Fisher en relación con la compactación de microcódigo lo llevó a proponer el concepto de una máquina VLIW, en donde los compiladores puedencontrolar directamente la ejecución paralela de las operaciones [3]. Gross y Hennesy [4] utilizaronla programación de instrucciones para manejar las bifurcaciones retrasadas en el primer conjunto de instrucciones MIPS RISC. El algoritmo de este capítulo se basa en el tratamiento más general de Bernstein y Rodeh [1] de la programación de operaciones para máquinas con paralelismoa nivel de instrucción.Patel y Davidson [9] desarrollaron por primera vez la idea básica de la canalización por software, para programar canalizaciones por hardware. Rau y Glaeser [10] utilizaron por primeravez la canalización por software para compilar para una máquina con hardware especializado,diseñado para soportar la canalización por software. El algoritmo aquí descrito se basa en Lam [7],que no supone ningún soporte de hardware especializado.Maq. Cap_10_AHO.indd 766 11/10/07 1:06:00 AM 1. Bernstein, D. y M. Rodeh, “Global instruction scheduling for superscalar machines”,Proc. ACM SIGPLAN 1991 Conference on Programming Language Design and Implementation, pp. 241-255. 2. Dasgupta, S., “The organization of microprogram stores”, Computing Surveys 11:1(1979), pp. 39-65. 3. Fisher, J.A., “Trace scheduling: a technique for global microcode compaction”, IEEETrans. On Computers C-30:7 (1981), pp. 478-490. 4. Gross, T. R. y Hennessy, J. L., “Optimizing delayed branches”, Proc. 15th Annual Workshop on Microprogramming (1982), pp. 114-120. 5. Hennessy, J. L. y D. A. Patterson, Computer Architecture: A Quantitative Approach,Tercera Edición, Morgan Kaufman, San Francisco, 2003. 6. Kuck, D., Y. Muraoka y S. Chen, “On the number of operations simultaneously executable in Fortran-like programs and their resulting speedup”, IEEE Transactions onComputers C-21:12 (1972), pp. 1293-1310. 7. Lam, M. S., “Software pipelining: an effective scheduling technique for VLIW machines”, Proc. ACM SIGPLAN 1988 Conference on Programming Language Design andImplementation, pp. 318-328. 8. Lamport, L., “The parallel execution of DO loops”, Comm. ACM 17:2 (1974), pp. 83-93. 9. Patel, J. H. y E. S. Davidson, “Improving the throughput of a pipeline by insertion ofdelays”, Proc. Third Annual Symposium on Computer Architecture (1976), pp. 159-164.10. Rau, B. R. y C. D. Glaeser, “Some scheduling techniques and an easily schedulablehorizontal architecture for high performance scientific computing”, Proc. 14th AnnualWorkshop on Microprogramming (1981), pp. 183-198.11. Tokoro, M., E. Tamura y T. Takizuka, “Optimization of microprograms”, IEEE Trans.on Computers C-30:7 (1981), pp. 491-504.12. Word, G., “Global optimization of microprograms through modular control constructs”,Proc. 12th Annual Workshop in Microprogramming (1979), pp. 1-6.10.7 Referencias para el capítulo 10 767Maq. Cap_10_AHO.indd 767 11/10/07 1:06:01 AMMaq. Cap_10_AHO.indd 768 11/10/07 1:06:01 AMCapítulo 11Optimización para el paralelismoy la localidad769Este capítulo muestra cómo un compilador puede mejorar el paralelismo y la localidad en losprogramas con un uso intensivo de cálculos, que involucren arreglos para agilizar los programasdestino que se ejecuten en sistemas con multiprocesadores. Muchas aplicaciones científicas, deingeniería y comerciales tienen una gran necesidad de ciclos computacionales. Entre algunosejemplos se encuentran el pronóstico del clima, el plegamiento de proteínas para diseñar fármacos, la dinámica de fluidos para diseñar sistemas de autopropulsión, y la cromodinámicacuántica para estudiar las interacciones fuertes en la física de alta energía.Una manera de agilizar un cálculo es utilizar el paralelismo. Por desgracia, no es fácil desarrollar software que pueda aprovechar las máquinas paralelas. Ya es bastante difícil dividir elcómputo en unidades que puedan ejecutarse en distintos procesadores en paralelo; ni siquieraeso garantiza un aumento en la velocidad. Además, debemos minimizar la comunicación entreprocesadores, ya que ¡es muy fácil que la sobrecarga de comunicación haga que el código enparalelo se ejecute aún con más lentitud que la ejecución secuencial!El proceso de minimizar la comunicación puede considerarse como un caso especial demejorar la localidad de los datos de un programa. En general, decimos que un programa tienebuena localidad de los datos si un procesador accede con frecuencia a los mismos datos queha utilizado recientemente. Sin duda, si un procesador en una máquina paralela tiene buenalocalidad, no necesita comunicarse con otros procesadores con frecuencia. Por ende, debemosconsiderar el paralelismo y la localidad de los datos, juntos. La localidad de los datos, por sísola, es también importante para el rendimiento de los procesadores individuales. Los procesadores modernos tienen uno o más niveles de cachés en la jerarquía de memoria; un accesoa memoria puede requerir cientos de ciclos de máquina, mientras que una coincidencia en lacaché sólo requeriría unos cuantos ciclos. Si un programa no tiene una buena localidad de datosy se producen fallas en la caché con frecuencia, su rendimiento disminuirá.Otra razón por la que el paralelismo y la localidad se tratan en conjunto en este mismo capítulo es que comparten la misma teoría. Si sabemos cómo optimizar la localidad de los datos,sabemos en dónde se encuentra el paralelismo. En este capítulo verá que el modelo de programaMaq. Cap_11_AHO A.indd 769 11/10/07 8:18:50 PM770 Capítulo 11. Optimización para el paralelismo y la localidadque utilizamos para el análisis de flujo de datos en el capítulo 9 es inadecuado para la paralelización y la optimización de la localidad. La razón es que el trabajo en el análisis del flujo dedatos supone que no hay diferencias entre las formas en que se llega a una instrucción dada, yen realidad estas técnicas del capítulo 9 aprovechan el hecho de que no hacemos diferencia entrelas distintas ejecuciones de la misma instrucción, por ejemplo, en un ciclo. Para paralelizar uncódigo, debemos razonar acerca de las dependencias entre las distintas ejecuciones dinámicasde la misma instrucción, para determinar si pueden ejecutarse en diferentes procesadores almismo tiempo.Este capítulo se enfoca en las técnicas para optimizar la clase de aplicaciones numéricasque utilizan arreglos como estructuras de datos, y para acceder a ellos con patrones regulares simples. Dicho en forma más específica, estudiaremos los programas que tienen accesos aarreglos afines con respecto a los índices de los ciclos circundantes. Por ejemplo, si i y j sonlas variables índice de los ciclos circundantes, entonces Z[i][j] y Z[i][i + j] son accesos afines.Una función de una o más variables, i1, i 2,…, i n es afín si puede expresarse como una suma deuna constante, más los múltiplos constantes de las variables, por decir, c 0 + c1x 1 + c2x2 + …+ c nx n, en donde c0, c1,…, c n son constantes. Por lo general, las funciones afines se conocencomo funciones lineales, aunque hablando en sentido estricto, las funciones lineales no tienen eltérmino c0.He aquí un ejemplo simple de un ciclo en este dominio:for (i = 0; i < 10; i++) { Z[i] = 0;}Como las iteraciones del ciclo se escriben en distintas ubicaciones, distintos procesadores puedenejecutar distintas iteraciones en forma concurrente. Por otro lado, si hay otra instrucción Z[j]= 1 en ejecución, tenemos que preocuparnos acerca de si i podría en algún momento ser igualque j, y de ser así, en qué orden debemos ejecutar esas instancias de las dos instrucciones quecomparten un valor común del índice del arreglo.Es importante saber qué iteraciones pueden hacer referencia a la misma ubicación de memoria. Este conocimiento nos permite especificar las dependencias de datos que deben respetarse al programar código, tanto para uniprocesadores como para multiprocesadores. Nuestroobjetivo es encontrar un programa que respete todas las dependencias de datos, de tal formaque las operaciones que acceden a la misma ubicación y líneas de caché se ejecuten cerca unade la otra si es posible, y en el mismo procesador, en el caso de los multiprocesadores.La teoría que presentamos en este capítulo tiene sus bases en el álgebra lineal y las técnicas deprogramación entera. Modelamos las iteraciones en un anidamiento de ciclos de n niveles comoun poliedro n-dimensional, cuyos límites se especifican mediante los límites de los ciclos en elcódigo. Hay funciones afines que asignan cada iteración a las ubicaciones del arreglo a las queaccede. Podemos usar la programación lineal entera para determinar si existen dos iteracionesque puedan hacer referencia a la misma ubicación.El conjunto de transformaciones de código que veremos aquí se pueden clasificar en doscategorías: particionamiento afín y bloqueo. El particionamiento afín divide los poliedros deiteraciones en componentes, para ejecutarse ya sea en distintas máquinas o una por una, enMaq. Cap_11_AHO A.indd 770 11/10/07 8:18:53 PMsecuencia. Por otro lado, el bloqueo crea una jerarquía de iteraciones. Suponga que recibimos unciclo que recorre un arreglo fila por fila. En vez de ello, podemos subdividir el arreglo en bloquesy visitar todos los elementos en un bloque antes de avanzar al siguiente. El código resultanteconsistirá en ciclos externos que recorren los bloques, y después ciclos internos para recorrer loselementos dentro de cada bloque. Se utilizan técnicas del álgebra lineal para determinar tantolas mejores particiones afines, como los mejores esquemas de bloqueo.A continuación, primero empezaremos con una visión general de los conceptos en la computación paralela y la optimización de la localidad en la sección 11.1. Después, la sección 11.2 esun ejemplo concreto extendido (multiplicación de matrices) que muestra cómo las transformaciones de ciclos que reordenan el cómputo dentro de un ciclo pueden mejorar tanto la localidadcomo la efectividad de la paralelización.Las secciones 11.3 a 11.6 presentan la información preliminar necesaria para las transformaciones de los ciclos. La sección 11.3 muestra cómo modelamos las iteraciones individuales enun anidamiento de ciclos; la sección 11.4 muestra cómo modelamos las funciones de los índicesde arreglos que asignan cada iteración del ciclo a las ubicaciones de arreglo a las que accede laiteración; La sección 11.5 muestra cómo determinar qué iteraciones en un ciclo hacen referenciaa la misma ubicación en el arreglo o la misma línea de caché, usando algoritmos estándar deálgebra lineal; y la sección 11.6 muestra cómo encontrar todas las dependencias de datos entrelas referencias a un arreglo en un programa.El resto del capítulo aplica estas preliminares para obtener las optimizaciones. La sección11.7 primero analiza el problema más simple de buscar paralelismo que no requiera sincronización. Para encontrar el mejor particionamiento afín, sólo buscamos la solución a la restricciónde que las operaciones que comparten una dependencia de datos deben asignarse al mismoprocesador.Pocos programas pueden paralelizarse sin requerir sincronización. Por lo tanto, en las secciones 11.8 a 11.9.9 consideramos el caso general de buscar paralelismo que requiera sincronización.Presentamos el concepto de las canalizaciones, mostramos cómo buscar el particionamiento afínque maximice el grado de canalización permitido por un programa. Mostramos cómo optimizarla localidad en la sección 11.10. Por último, hablaremos sobre cómo las transformaciones afinesson útiles para optimizar otras formas de paralelismo.11.1 Conceptos básicosEsta sección presenta los conceptos básicos relacionados con la paralelización y la optimizaciónde la localidad. Si las operaciones pueden ejecutarse en paralelo, también pueden reordenarsepara otros objetivos, como la localidad. Por el contrario, si las dependencias de datos en unprograma dictan que las instrucciones en un programa deben ejecutarse en serie, entonces esobvio que no hay paralelismo, ni la oportunidad de reordenar las instrucciones para mejorar lalocalidad. Por ende, el análisis de la paralelización también busca las oportunidades disponiblespara el movimiento de código, para mejorar la localidad de los datos.Para disminuir al mínimo la comunicación en el código en paralelo, agrupamos todas las operaciones relacionadas y las asignamos al mismo procesador. El código resultante debe, por lo tanto, tener localidad de los datos. Un método primitivo para obtener una buena localidad de los datos en ununiprocesador es hacer que el procesador ejecute el código asignado a cada procesador en sucesión.11.1 Conceptos básicos 771Maq. Cap_11_AHO A.indd 771 11/10/07 8:18:54 PM772 Capítulo 11. Optimización para el paralelismo y la localidadEn esta introducción, empezaremos con un panorama general sobre las arquitecturas decomputadora en paralelo. Después mostraremos los conceptos básicos de la paralelización, eltipo de transformaciones que pueden hacer una gran diferencia, así como los conceptos útilespara la paralelización. Más tarde hablaremos sobre cómo las consideraciones similares puedenusarse para optimizar la localidad. Por último, presentaremos de manera informal los conceptosmatemáticos que usamos en este capítulo.11.1.1 MultiprocesadoresLa arquitectura de máquina paralela más popular es el multiprocesador simétrico (SymmetricMultiprocessor, SMP). A menudo, las computadoras personales de alto rendimiento tienendos procesadores, y muchas máquinas servidor tienen cuatro, ocho e incluso hasta decenas deprocesadores. Además, como ya es posible acomodar varios procesadores de alto rendimientoen un solo chip, los multiprocesadores han empezado a tener mucha popularidad.Los procesadores en un multiprocesador simétrico comparten el mismo espacio de direcciones. Para comunicarse, un procesador sólo necesita escribir en una ubicación de memoria,que después otro procesador lee. Los multiprocesadores simétricos se llaman así debido a quetodos los procesadores pueden acceder a toda la memoria en el sistema con un tiempo deacceso uniforme. La figura 11.1 muestra la arquitectura de alto nivel de un multiprocesador.Los procesadores pueden tener su propia caché de primer nivel, de segundo nivel y, en algunoscasos, hasta de tercer nivel. Las cachés de más alto nivel se conectan a la memoria física, porlo general, a través de un bus compartido.Procesador Procesador Procesador ProcesadorCaché deprimer nivelCaché deprimer nivelCaché deprimer nivelCaché deprimer nivelMemoriaBusCaché desegundo nivelCaché desegundo nivelCaché desegundo nivelCaché desegundo nivelFigura 11.1: La arquitectura del multiprocesador simétricoLos multiprocesadores simétricos utilizan un protocolo de caché coherente para ocultar lapresencia de cachés al programador. Bajo dicho protocolo, se permite a varios procesadoresMaq. Cap_11_AHO A.indd 772 11/10/07 8:18:54 PMmantener copias de la misma línea1 de caché al mismo tiempo, siempre y cuando sólo esténleyendo los datos. Cuando un procesador desea escribir en una línea de caché, se eliminan lascopias de todas las demás cachés. Cuando un procesador solicita datos que no encuentra en sucaché, la solicitud pasa al bus compartido, y los datos se obtienen ya sea de la memoria o dela caché de otro procesador.El tiempo que requiere un procesador para comunicarse con otro es de aproximadamenteel doble del costo de un acceso a memoria. Los datos, en unidades de líneas de caché, primerodeben escribirse desde la caché del primer procesador a la memoria, y después se obtienen dela memoria a la caché del segundo procesador. Tal vez piense que la comunicación entre procesadores es muy económica, ya que sólo es aproximadamente el doble de lenta que un accesoa memoria. Sin embargo, debe recordar que los accesos a memoria son muy costosos cuandose les compara con las ocurrencias en la caché; pueden ser cientos de veces más lentos. Esteanálisis demuestra con claridad la similitud entre la paralelización eficiente y el análisis de lalocalidad. Para que un procesador trabaje bien, ya sea por su cuenta o dentro del contexto deun multiprocesador, debe encontrar en su caché la mayoría de los datos sobre los que opera.A principios de la década del 2000, el diseño de los multiprocesadores simétricos ya no escalaba más allá de varias decenas de procesadores, debido a que el bus compartido, o cualquierotro tipo de interconexión relacionada, no podía operar a la par con el número cada vez mayorde procesadores. Para hacer que los diseños de los procesadores fueran escalables, los arquitectos introdujeron otro nivel más en la jerarquía de memoria. En vez de tener memoria que deigual forma está alejada de cada procesador, distribuyeron las memorias de manera que cadaprocesador pudiera acceder a su memoria local con rapidez, como se muestra en la figura 11.2.Por ende, las memorias remotas constituyeron el siguiente nivel de la jerarquía de memoria;son en conjunto más grandes, pero también requieren más tiempo para su acceso. De maneraanáloga al principio en el diseño de jerarquías de memoria, que establece que las operaciones dealmacenamiento rápidas son necesariamente pequeñas, las máquinas que soportan la comunicación rápida entre procesadores tienen necesariamente un pequeño número de procesadores.Existen dos variantes de una máquina paralela con memorias distribuidas: las máquinasNUMA (Nonuniform memory access, acceso a memoria no uniforme) y las máquinas que pasan mensajes. Las arquitecturas NUMA proporcionan un espacio de direcciones compartido alsoftware, lo cual permite a los procesadores comunicarse mediante la lectura y la escritura dela memoria compartida. Sin embargo, en las máquinas que pasan mensajes, los procesadorestienen espacios de direcciones separados, y se comunican enviándose mensajes unos a otros.Observe que, aun cuando es más simple escribir código para las máquinas con memoria compartida, el software debe tener una buena localidad para cualquier tipo de máquina, para podertrabajar bien.11.1.2 Paralelismo en las aplicacionesUtilizamos dos métricas de alto nivel para estimar qué tan bueno será el rendimiento de unaaplicación en paralelo: la cobertura paralela, que representa el porcentaje del cómputo que seejecuta en paralelo, y la granularidad del paralelismo, que es la cantidad de cómputo que cadaprocesador puede ejecutar sin sincronizarse o comunicarse con otros. Un objetivo bastante1 Tal vez desee repasar la explicación sobre las cachés y las líneas de caché en la sección 7.4.11.1 Conceptos básicos 773Maq. Cap_11_AHO A.indd 773 11/10/07 8:18:55 PM774 Capítulo 11. Optimización para el paralelismo y la localidadProcesador Procesador Procesador ProcesadorCaché deprimer nivelCaché deprimer nivelCaché deprimer nivelCaché deprimer nivelMemoria Memoria Memoria MemoriaCaché desegundo nivelCaché desegundo nivelCaché desegundo nivelCaché desegundo nivelBus o cualquier otra interconexiónFigura 11.2: Máquinas de memoria distribuidasatractivo de la paralelización son los ciclos: un ciclo puede tener muchas iteraciones, y si sonindependientes unas de otras, hemos encontrado una gran fuente de paralelismo.Ley de AmdahlLa importancia de la cobertura del paralelismo se muestra en forma sintetizada mediante laLey de Amdahl. La ley de Amdahl establece que, si f es la fracción del código paralelizado, y sila versión paralelizada se ejecuta en una máquina con p procesadores sin sobrecarga de comunicación o paralelización, el aumento en velocidad es:Por ejemplo, si la mitad del cómputo es secuencial, el cómputo sólo puede aumentar su velocidad al doble, sin importar cuántos procesadores utilicemos. El aumento en velocidad que puedelograrse es un factor de 1.6 si tenemos 4 procesadores. Incluso si la cobertura del paralelismoes del 90%, obtenemos cuando mucho un factor de aumento de 3 en 4 procesadores, y un factorde 10 en un número ilimitado de procesadores.Granularidad del paralelismoEs ideal si todo el cómputo de una aplicación puede particionarse en muchas tareas independientesmenos específicas, ya que podemos tan sólo asignar las diferentes tareas a distintos procesadores.Uno de esos ejemplos es el proyecto SETI (Search for extra-terrestrial Intelligence, Búsquedade inteligencia extraterrestre), el cual es el experimento que utiliza computadoras personalesconectadas a través de Internet para analizar distintas porciones de datos de radiotelescopios enMaq. Cap_11_AHO A.indd 774 11/10/07 8:18:55 PMparalelo. Cada unidad de trabajo, que requiere sólo una pequeña cantidad de datos de entraday genera una pequeña cantidad de datos de salida, puede ejecutarse de manera independiente alas demás. Como resultado, dichos cálculos se ejecutan bien en máquinas através de Internet, quetiene una latencia de comunicación (retraso) relativamente alta y un ancho de banda bajo.La mayoría de las aplicaciones requieren más comunicación e interacción entre los procesadores, y aún así permiten un paralelismo menos específico. Por ejemplo, consideremos el servidorWeb responsable de atender un gran número de solicitudes, en su mayoría independientes, usando una base de datos común. Podemos ejecutar la aplicación en un multiprocesador, con un hiloque implemente la base de datos y varios hilos más que atiendan las solicitudes de los usuarios.Otros ejemplos incluyen el diseño de medicamentos o la simulación de perfiles aerodinámicos, endonde los resultados de muchos parámetros distintos pueden evaluarse en forma independiente.Algunas veces, hasta la evaluación de sólo un conjunto de parámetros en una simulación tardatanto tiempo, que es conveniente aumentar su velocidad mediante la paralelización. A medida que se reduce la granularidad del paralelismo disponible en una aplicación, se requiere un mejorsoporte para las comunicaciones entre los procesadores y un mayor esfuerzo de programación.Muchas aplicaciones científicas y de ingeniería que se ejecutan durante grandes periodos,con sus estructuras de control simples y sus extensos conjuntos de datos, pueden paralelizarse conmás facilidad y con mayor detalle que las aplicaciones antes mencionadas. Por ende, este capítulose dedica principalmente a las técnicas que se aplican a las aplicaciones numéricas, y en particular,a los programas que invierten la mayor parte de su tiempo manipulando datos en arreglos multidimensionales. A continuación examinaremos esta clase de programas.11.1.3 Paralelismo a nivel de cicloLos ciclos son el objetivo principal para la paralelización, en especial en las aplicaciones queutilizan arreglos. Las aplicaciones que se ejecutan por largos periodos tienen, por lo regular,arreglos extensos, los cuales conducen a ciclos que tienen muchas iteraciones, una para cadaelemento en el arreglo. Es común encontrar ciclos cuyas iteraciones son independientes entre sí.Podemos dividir el gran número de iteraciones de dichos ciclos entre los procesadores. Si lacantidad de trabajo realizada en cada iteración es casi la misma, con sólo dividir las iteracionesde manera uniforme a través de los procesadores se logrará el máximo paralelismo. El ejemplo11.1 es un ejemplo bastante simple, que muestra cómo podemos sacar ventaja del paralelismoa nivel de ciclo.Ejemplo 11.1: El ciclocalcula el cuadrado de las diferencias entre los elementos en los vectores X y Y, y lo almacenaen Z. El ciclo es paralelizable, ya que cada iteración accede a un conjunto distinto de datos. Podemos ejecutar el ciclo en una computadora con M procesadores, al proporcionar a cada procesador un ID único p = 0, 1,…, M − 1 y hacer que cada procesador ejecute el mismo código:11.1 Conceptos básicos 775Maq. Cap_11_AHO A.indd 775 11/10/07 8:18:56 PM776 Capítulo 11. Optimización para el paralelismo y la localidadParalelismo a nivel de tareaEs posible encontrar paralelismo fuera de las iteraciones en un ciclo. Por ejemplo, podemos asignar dos invocaciones a funciones distintas, o dos ciclos independientes, a dosprocesadores. A esta forma de paralelismo se le conoce como paralelismo a nivel de tarea.El nivel de tarea no es una fuente de paralelismo tan atractiva como el nivel de ciclo. Larazón es que el número de tareas independientes es una constante para cada programay no se escala con el tamaño de los datos, como lo hace el número de iteraciones de unciclo típico. Además, las tareas por lo general no son de igual tamaño, por lo que es difícilmantener todos los procesadores ocupados todo el tiempo.Dividimos las iteraciones en el ciclo de manera uniforme entre los procesadores; el p-ésimoprocesador recibe la p-ésima franja de iteraciones a ejecutar. Observe que el número de iteraciones tal vez no pueda dividirse entre M, por lo que nos aseguramos de que el último procesadorno ejecute más allá del límite del ciclo original, introduciendo una operación mínima. ✷El código en paralelo que se muestra en el ejemplo 11.1 es un programa SPMD (Singleprogram multiple data, Un solo programa, varios datos). Todos los procesadores ejecutan elmismo código, pero se parametriza mediante un identificador único para cada procesador, porlo que distintos procesadores pueden realizar diferentes acciones. Por lo general, un procesador,conocido como maestro, ejecuta toda la parte serial de los cálculos. El procesador maestro, alllegar a una sección paralelizada del código, despierta a todos los procesadores esclavos. Todoslos procesadores ejecutan las regiones paralelizadas del código. Al final de cada región paralelizada de código, todos los procesadores participan en una sincronización de barrera. Se garantiza que cualquier operación que se ejecute antes de que un procesador entre a una barrera desincronización se completará antes de permitir que cualquier otro procesador deje la barrera yejecute operaciones que vengan después de la misma.Si paralelizamos sólo los ciclos pequeños como los del ejemplo 11.1, entonces es probableque el código resultante tenga una cobertura baja de paralelismo y un paralelismo relativamente menos específico. Preferimos paralelizar los ciclos más externos en un programa, ya que esoproduce el paralelismo más general. Por ejemplo, considere la aplicación de una transformaciónFFT bidimensional que opera sobre un conjunto de datos n × n. Dicho programa realiza n FFTsen las filas de los datos, y después otras n FFTs en las columnas. Es preferible asignar lasn FFTs independientes a un procesador cada una, en vez de tratar de usar varios procesadorespara que colaboren con una FFT. El código es más fácil de escribir, la cobertura de paralelismoMaq. Cap_11_AHO A.indd 776 11/10/07 8:18:57 PMpara el algoritmo es del 100%, y el código tiene una buena localidad de los datos, ya que norequiere comunicación alguna mientras calcula una FFT.En muchas aplicaciones los ciclos más externos no son grandes, y no pueden paralelizarse.Sin embargo, el tiempo de ejecución de estas aplicaciones se ve dominado a menudo por losnúcleos (kernels) que consumen mucho tiempo, que pueden tener cientos de líneas de códigoconsistentes en ciclos con distintos niveles de anidamiento. Algunas veces es posible tomar elnúcleo, reorganizar su computación y particionarlo en unidades casi independientes, enfocándose en su localidad.11.1.4 Localidad de los datosExisten dos nociones bastante distintas de localidad de datos, que debemos tener en consideración al paralelizar los programas. La localidad temporal ocurre cuando se utilizan los mismos datos varias veces dentro de un periodo corto. La localidad espacial ocurre cuando distintoselementos de datos, que están ubicados unos cerca de otros, se utilizan dentro de un periodocorto. Una forma importante de la localidad espacial ocurre cuando todos los elementos queaparecen en una línea de caché se utilizan en conjunto. La razón es que, tan pronto como senecesita un elemento de una línea de caché, todos los elementos en la misma línea se llevan a lacaché y probablemente sigan ahí si se utilizan pronto. El efecto de esta localidad espacial es quese disminuyen al mínimo los fallos en la caché, lo cual produce un importante aumento en lavelocidad del programa.A menudo, los núcleos se pueden escribir en muchas formas equivalentes de manera semántica, pero con localidades de datos y rendimientos que varían en gran medida. El ejemplo 11.2muestra una forma alternativa de expresar los cálculos del ejemplo 11.1.Ejemplo 11.2: Al igual que el ejemplo 11.1, el siguiente código también encuentra los cuadrados de las diferencias entre los elementos de los vectores X y Y.El primer ciclo encuentra las diferencias, el segundo encuentra los cuadrados. En los programasreales aparece a menudo código como éste, ya que ésta es la forma en la que podemos optimizarun programa para las máquinas de vectores, que son supercomputadoras con instrucciones querealizan varias operaciones aritméticas simples sobre vectores a la vez. Podemos ver que loscuerpos de los dos ciclos aquí se fusionan en uno, en el ejemplo 11.1.Dado que los dos programas realizan los mismos cálculos, ¿cuál tiene un mejor rendimiento? El ciclo fusionado en el ejemplo 11.1 tiene un mejor rendimiento, ya que tiene una mejorlocalidad de los datos. Cada diferencia se eleva al cuadrado de inmediato, tan pronto como seproduce; de hecho, podemos guardar la diferencia en un registro, elevarla al cuadrado y escribirel resultado sólo una vez en la ubicación de memoria Z[i]. En contraste, el código en el ejemplo11.1 obtiene Z[i] una vez, y lo escribe dos veces. Además, si el tamaño del arreglo es más grande que la caché, Z[i] tiene que volver a obtenerse de memoria la segunda vez que se utiliza eneste ejemplo. Por ende, este código puede ejecutarse considerablemente más lento. ✷11.1 Conceptos básicos 777Maq. Cap_11_AHO A.indd 777 11/10/07 8:18:57 PM778 Capítulo 11. Optimización para el paralelismo y la localidad(a) Poner un arreglo en ceros, columna por columna.(b) Poner un arreglo en ceros, fila por fila.(c) Poner un arreglo en ceros, fila por fila en paralelo.Figura 11.3: Código secuencial y paralelo para poner un arreglo en cerosEjemplo 11.3: Suponga que queremos establecer todos los elementos del arreglo Z, almacenados en orden por filas (recuerde la sección 6.4.3), en cero. Las figuras 11.3(a) y (b) recorren elarreglo, columna por columna y fila por fila, respectivamente. Podemos transponer los ciclos enla figura 11.3(a) para llegar a la figura 11.3(b). En términos de localidad espacial, es preferibleponer en ceros el arreglo fila por fila, ya que todas las palabras en una línea de caché se ponenen ceros en forma consecutiva. En el método de columna por columna, aun cuando cada líneade caché se reutiliza mediante iteraciones consecutivas del ciclo externo, se limpiará el contenido de las líneas de caché antes de la reutilización si el tamaño de una columna es mayor queel tamaño de la caché. Para un mejor rendimiento, paralelizamos el ciclo externo de la figura11.3(b) de una manera similar a la que utilizamos en el ejemplo 11.1. ✷Los dos ejemplos anteriores ilustran varias características importantes asociadas con lasaplicaciones numéricas que operan sobre los arreglos:• A menudo, el código de los arreglos tiene muchos ciclos paralelizables.• Cuando los ciclos tienen paralelismo, sus iteraciones pueden ejecutarse en orden arbitrario; pueden reordenarse para mejorar de manera considerable la localidad de los datos.• A medida que creamos unidades extensas de cálculos en paralelo que son independientesentre sí, la ejecución de éstas en serie tiende a producir una buena localidad de los datos.11.1.5 Introducción a la teoría de la transformación afínEs difícil escribir programas secuenciales correctos y eficientes; es aún más difícil escribir programas en paralelo que sean correctos y eficientes. El nivel de dificultad se incrementa a medidaMaq. Cap_11_AHO A.indd 778 11/10/07 8:18:58 PMque disminuye la granularidad del paralelismo explotado. Como podemos ver en la sección anterior,los programadores deben poner atención a la localidad de los datos para obtener un alto rendimiento. Además, la tarea de tomar un programa secuencial existente y paralelizarlo es en extremodifícil. Es difícil atrapar todas las dependencias en el programa, en especial si no es un programacon el que estemos familiarizados. Es todavía más difícil depurar un programa en paralelo, ya quelos errores pueden ser no deterministas.Lo ideal sería que un compilador paralelizador tradujera en forma automática los programas secuenciales ordinarios en programas eficientes en paralelo, y que optimizara la localidadde estos programas. Por desgracia, los compiladores que no tienen un conocimiento de altonivel sobre la aplicación, sólo pueden preservar la semántica del algoritmo original, que puedeno ser susceptible a la paralelización. Además, tal vez los programadores hayan realizado elecciones arbitrarias que limiten el paralelismo del programa.Se han demostrado éxitos en la paralelización y las optimizaciones de localidad para aplicaciones numéricas en Fortran, que operan sobre arreglos con accesos afines. Sin apuntadores niaritmética de apuntadores, Fortran es más fácil de analizar. Tenga en cuenta que no todas lasaplicaciones tienen accesos afines; lo más notable es que muchas aplicaciones numéricas operansobre matrices poco densas, cuyos elementos se utilizan en forma indirecta a través de otroarreglo. Este capítulo se enfoca en la paralelización y las optimizaciones de los núcleos, queconsisten en su mayor parte en decenas de líneas.Como muestran los ejemplos 11.2 y 11.3, la paralelización y la optimización de la localidadrequieren que razonemos acerca de las distintas instancias de un ciclo y sus relaciones entre sí.Esta situación es muy distinta al análisis del flujo de datos, en donde combinamos la información asociada con todas las instancias en conjunto.Para el problema de optimizar ciclos con accesos a arreglos, utilizamos tres tipos de espacios. Cada espacio puede considerarse como puntos en una rejilla de una o más dimensiones.1. El espacio de iteraciones es el conjunto de las instancias de ejecución dinámicas en uncálculo; es decir, el conjunto de combinaciones de valores que reciben los índices delciclo.2. El espacio de datos es el conjunto de elementos del arreglo a los que se accede.3. El espacio de procesadores es el conjunto de procesadores en el sistema. Por lo general,a estos procesadores se les asignan números enteros o vectores de enteros para diferenciarlos.Como entrada se proporcionan un orden secuencial en el que se ejecutan las iteraciones, y funciones afines de acceso a arreglos (por ejemplo, X[i, j + 1]) que especifican cuáles instancias enel espacio de iteraciones acceden a cuáles elementos en el espacio de datos.Los resultados de la optimización, que de nuevo se representan como funciones afines, definen lo que hace cada procesador, y cuándo lo hace. Para especificar qué hace cada procesador,utilizamos una función afín para asignar a los procesadores instancias en el espacio de iteraciones original. Para especificar cuándo, usamos una función afín para asignar instancias enel espacio de iteraciones a un nuevo ordenamiento. El programa se deriva mediante el análisisde las funciones de acceso a los arreglos, en búsqueda de dependencias de datos y patrones dereutilización.11.1 Conceptos básicos 779Maq. Cap_11_AHO A.indd 779 11/10/07 8:18:59 PM780 Capítulo 11. Optimización para el paralelismo y la localidadEl siguiente ejemplo ilustrará los tres espacios (de iteraciones, de datos y de procesadores).También introducirá de manera informal los conceptos y cuestiones importantes que debemostratar al usar estos espacios para paralelizar el código. En secciones posteriores cubriremos condetalle cada uno de los conceptos.Ejemplo 11.4: La figura 11.4 ilustra el uso de los distintos espacios y sus relaciones en elsiguiente programa:Los tres espacios y las asignaciones entre ellos son:. . .0 10. . . . . .1 9 11 19 2010 . . . 90 1. . .9Región de los datos a los que se accedeEspacio de datosEspacio de iteracionesEspacio de procesadoresFunciones afines deíndices de arreglosParticionamiento afínFigura 11.4: Espacio de iteraciones, de datos y de procesadores para el ejemplo 11.41. Espacio de iteraciones: El espacio de iteraciones es el conjunto de iteraciones, cuyos IDsse proporcionan por los valores que guardan las variables de índice de ciclo. Un anidamiento de ciclos con d niveles (es decir, d ciclos anidados) tiene d variables índice, y porende se modela mediante un espacio d-dimensional. El espacio de iteraciones se delimitamediante los límites inferior y superior de los índices del ciclo. El ciclo de este ejemplodefine un espacio unidimensional de 10 iteraciones, etiquetadas mediante los valores delíndice del ciclo: i = 0, 1,…, 9.2. Espacio de datos: El espacio de datos lo proporcionan directamente las declaraciones delarreglo. En este ejemplo, los elementos en el arreglo se indexan mediante a = 0, 1,…, 99.Aun cuando todos los arreglos se aplanan en el espacio de direcciones de un programa,tratamos a los arreglos n-dimensionales como espacios n-dimensionales, y suponemosque los índices individuales permanecen dentro de sus límites. En este ejemplo, el arregloes unidimensional de todas formas.Maq. Cap_11_AHO A.indd 780 11/10/07 8:18:59 PM3. Espacio de procesadores: Pretendemos que hay un número ilimitado de procesadoresvirtuales en el sistema como nuestro objetivo inicial de paralelización. Los procesadores se organizan en un espacio multidimensional, una dimensión para cada ciclo en elanidamiento que deseamos paralelizar. Después de la paralelización, si tenemos menosprocesadores físicos que virtuales, dividimos los procesadores virtuales en bloques pares, yasignamos un bloque a cada procesador. En este ejemplo, sólo necesitamos diez procesadores, uno para cada iteración del ciclo. En la figura 11.4 suponemos que los procesadoresestán organizados en un espacio unidimensional y se enumeran así: 0, 1,…, 9, en dondela iteración i del ciclo se asigna al procesador i. Si hubiera, por decir, sólo cinco procesadores, podríamos asignar las iteraciones 0 y 1 al procesador 0, las iteraciones 2 y 3 alprocesador 1, y así sucesivamente. Como las iteraciones son independientes, no importacómo las asignemos, siempre y cuando cada uno de los cinco procesadores obtenga dositeraciones.4. Función afín de índice de arreglo: Cada acceso al arreglo en el código especifica una asignación de una iteración en el espacio de iteraciones, a un elemento del arreglo en el espacio dedatos. La función de acceso es afín si implica la multiplicación de las variables índice del ciclopor constantes y la suma de las constantes. Las funciones i + 10 e i del índice del arregloson afines. De la función de acceso podemos conocer la dimensión de los datos a los que seaccede. En este caso, como cada función de índice tiene una variable de ciclo, el espacio delos elementos del arreglo a los que se accede es unidimensional.5. Particionamiento afín: Para paralelizar un ciclo, usamos una función afín para asignariteraciones en un espacio de iteraciones a los procesadores en el espacio de procesadores.En nuestro ejemplo, sólo asignamos la iteración i al procesador i. También podemosespecificar un nuevo orden de ejecución con las funciones afines. Si deseamos ejecutar elciclo anterior en forma secuencial, pero a la inversa, podemos especificar la función deordenamiento brevemente con una expresión afín 10 − i. Por ende, la iteración 9 es la 1ªiteración en ejecutarse, y así sucesivamente.6. Región de los datos a los que se accede: Para encontrar el mejor particionamiento afín,es útil conocer la región de los datos a los que accede una iteración. Podemos obtener laregión de los datos a los que accede si combinamos la información del espacio de iteraciones con la función del índice del arreglo. En este caso, el acceso Z[i + 10] al arregloentra en contacto con la región {a | 10 ≤ a < 20}, y el acceso Z[i] entra en contacto conla región {a | 0 ≤ a < 10}.7. Dependencia de datos: Para determinar si el ciclo es paralelizable, preguntamos si hay unadependencia de datos que cruce el límite de cada iteración. Para este ejemplo, primeroconsideramos las dependencias de los accesos de escritura en el ciclo. Como la función deacceso Z[i + 10] asigna distintas iteraciones a distintas ubicaciones del arreglo, no haydependencias en relación con el orden en el que las diversas iteraciones escriben valoresen el arreglo. ¿Hay una dependencia entre los accesos de lectura y de escritura? Comosólo se escribe en Z[10], Z[11],…, Z[19] (mediante el acceso Z[i + 10]), y sólo se lee enZ[0], Z[1],…, Z[9] (mediante el acceso Z[i ]), no puede haber dependencias en relacióncon el orden relativo de una lectura y una escritura. Por ende, este ciclo es paralelizable.11.1 Conceptos básicos 781Maq. Cap_11_AHO A.indd 781 11/10/07 8:19:00 PM782 Capítulo 11. Optimización para el paralelismo y la localidadEs decir, cada iteración del ciclo es independiente de todas las demás iteraciones, y podemos ejecutar las iteraciones en paralelo, o en cualquier orden que seleccionemos. Sinembargo, tenga en cuenta que si realizamos un pequeño cambio, por decir al incrementarel límite superior en el índice i del ciclo a 10 o más, entonces habría dependencias, yaque se escribiría en algunos elementos del arreglo Z en una iteración y después se leería10 iteraciones más adelante. En ese caso, el ciclo no podría paralelizarse por completo, ytendríamos que pensar con cuidado cómo se particionaron las iteraciones entre los procesadores, y cómo ordenamos las iteraciones.✷Al formular el problema en términos de espacios multidimensionales y asignaciones afines entre estos espacios, podemos usar técnicas matemáticas estándar para resolver el problema de laparalelización y la optimización de la localidad en forma general. Por ejemplo, la región de datosa los que se accede puede encontrarse mediante la eliminación de variables, usando el algoritmode eliminación de Fourier-Motzkin. La dependencia de datos demuestra ser equivalente al problema de la programación lineal entera. Por último, para encontrar el particionamiento afín esnecesario resolver un conjunto de restricciones lineales. No se preocupe si no está familiarizadocon estos conceptos, ya que los explicaremos a partir de la sección 11.3.11.2 Multiplicación de matrices: un ejemplo detalladoVamos a presentar muchas de las técnicas utilizadas por los compiladores paralelos en unejemplo extendido. En esta sección, exploraremos el conocido algoritmo de multiplicación dematrices, para mostrar que no es común optimizar ni siquiera un programa paralelizable simpley sencillo. Veremos cómo al rescribir el código se puede mejorar la localidad de los datos; esdecir, los procesadores pueden hacer su trabajo con mucho menos comunicación (con la memoria global o con otros procesadores, dependiendo de la arquitectura) que si se elije el programadirecto. También hablaremos acerca de cómo el conocimiento de la existencia de líneas de cachéque guardan varios elementos consecutivos de datos puede mejorar el tiempo de ejecución deprogramas como la multiplicación de matrices.11.2.1 El algoritmo de multiplicación de matricesEn la figura 11.5 podemos ver un programa típico de multiplicación de matrices.2 Recibe dosmatrices de n × n, X y Y, y produce su producto en una tercera matriz de n × n, Z. Recuerde que Z i j (el elemento de la matriz Z que está en la fila i y la columna j) debe convertirseen2 En los programas en seudocódigo de este capítulo, por lo general, utilizaremos la sintaxis de C, pero para quelos accesos a arreglos multidimensionales (la cuestión central para la mayor parte del capítulo) sean más fáciles de leer,utilizaremos referencias a arreglos al estilo Fortran; es decir, Z[i, j] en vez de Z[i][j].Maq. Cap_11_AHO A.indd 782 11/10/07 8:19:01 PMFigura 11.5: El algoritmo básico de multiplicación de matricesEl código de la figura 11.5 genera n 2 resultados, cada uno de las cuales es un producto interno entre una fila y una columna de los dos operandos tipo matriz. Sin duda, los cálculos decada uno de los elementos de Z son independientes y pueden ejecutarse en paralelo.Entre más grande sea n, más veces entrará el algoritmo en contacto con cada elemento. Esdecir, hay 3n 2 ubicaciones entre las tres matrices, pero el algoritmo realiza n 3 operaciones, cadauna de las cuales multiplica un elemento de X por un elemento de Y, y suma el producto a unelemento de Z. Por ende, el algoritmo hace un uso intensivo de cálculos y los accesos a memoriano deben, en principio, constituir un cuello de botella.Ejecución serial de la multiplicación de matricesVamos a considerar primero cómo se comporta este programa cuando se ejecuta de manerasecuencial en un uniprocesador. El ciclo más interno lee y escribe en el mismo elemento de Z, yutiliza una fila de X y una columna de Y. Z[i, j] puede almacenarse con facilidad en un registroy no requiere accesos a memoria. Suponga, sin perder la generalidad, que la matriz se distribuyeen orden por filas, y que c es el número de elementos del arreglo en una línea de caché.X Y. . . n= 010 ï 1=ijFigura 11.6: El patrón de acceso de datos en la multiplicación de matricesLa figura 11.6 sugiere el patrón de acceso a medida que ejecutamos una iteración del cicloexterno de la figura 11.5. En especial, la imagen muestra la primera iteración, con i = 0. Cada11.2 Multiplicación de matrices: un ejemplo detallado 783Maq. Cap_11_AHO A.indd 783 11/10/07 8:19:01 PM784 Capítulo 11. Optimización para el paralelismo y la localidadvez que avanzamos de un elemento de la primera fila de X al siguiente, visitamos cada elemento en una sola columna de Y. En la figura 11.6 podemos ver la supuesta organización de lasmatrices en líneas de caché. Es decir, cada rectángulo pequeño representa a una línea de cachéque contiene cuatro elementos del arreglo (es decir, c = 4 y n = 12 en la imagen).El acceso a X impone una carga ligera sobre la caché. Una fila de X se esparce entre sólo n/clíneas de caché. Suponiendo que todos estos elementos caben en la caché, sólo se producenn/c fallos en la caché para un valor fijo del índice i, y el número total de fallos para todos loselementos de X es de n 2/c, el mínimo posible (suponemos que n es divisible entre c, por conveniencia).Sin embargo, al usar una fila de X, el algoritmo de multiplicación de matrices accede atodos los elementos de Y, columna por columna. Es decir, cuando j = 0, el ciclo interno llevaa la caché la primera columna completa de Y. Observe que los elementos de esa columna sealmacenan entre n líneas distintas de caché. Si la caché es lo bastante grande (o si n es lo bastante pequeña) para guardar n líneas de caché, y ningún otro uso de la caché obliga a expulsaralgunas de estas líneas de caché, entonces la columna para j = 0 aún estará en la caché cuandonecesitemos la segunda columna de Y. En ese caso, no habrá otros n fallos de caché al leer Y,hasta j = c, momento en el cual debemos llevar a la caché un conjunto totalmente distinto delíneas de caché para Y. Así, para completar la primera iteración del ciclo externo (con i = 0) serequieren entre n 2/c y n 2 fallos de caché, dependiendo de si las columnas de las líneas de cachépueden sobrevivir de una iteración del segundo ciclo a la siguiente.Además, al completar el ciclo externo, para i = 1, 2, y así sucesivamente, podemos tenermuchos fallos adicionales en la caché al leer Y, o ninguno en lo absoluto. Si la caché es lo bastante grande como para que puedan residir en ella todas las n 2/c líneas de caché que contienena Y, entonces no necesitamos más fallos de caché. En consecuencia, el número total de fallos decaché es 2n 2/c, la mitad para X y la mitad para Y. No obstante, si la caché puede guardar unacolumna de Y, pero no todo el contenido de Y, entonces debemos llevar a todos los elementosde Y en la caché de nuevo, cada vez que realicemos una iteración del ciclo externo. Es decir,el número de fallos en la caché es de n 2/c + n 3/c; el primer término es para X y el segundopara Y. Peor aún, si no podemos guardar ni siquiera una columna de Y en la caché, entoncestenemos n 2 fallos de caché por cada iteración del ciclo externo, y un total de n 2/c + n 3 fallosde caché.Paralelización fila por filaAhora vamos a considerar como podríamos usar cierto número de procesadores, por decir pprocesadores, para agilizar la ejecución de la figura 11.5. Un método obvio para la paralelización de la multiplicación de matrices es asignar distintas filas de Z a distintos procesadores. Unprocesador es responsable de n/p filas consecutivas (suponemos que n es divisible entre p, porconveniencia). Con esta división del trabajo, cada procesador debe acceder a n/p filas de lasmatrices X y Z, pero a toda la matriz Y completa. Un procesador calculará n 2/p elementos de Z,realizando para ello n 3/p operaciones de multiplicación y suma.Aunque, en consecuencia, el tiempo de cálculo disminuye en proporción a p, el costo decomunicación se eleva en proporción a p. Es decir, cada uno de p procesadores tiene que leern 2/p elementos de X, pero todos los n 2 elementos de Y. El número total de líneas de cachéque deben entregarse a las cachés de los p procesadores es n 2/c + pm 2/c; los dos términos sonMaq. Cap_11_AHO A.indd 784 11/10/07 8:19:03 PMpara entregar X y copias de Y, respectivamente. A medida que p se acerca a n, el tiempo decálculo se convierte en O(n 2) mientras que el costo de comunicación es O(n 3). Es decir, el busen el que se mueven los datos entre la memoria y las cachés de los procesadores se convierte enel cuello de botella. En consecuencia, con la distribución propuesta de los datos, si utilizamosun gran número de procesadores para compartir los cálculos en realidad se podría disminuir lavelocidad de los cálculos, en vez de agilizarlos.11.2.2 OptimizacionesEl algoritmo de multiplicación de matrices de la figura 11.5 muestra que, aun cuando un algoritmo puede reutilizar los mismos datos, puede tener una mala localidad de los mismos. Lareutilización de datos produce una ocurrencia en la caché, sólo si la reutilización se lleva a cabocon la suficiente prontitud, antes de que los datos se desplacen de la caché. En este caso, n 2operaciones de multiplicación-suma separan la reutilización del mismo elemento de datos en lamatriz Y, por lo que la localidad es mala. De hecho, n operaciones separan la reutilización dela misma línea de caché en Y. Además, en un multiprocesador, la reutilización puede produciruna ocurrencia en la caché sólo si el mismo procesador reutiliza los datos. Cuando consideramos una implementación en paralelo en la sección 11.2.1, vimos que cada procesador tenía queutilizar los elementos de Y. Por ende, la reutilización de Y no se convierte en localidad.Modificación de la distribución de los datosUna manera de mejorar la localidad de un programa es modificar la distribución de sus estructuras de datos. Por ejemplo, la acción de almacenar Y en orden por columnas hubieramejorado la reutilización de las líneas de caché para la matriz Y. La capacidad de aplicaciónde este método es limitada, debido a que, por lo general, se utiliza la misma matriz en distintasoperaciones. Si Y jugara el papel de X en otra multiplicación de matrices, entonces sufriría porestar almacenada en orden por columnas, ya que la primera matriz en una multiplicación sealmacena mejor en orden por filas.Uso de bloquesAlgunas veces es posible modificar el orden de ejecución de las instrucciones para mejorar lalocalidad de los datos. Sin embargo, la técnica de intercambiar ciclos no mejora la rutina demultiplicación de matrices. Suponga que la rutina se escribiera para generar una columna de lamatriz Z a la vez, en vez de una fila a la vez. Es decir, hacer que el ciclo j sea el ciclo externoy que el ciclo i sea el interno. Suponiendo que las matrices están almacenadas en orden porfila, la matriz Y disfruta de una mejor localidad espacial y temporal, pero sólo a expensas dela matriz X.El uso de bloques es otra forma de reordenar las iteraciones en un ciclo, que puede mejoraren forma considerable la localidad de un programa. En vez de calcular el resultado una fila ocolumna a la vez, dividimos la matriz en submatrices, o bloques, como lo sugiere la figura 11.7,y ordenamos las operaciones de manera que se utilice todo un bloque completo durante uncorto periodo. Por lo general, los bloques son cuadros con un lado de longitud B. Si B divideuniformemente a n, entonces todos los bloques son cuadrados. Si B no divide uniformemente11.2 Multiplicación de matrices: un ejemplo detallado 785Maq. Cap_11_AHO A.indd 785 11/10/07 8:19:03 PM786 Capítulo 11. Optimización para el paralelismo y la localidada n, entonces los bloques en los extremos inferior y derecho tendrán uno o ambos lados delongitud menor a B.nBFigura 11.7: Una matriz dividida en bloques de tamaño BLa figura 11.8 muestra una versión del algoritmo básico de multiplicación de matrices, endonde las tres matrices se han dividido en cuadros de tamaño B. Como en la figura 11.5, seasume que se han inicializado a 0 todos los elementos en Z. Suponemos que B divide a n; si noes así, entonces debemos modificar la línea (4) para que el límite superior sea mín(ii + B,n), ylo hacemos de manera similar para las líneas (5) y (6).Figura 11.8: Multiplicación de matrices con uso de bloquesLos tres ciclos externos, las líneas (1) a (3), utilizan los índices ii, jj y kk, que siempre seincrementan mediante B y, por lo tanto, siempre marcan la arista izquierda o superior de algunos bloques. Con valores fijos de ii, jj y kk, las líneas (4) a (7) permiten que los bloques con lasesquinas superiores izquierdas X[ii, kk ] y Y[kk, jj] realicen todas las posibles contribuciones albloque con la esquina superior izquierda Z[ii, jj].Si elegimos B en forma apropiada, podemos reducir de manera considerable el número defallos de caché, comparados con el algoritmo básico, cuando no pueden caber todos los elementos de X, Y o Z en la caché. Elija B de forma que sea posible acomodar un bloque de cada unaMaq. Cap_11_AHO A.indd 786 11/10/07 8:19:04 PMOtra visión de la multiplicación de matrices basada en bloquesPodemos imaginar que las matrices X, Y y Z de la figura 11.8 no son matrices de n × nde números de punto flotante, sino matrices de (n/B) × (n/B) cuyos elementos son en símatrices de B × B de números de punto flotante. Las líneas (1) a (3) de la figura 11.8 sonentonces como los tres ciclos del algoritmo básico de la figura 11.5, pero con n/B como eltamaño de las matrices, en vez de n. Entonces, podemos considerar que las líneas (4) a(7) de la figura 11.8 implementan una sola operación de multiplicar y sumar de la figura11.5. Observe que en esta operación, el paso de multiplicación individual es un paso demultiplicación de matrices, y utiliza el algoritmo básico de la figura 11.5 en los númerosde punto flotante que son elementos de las dos matrices involucradas. La suma de matrices es la suma a nivel de elementos de los números de punto flotante.de las matrices en la caché. Debido al orden de los ciclos, en realidad necesitamos cada bloqueZ en la caché sólo una vez, por lo que (como en el análisis del algoritmo básico de la sección11.2.1) no contaremos los fallos de caché debido a Z.Para llevar un bloque de X o Y a la caché, se requieren B 2/c fallos de caché; recuerde que ces el número de elementos en una línea de caché. Sin embargo, con bloques fijos de X y Y, realizamos B 3 operaciones de multiplicación y suma en las líneas (4) a (7) de la figura 11.8. Comotoda la multiplicación de matrices requiere n 3 operaciones de multiplicación y suma, el númerode veces que necesitamos llevar un par de bloques a la caché es n 3/B 3. Como requerimos 2B 2/cfallos de caché cada vez que lo hacemos, el número total de fallos de caché es 2n 3/Bc.Es interesante comparar 2n 3/Bc con los estimados que se proporcionan en la sección 11.2.1.Ahí vimos que si pueden caber todas las matrices completas en la caché, entonces basta conO(n 2/c) fallos de caché. Sin embargo, en ese caso podemos elegir B = n; es decir, hacer quecada matriz sea un solo bloque. De nuevo, obtenemos O(n 2/c) como nuestra estimación de losfallos de caché. Por otro lado, observamos que si no caben las matrices completas en la caché,requerimos O(n 3/c) fallos de caché, o incluso O(n 3) fallos de caché. En ese caso, suponiendo que aún podemos elegir un valor de B considerablemente grande (por ejemplo, B podríaser 200, y de todas formas podríamos meter tres bloques de números de 8 bytes en una cachéde un megabyte), hay una gran ventaja en cuanto al uso de bloques en una multiplicación dematrices.La técnica de los bloques puede volver a aplicarse en cada nivel de la jerarquía de memoria.Por ejemplo, tal vez queramos optimizar el uso de los registros, guardando los operandos deuna multiplicación de matrices de 2 × 2 en los registros. Elegimos tamaños de bloques cada vezmás grandes para los distintos niveles de cachés y la memoria física.De manera similar, podemos distribuir los bloques entre los procesadores para disminuir almínimo el tráfico de datos. Los experimentos demostraron que dichas optimizaciones puedenmejorar el rendimiento de un uniprocesador por un factor de 3, y el aumento en velocidad en unmultiprocesador está cerca de ser lineal, con respecto al número de procesadores utilizados.11.2 Multiplicación de matrices: un ejemplo detallado 787Maq. Cap_11_AHO A.indd 787 11/10/07 8:19:05 PM788 Capítulo 11. Optimización para el paralelismo y la localidad11.2.3 Interferencia de la cachéPor desgracia, hay algo más en la historia de la utilización de la caché. La mayoría de las cachésno son completamente asociativas (vea la sección 7.4.2). En una caché con asignación directa,si n es un múltiplo del tamaño de la caché, entonces todos los elementos en la misma fila deun arreglo de n × n competirán por la misma ubicación en la caché. En ese caso, al llevar elsegundo elemento de una columna se descartará la línea de caché del primero, aun cuando lacaché tiene la capacidad de mantener ambas líneas al mismo tiempo. A esta situación se leconoce como interferencia de la caché.Hay varias soluciones para este problema. La primera es reordenar los datos de una vezpor todas, para que los datos a los que se acceda se distribuyan en ubicaciones de datos consecutivas. La segunda es incrustar el arreglo de n × n en un arreglo más grande de m × n, endonde m se elije de manera que se disminuya al mínimo el problema de la interferencia. Latercera es que, en algunos casos podemos elegir un tamaño de bloque que garantice evitar la interferencia.11.2.4 Ejercicios para la sección 11.2Ejercicio 11.2.1: El algoritmo de multiplicación de matrices basado en bloques de la figura11.8 no inicializa la matriz Z a cero, como lo hace el código de la figura 11.5. Agregue los pasosque inicializan Z a ceros en la figura 11.8.11.3 Espacios de iteracionesLa motivación para este estudio es explotar las técnicas que, en escenarios simples como la multiplicación de matrices de la sección 11.2, fueron bastante simples y directas. En el escenariomás general se aplican las mismas técnicas, pero son mucho menos intuitivas. Pero mediantela aplicación de ciertas técnicas del álgebra lineal, podemos hacer que todo funcione en elescenario general.Como vimos en la sección 11.1.5, hay tres tipos de espacios en nuestro modelo de transformación: espacio de iteraciones, espacio de datos y espacio de procesadores. Aquí empezaremos conel espacio de iteraciones. El espacio de iteraciones de un anidamiento de ciclos se define comotodas las combinaciones de valores de índices de los ciclos en el anidamiento.A menudo, el espacio de iteraciones es rectangular, como en el ejemplo de multiplicación dematrices de la figura 11.5. Ahí, cada uno de los ciclos anidados tenía un límite inferior de 0 yun límite superior de n − 1. Sin embargo, en anidamientos de ciclos más complicados, pero aunasí bastante realistas, los límites inferiores y superiores en un índice de ciclo pueden dependerde los valores de los índices de los ciclos externos. En breve veremos un ejemplo.11.3.1 Construcción de espacios de iteraciones a partirde anidamientos de ciclosPara empezar, vamos a describir el tipo de anidamientos de ciclos que pueden manejarse mediante las técnicas a desarrollar. Cada ciclo tiene un solo índice de ciclo, el cual suponemos seincrementa en 1 durante cada iteración. Esta suposición es sin pérdida de generalidad, ya quesi el incremento es en base al entero c > 1, siempre podemos sustituir los usos del índice i porMaq. Cap_11_AHO A.indd 788 11/10/07 8:19:05 PMlos usos de ci + a para cierta constante a positiva o negativa, y después incrementar i por 1en el ciclo. Los límites del ciclo deben escribirse como expresiones afines de los índices del cicloexterno.Ejemplo 11.5: Considere el siguiente ciclo:que incrementa a i en 3 cada vez que se recorre el ciclo. El efecto es asignar 0 a cada uno de loselementos Z[2], Z[5], Z[8],…, Z[98]. Podemos obtener el mismo efecto con:Es decir, sustituimos 3j + 2 por i. El límite inferior i = 2 se convierte en j = 0 (sólo hay queresolver 3j + 2 = 2 para j), y el límite superior i ≤ 100 se convierte en j ≤ 32 (hay que simplificar 3j + 2 ≤ 100 para obtener j ≤ 32.67 y redondear, ya que j tiene que ser un entero).✷Por lo general, utilizaremos ciclos for en los anidamientos de ciclos. Un ciclo while o repeatpuede sustituirse por un ciclo for si hay un índice y límites superior e inferior para el índice,como sería el caso en algo parecido al ciclo de la figura 11.9(a). Un ciclo for como for (i=0;i<100; i++) sirve para el mismo propósito exacto.Sin embargo, algunos ciclos while o repeat no tienen un límite evidente. Por ejemplo, lafigura 11.9(b) puede o no terminar, pero no hay forma de saber qué condición sobre i en elcuerpo no visto del ciclo hace que éste interrumpa su ejecución. La figura 11.9(c) es otro casoproblema. Por ejemplo, la variable n podría ser un parámetro de una función. Sabemos que elciclo itera n veces, pero no sabemos cuál es el valor de n en tiempo de compilación, y de hechopodemos esperar que varias ejecuciones distintas del ciclo se ejecutarán distintos números deveces. En casos como (b) y (c), debemos tratar al límite superior sobre i como infinito.Un anidamiento de ciclos con d niveles puede modelarse mediante un espacio d-dimensional.Las dimensiones están ordenadas, en donde la k -ésima dimensión representa al k-ésimo cicloanidado, contando desde el ciclo más externo, hacia dentro. Un punto (x 1, x 2,…, xd) en esteespacio representa valores para todos los índices de ciclo; el índice del ciclo más externo tieneel valor x 1, el índice del segundo ciclo tiene el valor x 2, y así en lo sucesivo. El índice del ciclomás interno tiene el valor xd.Pero no todos los puntos en este espacio representan combinaciones de índices que ocurren realmente durante la ejecución de anidamientos de ciclos. Como una función afín de losíndices de los ciclos externos, cada límite de ciclo inferior y superior define una desigualdadque divide el espacio de iteraciones en dos medios espacios: aquellas que son iteraciones en elciclo (el medio espacio positivo) y las que no lo son (el medio espacio negativo). La conjunción(AND lógico) de todas las igualdades lineales representa la intersección de los medios espaciospositivos, lo cual define a un poliedro convexo, que llamaremos el espacio de iteraciones parael anidamiento de ciclos. Un poliedro convexo tiene la propiedad de que si hay dos puntos en el11.3 Espacios de iteraciones 789Maq. Cap_11_AHO A.indd 789 11/10/07 8:19:06 PM790 Capítulo 11. Optimización para el paralelismo y la localidad(a) Un ciclo while con límites evidentes.<algunas instrucciones que no implican a i><algunas instrucciones>(b) No está claro cuándo termina este ciclo, o si alguna vez lo hará.<algunas instrucciones que no implican a i o a n>(c) No sabemos el valor de n, por lo que nosabemos cuándo termina este ciclo.Figura 11.9: Algunos ciclos whilepoliedro, todos los puntos en la línea entre ellos también se encuentran en el poliedro. Todaslas iteraciones en el ciclo se representan mediante los puntos con las coordenadas enteras quese encuentran dentro del poliedro descrito por las desigualdades de los límites del ciclo. Y porel contrario, todos los puntos dentro del poliedro representan iteraciones del anidamiento deciclos en cierto momento.Figura 11.10: Un anidamiento de ciclos bidimensionalEjemplo 11.6: Considere el anidamiento de ciclos de 2 dimensiones de la figura 11.10. Podemos modelar este anidamiento de ciclos de 2 niveles mediante el poliedro bidimensional quese muestra en la figura 11.11. Los dos ejes representan los valores de los índices de ciclo i y j.El índice i puede recibir cualquier valor entero entre 0 y 5; el índice j puede recibir cualquiervalor tal que i ≤ j ≤ 7. ✷Maq. Cap_11_AHO A.indd 790 11/10/07 8:19:06 PMij0 << 5< j < 7ii jiFigura 11.11: El espacio de iteraciones del ejemplo 11.6Espacios de iteraciones y accesos a arreglosEn el código de la figura 11.10, el espacio de iteraciones también es la parte del arreglo Aa la que accede el código. Ese tipo de acceso, en donde los índices de arreglo son tambiéníndices de ciclo en cierto orden, es muy común. Sin embargo, no debemos confundir elespacio de las iteraciones, cuyas dimensiones son índices de ciclo, con el espacio de datos.Si hubiéramos utilizado en la figura 11.10 un acceso a arreglo como A[2∗i, i + j] en vezde A[i, j], la diferencia habría sido aparente.11.3.2 Orden de ejecución para los anidamientos de ciclosUna ejecución secuencial de un anidamiento de ciclos recorre las iteraciones en su espacio deiteraciones, en orden lexicográfico ascendente. Un vector i = [i0, i1,…, i n] es lexicográficamentemenor que otro vector i  = [i0, i1,…, in], lo cual se escribe como i ≺ i , y si sólo existe un valor de m < mín(n, n ) tal que [i 0, i 1,…, i m ] = [i0, i1,…, im ] y que i m + 1 < im+ 1 . Observe quem = 0 es posible, y de hecho común.Ejemplo 11.7: Con i como el ciclo externo, las iteraciones en el anidamiento de ciclos delejemplo 11.6 se ejecutan en el orden mostrado en la figura 11.12. ✷11.3.3 Formulación de matrices de desigualdadesLas iteraciones en un ciclo de n niveles puede representarse en forma matemática como: {i en Z d | Bi + b ≥ 0} (11.1)11.3 Espacios de iteraciones 791Maq. Cap_11_AHO A.indd 791 11/10/07 8:19:07 PM792 Capítulo 11. Optimización para el paralelismo y la localidadFigura 11.12: Orden de iteración para el anidamiento de ciclos de la figura 11.10Aquí,1. Z, como es convencional en matemáticas, representa al conjunto de enteros: positivos,negativos y cero.2. B es una matriz entera de d × d.3. b es un vector entero de longitud d.4. 0 es un vector de d 0’s.Ejemplo 11.8: Podemos escribir las desigualdades del ejemplo 11.6 como en la figura 11.13.Es decir, el rango de i se describe mediante i ≥ 0 y i ≤ 5; el rango de j se describe mediantej ≥ i y j ≤ 7. Debemos colocar cada una de estas desigualdades en la forma ui + vj + w ≥ 0.Así, [u, v ] se convierte en una fila de la matriz B en la desigualdad (11.1), y w se convierteen el componente correspondiente del vector b. Por ejemplo, i ≥ 0 es de esta forma, con u = 1,v = 0 y w = 0. Esta desigualdad se representa mediante la primera fila de B y el elementosuperior de b en la figura 11.13.Figura 11.13: La multiplicación de matriz-vector y una desigualdad de vectores representa lasdesigualdades que definen un espacio de iteracionesComo otro ejemplo, la desigualdad i ≤ 5 es equivalente a (−1)i + (0)j + 5 ≥ 0, y se representamediante la segunda fila de B y b en la figura 11.13. Además, j ≥ i se convierte en (−1)i + (1)j +0 ≥ 0 y se representa mediante la tercera fila. Por último, j ≤ 7 se convierte en (0)i + (−1)j + 7≥ 0 y es la última fila de la matriz y el vector. ✷Maq. Cap_11_AHO A.indd 792 11/10/07 8:19:08 PMManipulación de desigualdadesPara convertir las desigualdades, como en el ejemplo 11.8, podemos realizar transformaciones en forma muy parecida a lo que hacemos con las desigualdades; por ejemplo,sumar o restar de ambos lados, o multiplicar ambos lados por una constante. La únicaregla especial que debemos recordar es que, cuando multiplicamos ambos lados por unnúmero negativo, tenemos que invertir la dirección de la desigualdad. Por ende, i ≤ 5,multiplicada por –1, se convierte en –i ≥ –5. Al sumar 5 a ambos lados obtenemos –i +5 ≥ 0, que en esencia es la segunda fila de la figura 11.13.11.3.4 Incorporación de constantes simbólicasAlgunas veces debemos optimizar un anidamiento de ciclos que involucra a ciertas variablesque son invariantes de ciclo para todos los ciclos en el anidamiento. A dichas variables les podemos llamar constantes simbólicas, pero para describir los límites de un espacio de iteracionestenemos que tratarlas como variables y crear una entrada para ellas en el vector de índices deciclo; es decir, el vector i en la formulación general de desigualdades (11.1).Ejemplo 11.9: Considere el siguiente ciclo simple:Este ciclo define un espacio de iteraciones unidimensional, con el índice i, delimitado por i ≥ 0e i ≤ n. Como n es una constante simbólica, debemos incluirla como una variable, lo cualnos proporciona un vector de índices de ciclo [i, n]. En forma de matriz-vector, este espacio deiteraciones se define mediante lo siguiente:enObserve que, aunque el vector de índices de arreglo tiene dos dimensiones, sólo la primera deéstas, que representa a i, forma parte de la salida: el conjunto de puntos que yacen con el espacio de iteraciones. ✷11.3.5 Control del orden de ejecuciónLas desigualdades lineales extraídas de los límites inferior y superior de un cuerpo de ciclodefinen a un conjunto de iteraciones sobre un poliedro convexo. Como tal, la representaciónno asume un orden de ejecución entre las iteraciones dentro del espacio de iteraciones. Elprograma original impone un orden secuencial en las iteraciones, que viene siendo el ordenlexicográfico con respecto a las variables índice de ciclo, ordenadas desde el más externo hasta11.3 Espacios de iteraciones 793Maq. Cap_11_AHO A.indd 793 11/10/07 8:19:09 PM794 Capítulo 11. Optimización para el paralelismo y la localidadel más interno. Sin embargo, las iteraciones en el espacio pueden ejecutarse en cualquier orden,siempre y cuando se respeten sus dependencias de datos (es decir, no cambia el orden en elque se llevan a cabo las escrituras y lecturas de cualquier elemento de un arreglo mediante lasdiversas instrucciones de asignación dentro del anidamiento de ciclos).Es difícil elegir un ordenamiento que respete las dependencias de datos y optimice para lalocalidad de los datos y el paralelismo, y lo trataremos más adelante, empezando en la sección11.7. Aquí supondremos que se da un ordenamiento válido y conveniente, y mostraremos cómogenerar código para asegurar que se cumpla ese orden. Vamos a empezar por mostrar un ordenamiento alternativo para el ejemplo 11.6.Ejemplo 11.10: No hay dependencias entre las iteraciones en el programa del ejemplo 11.6.Por lo tanto, podemos ejecutar las iteraciones en orden arbitrario, en secuencia o en formaconcurrente. Como la iteración [i, j] accede al elemento Z [i, j] en el código, el programa originalvisita el arreglo en el orden de la figura 11.14(a). Para mejorar la localidad espacial, preferimos visitar palabras contiguas en el arreglo en forma consecutiva, como en la figura 11.14(b).Este patrón de acceso se obtiene si ejecutamos las iteraciones en el orden mostrado en lafigura 11.14(c). Es decir, en vez de recorrer el espacio de iteraciones en la figura 11.11 en forma horizontal, lo recorremos en forma vertical, por lo que j se convierte en el índice del cicloexterno. El código que ejecuta las iteraciones en el orden anterior es:✷Dado un poliedro convexo y un ordenamiento de las variables índice, ¿cómo generamos loslímites de ciclo que recorren el espacio en orden lexicográfico de las variables? En el ejemploanterior, la restricción i ≤ j se muestra como un límite inferior para el índice j en el ciclo interno del programa original, pero como un límite superior para el índice i, de nuevo en el ciclointerno, en el programa transformado.Los límites del ciclo más externo, expresados como combinaciones lineales de constantes simbólicas y constantes, definen el rango de todos los valores posibles que puede tomar. Los límitespara las variables del ciclo interno se expresan como combinaciones lineales de variables índice delciclo externo, constantes simbólicas y constantes. Definen el rango que puede tomar la variablepara cada combinación de valores en las variables del ciclo externo.ProyecciónHablando en sentido geométrico, podemos encontrar los límites de ciclo del índice del cicloexterno en un anidamiento de ciclos con dos niveles, proyectando el poliedro convexo que representa el espacio de iteraciones hacia la dimensión externa del espacio. La proyección de unpoliedro en un espacio con menores dimensiones es, por intuición, la sombra que proyecta elobjeto hacia ese espacio. La proyección del espacio de iteraciones bidimensional en la figura11.11 hacia el eje i es la línea vertical de 0 a 5; y la proyección hacia el eje j es la línea horizontalMaq. Cap_11_AHO A.indd 794 11/10/07 8:19:10 PM(a) Orden de acceso original.(b) Orden preferido de acceso.(c) Orden preferido de iteraciones.Figura 11.14: Reordenamiento de los accesos e iteraciones para un anidamiento de ciclos11.3 Espacios de iteraciones 795Maq. Cap_11_AHO A.indd 795 11/10/07 8:19:11 PM796 Capítulo 11. Optimización para el paralelismo y la localidadde 0 a 7. Cuando proyectamos un objeto tridimensional a lo largo del eje z hacia un plano x yy bidimensional, eliminamos la variable z, perdiendo la altura de los puntos individuales y sóloregistrando la huella bidimensional del objeto en el plano x -y.La generación de límites de ciclo es sólo uno de los diversos usos de la proyección. La proyección puede definirse formalmente de la siguiente manera. Suponga que S es un poliedron-dimensional. La proyección de S hacia la primera m de sus dimensiones es el conjunto depuntos (x1, x2,…, x m) tales que para ciertos valores x m + 1, x m + 2,…, x n, el vector [x1, x 2,…, x n]está en S. Podemos calcular la proyección mediante el uso de la eliminación de Fourier-Motzkin, como se muestra a continuación:Algoritmo 11.11: Eliminación de Fourier-Motzkin.ENTRADA: Un poliedro S con las variables x1, x 2,…, x n. Es decir, S es un conjunto de restricciones lineales que involucran a las variables x i . Una variable xm dada se especifica como lavariable a eliminar.SALIDA: Un poliedro S con las variables x 1,…, xm− 1, xm + 1,…, xn (es decir, todas las variablesde S excepto x m) que es la proyección de S en otras dimensiones distintas de la m-ésima.MÉTODO: Haga que C sea todas las restricciones en S que involucran a x m . Haga lo siguiente:1. Para cada par de un límite inferior y un límite superior sobre x m en C, tal como: cree la nueva restricciónObserve que c1 y c2 son enteros, pero L y U pueden ser expresiones con variables distintas a x m .2. Si los enteros c1 y c 2 tienen un factor común, divida ambos lados entre ese factor.3. Si la nueva restricción no puede cumplirse, entonces no hay solución para S; es decir, lospoliedros S y S son espacios vacíos.4. S es el conjunto de restricciones S − C, más todas las restricciones generadas en el paso 2.Por cierto, tenga en cuenta que si x m tiene u límites inferiores y v límites superiores, al eliminarx m se producen hasta uv desigualdades, pero no más. ✷Las restricciones agregadas en el paso (1) del Algoritmo 11.11 corresponden a las complicaciones de las restricciones C en el resto de las variables en el sistema. Por lo tanto, hay unaMaq. Cap_11_AHO A.indd 796 11/10/07 8:19:13 PMsolución en S si, y sólo si existe por lo menos una solución correspondiente en S. Dada una solución en S, puede encontrarse el rango de la x m correspondiente al sustituir todas las variables,excepto x m, en las restricciones C por sus valores actuales.Ejemplo 11.12: Considere las desigualdades que definen el espacio de iteraciones de la figura11.11. Suponga que deseamos utilizar la eliminación de Fourier-Motzkin para proyectar el espaciobidimensional lejos de la dimensión i y hacia la dimensión j. Hay un límite inferior sobre i: 0 ≤ iy dos límites superiores: i ≤ j e i ≤ 5. Esto genera dos restricciones: 0 ≤ j y 0 ≤ 5. Esta última espor lo común verdadera, y puede ignorarse. La primera proporciona el límite inferior sobre j, y ellímite superior original j ≤ 7 proporciona el límite superior. ✷Generación de límites de cicloAhora que hemos definido la eliminación de Fourier-Motzkin, el algoritmo para generar los límites de ciclo para iterar a través de un poliedro convexo (Algoritmo 11.13) es simple. Calculamoslos límites de ciclo en orden, desde el más interno hasta los ciclos externos. Todas las desigualdades que involucran a las variables índice del ciclo más interno se escriben como los límitesinferior o superior de la variable. Después proyectamos hacia fuera la dimensión que representael ciclo más interno y obtenemos un poliedro con una dimensión menos. Repetimos hasta que sehayan encontrado todos los límites para todas las variables índice del ciclo.Algoritmo 11.13: Cálculo de los límites para un orden dado de variables.ENTRADA: Un poliedro convexo S sobre las variables v1,…, vn.SALIDA: Un conjunto de límites inferiores Li y límites superiores Ui para cada vi, expresadasólo en términos de las vj, para j < i.MÉTODO: El algoritmo se describe en la figura 11.15. ✷Ejemplo 11.14: Aplicamos el Algoritmo 11.13 para generar los límites de ciclo que recorrenel espacio de iteraciones de la figura 11.11 en sentido vertical. Las variables se ordenan como j,i. El algoritmo genera estos límites:Li : 0Ui : 5, jLj : 0Uj : 7Debemos satisfacer todas las restricciones, por lo que el límite sobre i es mín(5, j). No hayredundancias en este ejemplo. ✷11.3 Espacios de iteraciones 797Maq. Cap_11_AHO A.indd 797 11/10/07 8:19:13 PM798 Capítulo 11. Optimización para el paralelismo y la localidad Sn = S; /* Usar el Algoritmo 11.11 para encontrar los límites */ for ( i = n; i ≥ 1; i − − ) {Lvi = todos los límites inferiores sobre vi en Si;Uvi = todos los límites superiores sobre vi en Si;Si− 1 = Restricciones devueltas al aplicar el Algoritmo 11.11para eliminar vi de las restricciones Si; } /* Eliminar redundancias */ S = ∅; for ( i = 1; i ≤ n; i + + ) { Eliminar límites en Lvi y Uvi, implicados por S;Agregar las restricciones restantes de Lvi y Uvi sobre vi a S; }Figura 11.15: Código para expresar los límites de las variables con respecto a un ordenamientodado de las variablesFigura 11.16: Ordenamiento por diagonales del espacio de iteraciones de la figura 11.1111.3.6 Cambio de ejesHay que tener en cuenta que los recorridos horizontal y vertical del espacio de iteraciones, comovimos antes, son sólo dos de las formas más comunes de visitar el espacio de iteraciones. Existenmuchas otras posibilidades; por ejemplo, podemos recorrer el espacio de iteraciones en el ejemplo 11.6 diagonal por diagonal, como se describe a continuación en el ejemplo 11.15.Ejemplo 11.15: Podemos recorrer el espacio de iteraciones mostrado en la figura 11.11 ensentido diagonal, usando el orden que se muestra en la figura 11.16. La diferencia entre lascoordenadas j e i en cada diagonal es una constante, que empieza con 0 y termina con 7. Así,definimos una nueva variable k = j − i y recorremos el espacio de iteraciones en orden lexicográfico con respecto a k y j. Si sustituimos i = j − k en las desigualdades obtenemos:Maq. Cap_11_AHO A.indd 798 11/10/07 8:19:14 PMPara crear los límites de ciclo para el orden antes descrito, podemos aplicar el Algoritmo 11.13al conjunto anterior de desigualdades con el ordenamiento de variables k, j.De estas desigualdades generamos el siguiente código, sustituyendo i por j − k en los accesosal arreglo.✷En general, podemos cambiar los ejes de un poliedro creando nuevas variables índice deciclo que representen combinaciones afines de las variables originales, y definiendo un ordensobre esas variables. Lo difícil está en elegir los ejes correctos para satisfacer las dependenciasde datos y lograr al mismo tiempo los objetivos de paralelismo y localidad. En la sección 11.7hablaremos sobre este problema. Lo que hemos establecido aquí es que, una vez que se elijenlos ejes, es muy sencillo generar el código deseado, como se muestra en el ejemplo 11.15.Hay muchos otros órdenes de recorrido de iteraciones que esta técnica no maneja. Por ejemplo, tal vez queramos visitar todas las filas impares en un espacio de iteraciones antes de visitarlas filas pares. O tal vez queramos empezar con las iteraciones en la parte media del espaciode iteraciones, y avanzar hacia los contornos. Sin embargo, para las aplicaciones que tienenfunciones de acceso afines, las técnicas aquí descritas cubren la mayoría de los ordenamientosde iteraciones convenientes.11.3.7 Ejercicios para la sección 11.3Ejercicio 11.3.1: Convierta cada uno de los siguientes ciclos a una forma en la que cada unode los índices de ciclo se incremente en 1:Ejercicio 11.3.2: Dibuje o describa los espacios de iteraciones para cada uno de los siguientesanidamientos de ciclos:a) El anidamiento de ciclos de la figura 11.17(a).b) El anidamiento de ciclos de la figura 11.17(b). c) El anidamiento de ciclos de la figura 11.17(c).11.3 Espacios de iteraciones 799Maq. Cap_11_AHO A.indd 799 11/10/07 8:19:15 PM800 Capítulo 11. Optimización para el paralelismo y la localidad(a) Anidamiento de ciclos para el ejercicio 11.3.2(a).(b) Anidamiento de ciclos para el ejercicio 11.3.2(b).(c) Anidamiento de ciclos para el ejercicio 11.3.2(c).Figura 11.17: Anidamientos de ciclos para el ejercicio 11.3.2Ejercicio 11.3.3: Escriba las restricciones implicadas por cada uno de los anidamientos deciclos de la figura 11.17 en la forma de la ecuación (11.1). Es decir, proporcione los valores de losvectores i y b y de la matriz B.Ejercicio 11.3.4: Invierta cada uno de los órdenes de anidamiento de ciclos para los anidamientos de la figura 11.17.Ejercicio 11.3.5: Use el algoritmo de eliminación de Fourier-Motzkin para eliminar i de cadauno de los conjuntos de restricciones obtenidos en el ejercicio 11.3.3.Ejercicio 11.3.6: Use el algoritmo de eliminación de Fourier-Motzkin para eliminar j de cadauno de los conjuntos de restricciones obtenidos en el ejercicio 11.3.3.Ejercicio 11.3.7: Para cada uno de los anidamientos de ciclos de la figura 11.17, modifique elcódigo de manera que se sustituya el eje i por la diagonal mayor; es decir, la dirección del ejeestá caracterizada por i = j. El nuevo eje debe corresponder al ciclo más externo.Ejercicio 11.3.8: Repita el ejercicio 11.3.7 para los siguientes cambios de ejes:a) Sustituya i por i + j; es decir, la dirección del eje consiste en las líneas para las cualesi + j es una constante. El nuevo eje corresponde al ciclo más externo.b) Sustituya j por i − 2j. El nuevo eje corresponde al ciclo más externo.Maq. Cap_11_AHO A.indd 800 11/10/07 8:19:16 PMEjercicio 11.3.9: Haga que A, B y C sean constantes enteras en el siguiente ciclo, con C > 1y B > A;Vuelva a escribir el ciclo de manera que el incremento de la variable de ciclo sea 1 y que la inicialización sea a 0; es decir, que sea de la forma:para los enteros D, E y F. Exprese D, E y F en términos de A, B y C.Ejercicio 11.3.10: Par un anidamiento genérico de dos ciclos:en donde desde A hasta E son constantes enteras, escriba las restricciones que definen el espacio de iteraciones del anidamiento de ciclos en forma matriz-vector; es decir, en la formaBi + b = 0.Ejercicio 11.3.11: Repita el ejercicio 11.3.10 para un anidamiento genérico de dos ciclos conlas constantes enteras simbólicas m y n, como en:Como antes, A, B y C representan constantes enteras específicas. Sólo i, j, m y n deben mencionarse en el vector de valores desconocidos. Además, recuerde que sólo i y j son variables desalida para la expresión.11.4 Índices de arreglos afinesEl enfoque de este capítulo es sobre la clase de accesos a arreglos afines, en donde cada índicede arreglo se expresa en forma de expresiones afines de índices de ciclo y constantes simbólicas. Las funciones afines proporcionan una asignación sintetizada del espacio de iteraciones alespacio de datos, ayudando a determinar qué iteraciones se asignan a los mismos datos o a lamisma línea de caché.Así como los límites afines superior e inferior de un ciclo pueden representarse como uncálculo de matriz-vector, podemos hacer lo mismo para las funciones de acceso afines. Una vezcolocadas en la forma matriz-vector, podemos aplicar el álgebra lineal estándar para encontrarla información pertinente, como las dimensiones de los datos a los que se accedió, y qué iteraciones se refieren a los mismos datos.11.4 Índices de arreglos afi nes 801!Maq. Cap_11_AHO A.indd 801 11/10/07 8:19:17 PM802 Capítulo 11. Optimización para el paralelismo y la localidad11.4.1 Accesos afinesDecimos que el acceso a un arreglo en un ciclo es afín si:1. Los límites del ciclo se expresan como expresiones afines de las variables y constantessimbólicas del ciclo circundante.2. El índice para cada dimensión del arreglo es también una expresión afín de variables yconstantes simbólicas del ciclo circundante.Ejemplo 11.16: Suponga que i y j son variables índices de ciclo delimitadas por expresiones afines. Algunos ejemplos de accesos a arreglos afines son Z [i ], Z [i + j + 1], Z [0], Z [i, i ] yZ[2 ∗ i + 1, 3 ∗ j − 10]. Si n es una constante simbólica para un anidamiento de ciclos, entonces Z [3 ∗ n, n − j ] es otro ejemplo de un acceso afín a un arreglo. Sin embargo, Z [i ∗ j ] yZ [n ∗ j ] no son accesos afines. ✷Cada acceso afín a un arreglo puede describirse mediante dos matrices y dos vectores. El primer par matriz-vector es el de B y b, que describen el espacio de iteraciones para el acceso, comoen la desigualdad de la Ecuación (11.1). El segundo par que, por lo general, conocemos como Fy f, representa la(s) función(es) de las variables índice de ciclo que producen el(los) índice(s) dearreglo utilizado(s) en las diversas dimensiones del acceso al arreglo.De manera formal, representamos a un acceso al arreglo en un anidamiento de ciclos queutiliza un vector de variables índice i mediante el cuadrado F = F, f, B, b	; asigna un vector identro de los límites:Bi + b ≥ 0a la siguiente ubicación del elemento del arreglo:Fi + fEjemplo 11.17: En la figura 11.18 hay algunos accesos comunes a arreglos, expresados en notación de matrices. Los dos índices de ciclo son i y j, y éstos forman el vector i. Además, X, Yy Z son arreglos con 1, 2 y 3 dimensiones, respectivamente.El primer acceso, A[i − 1], se representa mediante una matriz F de 1 × 2 y un vector fde longitud 1. Observe que cuando realizamos la multiplicación matriz-vector y le sumamosel vector f, nos quedamos con una sola función, i − 1, que es exactamente la fórmula para elacceso al arreglo unidimensional X. Observe además el tercer acceso, Y[j, j + 1], que despuésde la multiplicación y suma de matriz-vector, produce un par de de funciones, ( j, j + 1). Éstosson los índices de las dos dimensiones del acceso al arreglo.Por último, observemos el cuarto acceso Y[1, 2]. Este acceso es una constante, y no es desorprender que la matriz F conste sólo de 0s. Por ende, el vector de los índices de ciclo, i, noaparece en la función de acceso. ✷Maq. Cap_11_AHO A.indd 802 11/10/07 8:19:18 PMACCESO EXPRESIÓN AFÍNFigura 11.18: Algunos accesos a arreglos y sus representaciones matriz - vector11.4.2 Accesos afines y no afines en la prácticaHay ciertos patrones de acceso de datos comunes que se encuentran en los programas numéricosque no pueden ser afines. Los programas que involucran matrices poco densas son un ejemploimportante. Una representación popular para las matrices poco densas es almacenar sólo loselementos distintos de cero en un vector, y se utilizan arreglos de índices auxiliares para marcaren dónde empieza una fila y cuáles columnas contienen valores distintos de cero. Los accesosindirectos a los arreglos se utilizan para acceder a dichos datos. Un acceso de este tipo, comoX[Y[i]], es un acceso no afín al arreglo X. Si la poca densidad es regular, como en las matricesen banda que tienen valores distintos de cero sólo alrededor de la diagonal, entonces puedenusarse arreglos densos para representar las subregiones con elementos distintos de cero. En esecaso, los accesos pueden ser afines.Otro ejemplo común de accesos no afines son los arreglos linealizados. Algunas veces, losprogramadores utilizan un arreglo lineal para almacenar un objeto lógicamente multidimensional. Una razón del por qué éste es el caso es que las dimensiones del arreglo tal vez no seconozcan en tiempo de compilación. Un acceso que, por lo general, se vería como Z[i, j], se expresaría como Z[i ∗ n + j], que es una función cuadrática. Podemos convertir el acceso lineal11.4 Índices de arreglos afi nes 803Maq. Cap_11_AHO A.indd 803 11/10/07 8:19:19 PM804 Capítulo 11. Optimización para el paralelismo y la localidaden un acceso multidimensional, si cada acceso puede descomponerse en dimensiones separadascon la garantía de que ninguno de sus componentes excederá su límite. Por último, observamosque los análisis de las variables de inducción pueden usarse para convertir ciertos accesos noafines en afines, como se muestra en el ejemplo 11.18.Ejemplo 11.18: Podemos rescribir el siguiente código:comopara hacer que el acceso a la matriz Z sea afín. ✷11.4.3 Ejercicios para la sección 11.4Ejercicio 11.4.1: Para cada uno de los siguientes accesos a un arreglo, proporcione el vector fy la matriz F que los describe. Suponga que el vector de índices i es i, j,…, y que todos los índicesde ciclo tienen límites afines.11.5 Reutilización de datosDe las funciones de acceso a arreglos derivamos dos tipos de información útiles para la optimización de la localidad y la paralelización:1. Reutilización de datos : para la optimización de la localidad, deseamos identificar conjuntos de iteraciones que accedan a los mismos datos o la misma línea de caché.2. Dependencia de datos : para que la paralelización y las transformaciones de ciclo de localidad sean correctas, debemos identificar todas las dependencias de datos en el código.Recuerde que dos accesos (no necesariamente distintos) tienen una dependencia de datossi las instancias de los accesos pueden referirse a la misma ubicación de memoria, y porlo menos uno de ellos es una escritura.Maq. Cap_11_AHO A.indd 804 11/10/07 8:19:19 PMEn muchos casos, cada vez que identificamos iteraciones que reutilizan los mismos datos, haydependencias de datos entre ellos.Cada vez que hay una dependencia de datos, es obvio que se reutilizan los mismos datos. Porejemplo, en la multiplicación de matrices, el mismo elemento en el arreglo de salida se escribeO(n) veces. Las operaciones de escritura deben ejecutarse en el orden de ejecución original; 3hay reutilización debido a que podemos asignar el mismo elemento a un registro.Sin embargo, no toda la reutilización puede explotarse en las optimizaciones de localidad;he aquí un ejemplo que ilustra esta cuestión.Ejemplo 11.19: Considere el siguiente ciclo:Observamos que el ciclo escribe a una ubicación distinta en cada iteración, por lo que no hayreutilizaciones ni dependencias en las distintas operaciones de escritura. Sin embargo, el ciclolee las ubicaciones 5, 8, 11, 14, 17,…, y escribe en las ubicaciones 3, 10, 17, 24,…. Las iteraciones de lectura y escritura acceden a los mismos elementos 17, 38 y 59, y así sucesivamente.Es decir, los enteros de la forma 17 + 21j para j = 0, 1, 2,… son todos aquellos enteros quepueden escribirse como 7i 1 = 3 y como 3i 2 + 5, para algunos enteros i 1 e i 2. Sin embargo, estareutilización ocurre raras veces y, cuando ocurre, no puede explotarse con facilidad. ✷La dependencia de datos es diferente del análisis de la reutilización en que uno de los accesos que comparten una dependencia de datos debe ser un acceso de escritura. Lo más importante es que la dependencia de datos debe ser tanto correcta como precisa. Tiene que encontrartodas las dependencias en búsqueda de exactitud, y no debe encontrar dependencias ilegítimas,ya que pueden provocar una serialización innecesaria.Con la reutilización de datos, sólo debemos buscar en dónde se encuentra la mayoría de lasreutilizaciones explotables. Esto es mucho más simple, por lo que trataremos este tema en estasección, y manejaremos las dependencias de datos en la siguiente. Simplificamos el análisis dela reutilización al ignorar los límites de ciclo, ya que pocas veces cambian la forma de la reutilización. La mayor parte de la reutilización que puede explotarse mediante el particionamientoafín reside entre las instancias de los mismos accesos a arreglos, y los accesos que compartenla misma matriz de coeficientes (que, por lo general, llamamos F en la función de índices afín).Como se muestra en párrafos anteriores, los patrones de acceso como 7i + 3 y 3i + 5 no tienenreutilización de interés.11.5.1 Tipos de reutilizaciónEmpezaremos primero con el ejemplo 11.20 para ilustrar los distintos tipos de reutilizaciones dedatos. A continuación, debemos diferenciar entre el acceso como una instrucción en un programa;11.5 Reutilización de datos 8053 Aquí hay un punto delicado. Debido a la propiedad conmutativa de la suma, obtendríamos la misma respuestaa la suma sin importar el orden en el que la realizáramos. Sin embargo, este caso es muy especial. En general, es demasiado complejo para que el compilador determine qué cálculo se está realizando mediante una secuencia de pasosaritméticos seguidos de escrituras, y no podemos confiar en que haya reglas algebraicas que nos ayuden a reordenarlos pasos con seguridad.Maq. Cap_11_AHO A.indd 805 11/10/07 8:19:20 PM806 Capítulo 11. Optimización para el paralelismo y la localidadpor ejemplo, x = Z[i, j], de la ejecución de esta instrucción muchas veces, a medida que ejecutamos el anidamiento de ciclos. Por cuestión de énfasis, podemos referirnos a la misma instruccióncomo un acceso estático, mientras que las diversas iteraciones de la instrucción, a medida queejecutamos su anidamiento de ciclos, se conocen como accesos dinámicos.Las reutilizaciones pueden clasificarse como propias y de grupo. Si las iteraciones que reutilizan los mismos datos provienen del mismo acceso estático, nos referimos a la reutilizacióncomo propia; si provienen de distintos accesos, nos referimos a la reutilización como de grupo.La reutilización es temporal si se hace referencia a la misma ubicación exacta; es espacial si sehace referencia a la misma línea de caché.Ejemplo 11.20: Considere el siguiente anidamiento de ciclos:Cada uno de los accesos Z[j], Z[j + 1] y Z[j + 2] tiene reutilización espacial propia, ya que lasiteraciones consecutivas del mismo acceso hacen referencia a elementos contiguos del arreglo.Es muy probable que los elementos contiguos residan en la misma línea de caché. Además, todos tienen reutilización temporal propia, ya que los elementos exactos se utilizan una y otra vezen cada iteración en el ciclo externo. Asimismo, todos tienen la misma matriz de coeficientes ypor ende, tienen reutilización de grupo. Hay una reutilización de grupo, tanto temporal comoespacial, entre los distintos accesos. Aunque hay 4n2 accesos en este código, si la reutilizaciónpuede explotarse, sólo debemos llevar cerca de n/c líneas de caché a la caché, en donde c esel número de palabras en una línea de caché. Dejamos un factor de n debido a la reutilizaciónespacial propia, un factor de c debido a la localidad espacial, y por último un factor de 4 debidoa la reutilización de grupo. ✷A continuación mostraremos cómo podemos usar el álgebra lineal para extraer la información de reutilización de los accesos afines a un arreglo. Nos interesa no sólo encontrar cuántosahorros potenciales hay, sino también qué iteraciones están reutilizando los datos, de forma quepodamos tratar de acercarlas entre sí para explotar la reutilización.11.5.2 Reutilización propiaPuede haber ahorros considerables en los accesos a memoria al explotar la reutilización propia.Si los datos referenciados por un acceso estático tienen k dimensiones y el acceso está anidadoen un ciclo con profundidad d, para cierta d > k, entonces los mismos datos pueden reutilizarse n d − k veces, en donde n es el número de iteraciones en cada ciclo. Por ejemplo, si un anidamiento de ciclos con 3 niveles accede a una columna de un arreglo, entonces hay un factor deahorro potencial de n2 accesos. Resulta que la dimensionalidad de un acceso corresponde alconcepto del rango de la matriz de coeficientes en el acceso, y podemos encontrar qué iteraciones se refieren a la misma ubicación, al buscar el espacio nulo de la matriz, como se explica acontinuación.Maq. Cap_11_AHO A.indd 806 11/10/07 8:19:21 PMRango de una matrizEl rango de una matriz F es el mayor número de columnas (o de manera equivalente, filas) deF que son linealmente independientes. Un conjunto de vectores es linealmente independiente sininguno de los vectores puede escribirse como una combinación lineal de un número finito demuchos otros vectores en el conjunto.Ejemplo 11.21: Considere la siguiente matriz:Observe que la segunda fila es la suma de las filas primera y tercera, mientras que la cuartafila es la tercera fila menos el doble de la primera. Sin embargo, las filas primera y tercera sonlinealmente independientes; ninguna es múltiplo de la otra. Por ende, el rango de la matriz es 2.También podríamos sacar esta conclusión al examinar las columnas. La tercera columna esel doble de la segunda columna menos la primera. Por otro lado, dos columnas cualesquiera sonlinealmente independientes. De nuevo, concluimos que el rango es 2. ✷Ejemplo 11.22: Vamos a analizar los accesos a los arreglos en la figura 11.18. El primer acceso, X[i − 1], tiene una dimensión de 1, ya que el rango de la matriz [1 0] es 1. Es decir, laúnica fila es linealmente independiente, al igual que la primera columna.El segundo acceso, Y[i, j ], tiene una dimensión de 2. La razón es que la matriz:tiene dos filas independientes (y, por lo tanto, dos columnas independientes, desde luego). Eltercer acceso, Y[ j, j + 1], es de dimensión 1, ya que la matriz:tiene un rango de 1. Observe que las dos filas son idénticas, por lo que sólo una es linealmenteindependiente. En forma equivalente, la primera columna es 0 veces la segunda columna, por loque las columnas no son independientes. Por intuición, en un arreglo Y grande y cuadrado, losúnicos elementos a los que se accede se encuentran a lo largo de una línea unidimensional, justoencima de la diagonal principal.El cuarto acceso, Y[1, 2] tiene dimensión 0, ya que una matriz que sólo tiene 0s tiene unrango de 0. Observe que para dicha matriz, no podemos encontrar una suma lineal ni siquierade una fila que sea distinta de cero. Finalmente, el último acceso Z[i, i, 2 ∗ i + j ], tiene una dimensión de 2. Observe que en la matriz para este acceso:11.5 Reutilización de datos 807Maq. Cap_11_AHO A.indd 807 11/10/07 8:19:22 PM808 Capítulo 11. Optimización para el paralelismo y la localidadlas últimas dos filas son linealmente independientes; ninguna es múltiplo de la otra. No obstante,la primera fila es una “suma” lineal de las otras dos filas, con ambos coeficientes 0. ✷Espacio nulo de una matrizUna referencia en un anidamiento de ciclos con d niveles y rango r accede a O(n r ) elementos dedatos en O(n d ) iteraciones, por lo que en promedio, O(n d − r ) iteraciones deben hacer referenciaal mismo elemento del arreglo. ¿Cuáles iteraciones acceden a los mismos datos? Suponga que unacceso en este anidamiento de ciclos se representa mediante una combinación matriz-vector Fy f. Suponga que i e i son dos iteraciones que hacen referencia al mismo elemento del arreglo.Entonces, Fi + f = Fi + f. Al reordenar los términos, obtenemos:F(i − i  ) = 0.Hay un concepto reconocido del álgebra lineal, que caracteriza cuando i e i cumplen conla ecuación anterior. El conjunto de todas las soluciones a la ecuación Fv = 0 se conoce comoel espacio nulo de F. Por ende, dos iteraciones se refieren al mismo elemento del arreglo si ladiferencia de sus vectores índice de ciclo pertenece al espacio nulo de la matriz F.Es fácil ver que el vector nulo, v = 0, siempre cumple con Fv = 0. Es decir, dos iteracionessin duda se refieren al mismo elemento del arreglo si su diferencia es 0; en otras palabras, si sonen realidad la misma iteración. Además, el espacio nulo es sin duda un espacio vectorial. Es decir,si Fv1 = 0 y Fv2 = 0, entonces F(v1 + v2) = 0 y F(cv1) = 0.Si la matriz F tiene un rango completo; es decir, si su rango es d, entonces el espacio nulo deF consiste sólo en el vector nulo. En ese caso, todas las iteraciones en un anidamiento de ciclosse refieren a datos distintos. En general, la dimensión del espacio nulo, también conocida como lanulidad, es d − r. Si d > r, entonces para cada elemento hay un espacio (d − r)-dimensional deiteraciones que acceden a ese elemento.El espacio nulo puede representarse mediante sus vectores básicos. Un espacio nulo k -dimensional se representa mediante k vectores independientes; cualquier vector que pueda expresarse como una combinación lineal de los vectores básicos pertenece al espacio nulo.Ejemplo 11.23: Vamos a reconsiderar la matriz del ejemplo 11.21:En ese ejemplo determinamos que el rango de la matriz es 2; por ende, la nulidad es 3 − 2 = 1.Para encontrar una base para el espacio nulo, que en este caso debe ser un solo vector distintode cero, de longitud 3, podemos suponer que un vector en el espacio nulo es [x, y, z ] y tratar deresolver la siguiente ecuación:Maq. Cap_11_AHO A.indd 808 11/10/07 8:19:22 PMSi multiplicamos las primeras dos filas por el vector de valores desconocidos, obtenemos las siguientes dos ecuaciones:Podríamos escribir también las ecuaciones que provienen de las tercera y cuarta filas, perocomo no hay tres filas linealmente independientes, sabemos que las ecuaciones adicionales noagregan nuevas restricciones sobre x, y y z. Por ejemplo, la ecuación que obtenemos de la tercera fila, 4x + 5y + 6z = 0, se puede obtener si restamos la primera ecuación de la segunda.Debemos eliminar todas las variables que podamos de las ecuaciones anteriores. Empiecepor usar la primera ecuación para resolver para x ; es decir, x = −2y − 3z. Después sustituyapara x en la segunda ecuación, para obtener −3y = 6z, o y = −2z. Como x = −2y − 3z, y y =−2z, resulta que x = z. Por ende, el vector [x, y, z ] es en realidad [z, −2z, z ]. Podemos elegir dez cualquier valor distinto de cero, para formar el único vector básico para el espacio nulo. Porejemplo, podemos elegir z = 1 y usar [1, −2, 1] como la base del espacio nulo. ✷Ejemplo 11.24: El rango, la nulidad y el espacio nulo para cada una de las referencias en elejemplo 11.7 se muestran en la figura 11.19. Observe que la suma del rango y la nulidad entodos los casos es la profundidad del anidamiento de ciclos, 2. Como los accesos Y[i, j] y Z[1, i,2 ∗ i + j] tienen un rango de 2, todas las iteraciones hacen referencia a distintas ubicaciones.Los accesos X[i − 1] y Y[j, j + 1] tienen matrices de rango 1, por lo que hay O(n) iteraciones que hacen referencia a la misma ubicación. En el caso anterior, hay filas enteras en elespacio de iteraciones que se refieren a la misma ubicación. En otras palabras, las iteracionesque sólo difieren en la dimensión j comparten la misma ubicación, que se representa en formasintetizada mediante la base del espacio nulo, [0, 1]. Para Y[j, j + 1], hay columnas enterasen el espacio de iteraciones que se refieren a la misma ubicación, y este hecho se representa enforma sintetizada mediante la base del espacio nulo, [1, 0].Por último, el acceso Y[1, 2] se refiere a la misma ubicación en todas las iteraciones. Elespacio nulo correspondiente tiene 2 vectores básicos, [0, 1], [1, 0], lo cual significa que todoslos pares de iteraciones en el anidamiento de ciclos se refieren exactamente a la misma ubicación. ✷11.5.3 Reutilización espacial propiaEl análisis de la reutilización espacial depende de la distribución de los datos de la matriz. En C,las matrices se distribuyen en orden por filas y en Fortran se distribuyen en orden por columnas.11.5 Reutilización de datos 809Maq. Cap_11_AHO A.indd 809 11/10/07 8:19:23 PM810 Capítulo 11. Optimización para el paralelismo y la localidadACCESO EXPRESIÓN AFÍN RANGO NULIDADBASE DELESPACIO NULOFigura 11.19: Rango y nulidad de los accesos afinesEn otras palabras, los elementos de un arreglo X[i, j ] y X[i, j + 1] son contiguos en C; X[i, j] yX[i + 1, j] son contiguos en Fortran. Sin perder la generalidad, en el resto del capítulo adoptaremos la distribución de los arreglos en C (orden por filas). Como una primera aproximación, vamos a considerar dos elementos de un arreglo paracompartir la misma línea de caché, si y sólo si comparten la misma fila en un arreglo bidimensional. En forma más general, en un arreglo de d dimensiones tomamos elementos del arreglopara compartir una línea de caché, si sólo difieren en la última dimensión. Como para un arreglo y caché comunes, muchos elementos del arreglo pueden caber en una línea de caché, debehaber un aumento considerable en la velocidad al acceder a toda una fila completa en orden,aun cuando, hablando en sentido estricto, en ocasiones tenemos que esperar para cargar unanueva línea de caché.El truco para descubrir y aprovechar la reutilización espacial propia es retirar la últimafila de la matriz de coeficientes F. Si la matriz trunca resultante tiene un rango menor quela profundidad del anidamiento de ciclos, entonces podemos asegurar la localidad espacial, alverificar que el ciclo más interno sólo varíe la última coordenada del arreglo.Ejemplo 11.25: Considere el último acceso, Z[1, i, 2 ∗ i + j], en la figura 11.19. Si eliminamosla última fila, nos quedamos con la siguiente matriz trunca:Maq. Cap_11_AHO A.indd 810 11/10/07 8:19:24 PMEs evidente que el rango de esta matriz es 1, y como el anidamiento de ciclos tiene profundidad 2,existe la oportunidad de la reutilización espacial. En este caso, como j es el índice del ciclointerno, éste visita los elementos contiguos del arreglo Z, almacenados en orden por filas. Alhacer a i el índice del ciclo interior no se producirá una localidad espacial, ya que a medida quecambia i, cambian tanto la segunda como la tercera dimensión. ✷La regla general para determinar si hay reutilización espacial propia es la siguiente. Comosiempre, suponemos que los índices de ciclo corresponden a las columnas de la matriz de coeficientes en orden, con el ciclo más externo primero, y el ciclo más interno al último. Así, paraque pueda haber reutilización espacial, el vector [0, 0,…, 0, 1] debe estar en el espacio nulode la matriz trunca. La razón es que si este vector está en el espacio nulo, entonces al corregirtodos los índices de ciclo excepto el más interno, sabemos que todos los accesos dinámicos durante una ejecución a través del ciclo interno varían sólo en el último índice del arreglo. Si elarreglo se almacena en orden por filas, entonces estos elementos están cerca unos de otros, talvez en la misma línea de caché.Ejemplo 11.26: Observe que [0, 1] (transpuesto como un vector columna) está en el espacionulo de la matriz trunca del ejemplo 11.25. Por ende, como se mencionó ahí, esperamos quecon j como el índice del ciclo interno, haya localidad espacial. Por otro lado, si invertimos elorden de los ciclos, de manera que i sea el ciclo interno, entonces la matriz de coeficientes seconvierte en:Ahora, [0, 1] no está en el espacio nulo de esta matriz. En vez de ello, el espacio nulo se generamediante el vector básico [1, 0]. Entonces, como sugerimos en el ejemplo 11.25, no esperamoslocalidad espacial si i es el ciclo interno.No obstante, hay que observar que la prueba para [0, 0,…, 0, 1], estando en el espacio nulo,no es suficiente para asegurar la localidad espacial. Por ejemplo, suponga que el acceso no fueraZ[1, i, 2 ∗ i + j] sino Z[1, i, 2 ∗ i + 50 ∗ j]. Entonces, sólo se accedería a cada quincuagésimoelemento de Z durante una ejecución del ciclo interno, y no reutilizaríamos una línea de cachésino hasta que fuera lo bastante larga como para contener más de 50 elementos. ✷11.5.4 Reutilización de grupoCalculamos la reutilización de grupo sólo entre los accesos en un ciclo que comparte la mismamatriz de coeficientes. Dados dos accesos dinámicos Fi1 + f 1 y Fi2 + f 2, la reutilización de losmismos datos requiere que:Fi1 + f 1 = Fi2 + f 211.5 Reutilización de datos 811Maq. Cap_11_AHO A.indd 811 11/10/07 8:19:25 PM812 Capítulo 11. Optimización para el paralelismo y la localidadoF(i1 − i2) = (f2 − f1).Suponga que v es una solución a esta ecuación. Entonces, si w es cualquier vector en el espacio nulo de F1, w + v es también una solución, y de hecho ésas son todas las soluciones a la ecuación.Ejemplo 11.27: El siguiente anidamiento de ciclos de 2 niveles:tiene dos accesos al arreglo, Z[i, j] y Z[i − 1, j]. Observe que estos dos accesos se caracterizanpor la siguiente matriz de coeficientes:como el segundo acceso, Y[i, j] en la figura 11.19. Esta matriz tiene un rango de 2, por lo queno hay reutilización temporal propia.No obstante, cada acceso exhibe una reutilización espacial propia. Como se describe en lasección 11.5.3, cuando eliminamos la fila inferior de la matriz, nos quedamos sólo con la filasuperior, [1, 0], que tiene un rango de 1. Como [0, 1] está en el espacio nulo de esta matriztrunca, esperamos la reutilización espacial. A medida que cada incremento del índice j del ciclointerno incrementa el segundo índice del arreglo por uno, de hecho accedemos a los elementosadyacentes del arreglo, y utilizamos al máximo cada línea de caché.Aunque no hay una reutilización temporal propia para cualquiera de los accesos, observeque las dos referencias Z[i, j ] y Z[i −1, j] acceden casi al mismo conjunto de elementos delarreglo. Es decir, hay una reutilización temporal de grupo, ya que los datos leídos por el accesoZ[i − 1, j] son los mismos que escribe el acceso Z[i, j], excepto para el caso en el que i = 1.Este patrón simple se aplica a todo el espacio de iteraciones completo y puede explotarse paramejorar la localidad de los datos en el código. De manera formal, si descontamos los límites deciclo, los dos accesos Z[i, j] y Z[i − 1, j] se refieren a la misma ubicación en las iteraciones (i1, j1)y (i2, j 2), respectivamente, siempre y cuando:Si rescribimos los términos, obtenemos lo siguiente:Es decir, j1 = j2 e i2 = i 1 + 1.Observe que la reutilización ocurre a lo largo del eje i del espacio de iteraciones. Es decir,la iteración (i 2, j 2) ocurre n iteraciones (del ciclo interior) después de la iteración (i 1, j 1). PorMaq. Cap_11_AHO A.indd 812 11/10/07 8:19:25 PMende, se ejecutan muchas iteraciones antes de reutilizar los datos escritos. Estos datos puedenseguir o no en la caché. Si la caché puede guardar dos filas consecutivas de la matriz Z, entoncesel acceso Z[i − 1, j] no falla en la caché, y el número total de fallos de caché para todo el anidamiento de ciclos completo es n 2/c, en donde c es el número de elementos por línea de caché.En caso contrario habrá el doble de fallos, ya que ambos accesos estáticos requieren una nuevalínea de caché para cada c accesos dinámicos. ✷Ejemplo 11.28: Suponga que hay dos accesos:A[i, j, i + j] y A[i + 1, j − 1, i + j]en un anidamiento de ciclos de 3 niveles, con los índices i, j y k del ciclo externo al interno. Entonces, dos accesos i 1 = [i 1, j 1, k 1] e i 2 = [i 2, j 2, k 2] reutilizan el mismo elemento cada vez queUna solución a esta ecuación para un vector v = [i1 − i 2, j 1 − j 2, k 1 − k 2] es v = [1, −1, 0];es decir, i 1 = i 2 + 1, j 1 = j 2 − 1 y k 1 = k 2.4 Sin embargo, el espacio nulo de la matrizse genera mediante el vector básico [0, 0, 1]; es decir, el tercer índice de ciclo, k, puede ser arbitrario. Por ende, v, la solución a la anterior ecuación, es cualquier vector [1, −1, m] para cierta m.Dicho de otra forma, un acceso dinámico a A[i, j, i + j ], en un anidamiento de ciclos conlos índices i, j y k, se reutiliza no sólo por otros accesos dinámicos A[i, j, i + j] con los mismosvalores de i y j y un valor distinto de k, sino también por accesos dinámicos A[i + 1, j − 1, i + j]con los valores de índice de ciclo i + 1, j − 1 y cualquier valor de k. ✷Aunque no lo haremos aquí, podemos razonar acerca de la reutilización espacial de grupoen forma parecida. Al igual que con la explicación de la reutilización espacial propia, sólo eliminamos la última dimensión de toda consideración.La extensión de la reutilización es diferente para las distintas categorías de reutilización. Lareutilización temporal propia proporciona el mayor beneficio: una referencia con un espacio nulok-dimensional reutiliza los mismos datos O(n)k veces. El grado de reutilización espacial propiase limita en base a la longitud de la línea de caché. Por último, el grado de reutilización de grupo selimita en base al número de referencias en un grupo que comparte la reutilización.11.5 Reutilización de datos 8134 Es interesante observar que, aunque hay una solución en este caso, no habría solución si modificáramos uno delos terceros componentes de i + j a i + j + 1. Es decir, en el ejemplo que se da, ambos accesos entran en contacto conesos elementos del arreglo que se encuentran en el subespacio S bidimensional definido por “el tercer componente esla suma de los primeros dos componentes”. Si cambiáramos i + j a i + j + 1, ninguno de los elementos que hicierancontacto con el segundo acceso se encontrarían en S, y no habría ningún tipo de reutilización.Maq. Cap_11_AHO A.indd 813 11/10/07 8:19:27 PM814 Capítulo 11. Optimización para el paralelismo y la localidad11.5.5 Ejercicios para la sección 11.5Ejercicio 11.5.1: Calcule los rangos de cada una de las matrices en la figura 11.20. Proporcione un conjunto máximo de columnas linealmente dependientes y un conjunto máximo defilas linealmente dependientes.Figura 11.20: Calcule los rangos y espacios nulos de estas matricesEjercicio 11.5.2: Encuentre una base para el espacio nulo de cada matriz de la figura 11.20.Ejercicio 11.5.3: Suponga que el espacio de iteraciones tiene las dimensiones (variables) i, jy k. Para cada uno de los siguientes accesos, describa los subespacios que hacen referencia a lossiguientes elementos individuales del arreglo:Ejercicio 11.5.4: Suponga que el arreglo A se almacena en orden por filas y que se accede almismo dentro del siguiente anidamiento de ciclos:for (i = 0; i < 100; i++) for (j = 0; j < 100; j++) for (k = 0; k < 100; k++) <cierto acceso a A>Indique para cada uno de los siguientes accesos si es posible rescribir los ciclos, de manera que elacceso a A exhiba una reutilización espacial propia; es decir, que se utilicen todas las líneas de lacaché en forma consecutiva. Muestre cómo rescribir los ciclos, en caso de ser así. Nota: la rescritura de los ciclos puede implicar tanto el reordenamiento como la introducción de nuevos índicesde ciclo. Sin embargo, no puede modificar la distribución del arreglo; por ejemplo, cambiándoloal orden por columnas. Además, tenga en cuenta lo siguiente: en general, el reordenamiento de losíndices de ciclo puede ser válido o inválido, dependiendo de los criterios que desarrollemos en la siguiente sección. Sin embargo, en este caso, en donde el efecto de cada acceso es tan sólo establecerun elemento del arreglo a 0, no tenemos que preocuparnos por el efecto de reordenar los ciclos, enlo que a la semántica del programa respecta.!Maq. Cap_11_AHO A.indd 814 11/10/07 8:19:27 PMEjercicio 11.5.5: En la sección 11.5.3 comentamos que obtenemos una localidad espacial si elciclo más interno varía sólo como la última coordenada de un acceso al arreglo. Sin embargo, esaafirmación depende de nuestra suposición de que el arreglo se almacenó en orden por filas. ¿Quécondición aseguraría una localidad espacial, si el arreglo se almacenara en orden por columnas?Ejercicio 11.5.6: En el ejemplo 11.28 observamos que la existencia de la reutilización entredos accesos similares dependía en gran parte de las expresiones particulares para las coordenadas del arreglo. Generalice nuestra observación sobre eso para determinar para qué funcionesf(i, j) hay reutilización entre los accesos A[i, j, i + j] y A[i + 1, j − 1, f(i, j)].Ejercicio 11.5.7: En el ejemplo 11.27 sugerimos que habrá más fallos de caché de los necesarios, si las filas de la matriz Z son tan largas que no puedan caber en la caché. Si ése es elcaso, ¿cómo podríamos rescribir el anidamiento de ciclos para poder garantizar la reutilizaciónespacial de grupo?11.6 Análisis de dependencias de datos de arreglosLa paralelización o las optimizaciones locales ordenan con frecuencia las operaciones ejecutadasen el programa original. Al igual que con todas las optimizaciones, las operaciones sólo puedenreordenarse si este reordenamiento no modifica los resultados del programa. Como en generalno podemos comprender con detalle lo que hace un programa, la optimización de código adoptauna prueba conservadora más simple para cuando podemos estar seguros que no se ven afectados los resultados del programa: comprobamos que las operaciones en cualquier ubicaciónde memoria se realicen en el mismo orden en el programa original y en el modificado. En elpresente estudio, nos enfocamos en los accesos a los arreglos, por lo que los elementos de unarreglo son las ubicaciones de memoria de interés.Es evidente que dos accesos, ya sean de lectura o de escritura, son independientes (puedenreordenarse) si hacen referencia a dos ubicaciones distintas. Además, las operaciones de lectura no modifican el estado de la memoria y, por lo tanto, también son independientes. En basea la sección 11.5, decimos que dos accesos son dependientes de datos si hacen referencia a la mismaubicación de memoria y, por lo menos, uno de ellos es una operación de escritura. Para asegurarnosde que el programa modificado hace lo mismo que el original, el nuevo programa debe preservarel ordenamiento de ejecución relativo entre cada par de operaciones dependientes de datos en elprograma original.En la sección 10.2.1 vimos que hay tres tipos de dependencias de datos:1. La dependencia verdadera, en donde una escritura va seguida de una lectura de la mismaubicación.11.6 Analisis de dependencias de datos de arreglos 815!!Maq. Cap_11_AHO A.indd 815 11/10/07 8:19:28 PM816 Capítulo 11. Optimización para el paralelismo y la localidad2. La antidependencia, en donde una lectura va seguida de una escritura a la misma ubicación.3. La dependencia de salida, que consiste en dos escrituras a la misma ubicación.En la explicación anterior, la dependencia de datos se define para los accesos dinámicos.Decimos que un acceso estático en un programa depende de otro, siempre y cuando exista unainstancia dinámica del primer acceso que dependa de cierta instancia del segundo.5Es fácil ver cómo puede usarse la dependencia de datos en la paralelización. Por ejemplo,si no se encuentran dependencias de datos en los accesos de un ciclo, podemos asignar con facilidad cada iteración a un procesador distinto. La sección 11.7 habla acerca de cómo podemosusar esta información de manera sistemática en la paralelización.11.6.1 Definición de la dependencia de datosde los accesos a arreglosVamos a considerar dos accesos estáticos al mismo arreglo en ciclos que tal vez son diferentes.El primero se representa mediante la función de acceso y los límites F = F, f, B, b	 y es unanidamiento de ciclos con d niveles; el segundo se representa mediante F  = F, f, B, b	 yes un anidamiento de ciclos con d niveles. Estos accesos son dependientes de datos si:1. Por lo menos uno de ellos es una referencia de escritura.2. Existen vectores i en Zd, e i en Zd de tal forma que: (a) Bi ≥ 0, (b) Bi ≥ 0, y (c) Fi + f = Fi + f.Como, por lo general, un acceso estático abarca muchos accesos dinámicos, también es importante preguntarnos si sus accesos dinámicos pueden hacer referencia a la misma ubicaciónde memoria. Para buscar dependencias entre instancias del mismo acceso estático, suponemosque F = F  y aumentamos la definición anterior con la restricción adicional de que i ≠ i paradescartar la solución trivial.Ejemplo 11.29: Considere el siguiente anidamiento de ciclos con 1 nivel:Este ciclo tiene dos accesos: Z[i − 1] y Z[i]; el primero es una referencia de lectura y el segundouna escritura. Para buscar todas las dependencias de datos en este programa, debemos verificarsi la referencia de escritura comparte una dependencia consigo misma y con la referencia delectura:5 Recuerde la diferencia entre los accesos estáticos y dinámicos. Un acceso estático es una referencia a un arreglo enuna ubicación específica en un programa, mientras que un acceso dinámico es una ejecución de esa referencia.Maq. Cap_11_AHO A.indd 816 11/10/07 8:19:29 PM1. Dependencia de datos entre Z [i − 1] y Z [i ]. Excepto por la primera iteración, cada iteración lee el valor escrito en la iteración anterior. En sentido matemático, sabemos quehay una dependencia debido a que existen enteros i e i tales que: 1 ≤ i ≤ 10, 1≤ i ≤ 10 e i − 1 = i.Hay nueve soluciones para el sistema anterior de restricciones: (i = 2, i = 1), (i = 3,i = 2), y así sucesivamente.2. Dependencia de datos entre Z [i ] y consigo misma. Es fácil ver que las distintas iteracionesen el ciclo escriben en distintas ubicaciones; es decir, no hay dependencias de datos entrelas instancias de la referencia de escritura Z[i]. En sentido matemático, sabemos que noexiste una dependencia, ya que no existen enteros i e i que cumplan con: 1 ≤ i ≤ 10, 1 ≤ i ≤ 10 i = i e i ≠ i .Observe que la tercera condición, i = i, se debe al requerimiento que establece que Z[i]y Z[i] son la misma ubicación de memoria. La cuarta condición contradictoria, i ≠ i, sedebe al requerimiento que establece que la dependencia no sea trivial (entre los distintosaccesos dinámicos).No es necesario considerar las dependencias de datos entre la referencia de lectura Z[i − 1] yella misma, ya que dos accesos de lectura cualesquiera son independientes. ✷11.6.2 Programación lineal enteraLa dependencia de datos requiere buscar si existen enteros que cumplan con un sistema consistente en igualdades y desigualdades. Las igualdades se derivan de las matrices y los vectoresque representan a los accesos; las desigualdades se derivan de los límites de ciclo. Las igualdades pueden expresarse como desigualdades: una igualdad x = y puede sustituirse por dosdesigualdades, x ≥ y y y ≥ x.Por ende, la dependencia de datos puede parafrasearse como una búsqueda de solucionesenteras que cumplan con un conjunto de desigualdades lineales, lo cual es precisamente elreconocido problema de la programación lineal entera. La programación lineal entera es unproblema NP-completo. Aunque no se conoce un algoritmo polinomial, se ha desarrolladola heurística para resolver programas lineales que involucran muchas variables, los cualespueden ser bastante rápidos en muchos casos. Por desgracia, dicha heurística estándar es inapropiada para el análisis de la dependencia de datos, en donde el reto es resolver muchos programas lineales enteros pequeños y simples, en vez de programas lineales enteros grandes ycomplicados.El algoritmo del análisis de dependencias de datos consiste en tres partes:11.6 Analisis de dependencias de datos de arreglos 817Maq. Cap_11_AHO A.indd 817 11/10/07 8:19:30 PM818 Capítulo 11. Optimización para el paralelismo y la localidad1. Aplicar la prueba del GCD (Greatest Common Divisor, Máximo común divisor), quecomprueba si hay una solución entera a las igualdades, usando la teoría de las ecuaciones diofantinas lineales. Si no hay soluciones enteras, entonces no hay dependencias dedatos. En cualquier otro caso, usamos las igualdades para sustituir para algunas de lasvariables, con lo cual obtenemos desigualdades más simples.2. Usar un conjunto de heurística simple para manejar los números grandes de desigualdades comunes.3. En el caso extraño en el que no funcione la heurística, utilizamos un solucionador de programación entera lineal, el cual usa un método de bifurcar y delimitar, basado en la eliminaciónde Fourier-Motzkin.11.6.3 La prueba del GCDEl primer subproblema es comprobar la existencia de soluciones enteras a las igualdades. Lasecuaciones con la estipulación de que las soluciones deben ser enteras se conocen como ecuacionesdiofantinas. El siguiente ejemplo muestra cómo surge la cuestión de las soluciones enteras; también demuestra que, aun cuando muchos de nuestros ejemplos involucran un solo anidamiento deciclos a la vez, la formulación de las dependencias de datos se aplica a los accesos que tal vezse encuentren en distintos ciclos.Ejemplo 11.30: Considere el siguiente fragmento de código:El acceso Z[2 ∗ i] sólo entra en contacto con los elementos pares de Z, mientras que el accesoZ[2 ∗ j + 1] sólo entra en contacto con los elementos impares. Es evidente que estos dos accesosno comparten dependencia de datos, sin importar los límites de los ciclos. Podemos ejecutariteraciones en el segundo ciclo antes que el primero, o intercalar las iteraciones. Este ejemplono es tan artificial como parece. Un ejemplo en el que los números pares e impares se tratan enforma distinta es un arreglo de números complejos, en donde los componentes real e imaginariose distribuyen uno al lado de otro.Para demostrar la ausencia de las dependencias de datos en este ejemplo, razonamos de lasiguiente manera. Suponga que hay enteros i y j tales que Z[2 ∗ i] y Z[2 ∗ j + 1] sean el mismoelemento del arreglo. Obtenemos la siguiente ecuación diofantina:2i = 2j + 1.No hay enteros i y j que puedan cumplir la ecuación anterior. La prueba es que si i es un entero, entonces 2i es par. Si j es un entero, entonces 2j es par, por lo que 2j + 1 es impar. NingúnMaq. Cap_11_AHO A.indd 818 11/10/07 8:19:30 PMnúmero par es también un número impar. Por lo tanto, la ecuación no tiene soluciones enteras,y por ende no hay dependencia entre los accesos de lectura y de escritura. ✷Para describir cuándo hay una solución a una ecuación diofantina lineal, necesitamos elconcepto del máximo común divisor de dos o más enteros. El GCD de los enteros a 1, a 2,…, an,que se denota como gcd(a 1, a 1,…, an), es el entero más grande que divide de manera uniformea todos estos enteros. Los GCDs pueden calcularse con eficiencia mediante el reconocido algoritmo de euclides (vea el recuadro que habla sobre ese tema).Ejemplo 11.31: gcd(24, 36, 54) = 6, ya que 24/6, 36/6 y 54/6 tienen un residuo de 0, y además cualquier entero mayor que 6 debe dejar un residuo distinto de cero en por lo menos unade las divisiones con 24, 36 y 54. Por ejemplo, 12 divide a 24 y 36 de manera uniforme, perono a 54. ✷La importancia del GCD se explica en el siguiente teorema.Teorema 11.32: La siguiente ecuación diofantina lineal:a 1 x 1 + a 2 x 2 + … + anxn = ctiene una solución entera para x 1, x 2,…, xn, si y solo si gcd(a 1, a 2,…, an) divide a c. ✷Ejemplo 11.33: En el ejemplo 11.30 observamos que la ecuación diofantina lineal 2i = 2j + 1no tiene solución. Podemos escribir esta ecuación así:2i − 2j = 1.Ahora, gcd(2, −2) = 2, y 2 no divide a 1 de manera uniforme. Por ende, no hay solución.Para otro ejemplo, considere la siguiente ecuación:24x + 36y + 54z = 30.Como gcd(24, 36, 54) = 6, y 30/6 = 5, hay una solución en los enteros para x, y y z. Una solución es x = − 1, y = 0 y z = 1, pero hay una infinidad de soluciones más. ✷El primer paso para el problema de la dependencia de datos es utilizar un método estándar,como la eliminación gaussiana, para resolver las igualdades dadas. Cada vez que se construyeuna ecuación lineal, se aplica el Teorema 11.32 para descartar, si es posible, la existencia de unasolución entera. Si podemos descartar dichas soluciones, entonces hay que responder “no”. Encaso contrario, utilizamos la solución de las ecuaciones para reducir el número de variables en lasdesigualdades.Ejemplo 11.34: Considere las siguientes dos igualdades:11.6 Analisis de dependencias de datos de arreglos 819Maq. Cap_11_AHO A.indd 819 11/10/07 8:19:31 PM820 Capítulo 11. Optimización para el paralelismo y la localidadEl algoritmo de EuclidesEl algoritmo euclidiano para encontrar el gcd(a, b) funciona de la siguiente manera. Primero hay que suponer que a y b son enteros positivos, y que a ≥ b. Observe que el GCDde números negativos, o el GCD de un número negativo y uno positivo es el mismo queel GCD de sus valores absolutos, por lo que podemos suponer que todos los enteros sonpositivos.Si a = b, entonces gcd(a, b) = a. Si a > b, suponga que c es el residuo de a/b. Si c = 0,entonces b divide a a de manera uniforme, por lo que gcd(a, b) = b. En caso contrario, hayque calcular gcd(b, c); este resultado también será gcd(a, b).Para calcular gcd(a 1, a 2,…, an), para n > 2, utilizamos el algoritmo de Euclides paracalcular gcd(a 1, a 2) = c. Después calculamos en forma recursiva gcd(c, a 3, a 4,…, an).Si analizamos cada igualdad por separado, quizá habría una solución. Para la primera igualdad, gcd(1, −2, 1) = 1 divide a 0, y para la segunda igualdad, gcd(3, 2, 1) = 1 divide a 5. Noobstante, si utilizamos la primera igualdad para resolver para z = 2y − x y sustituimos para zen la segunda igualdad, obtenemos 2x + 4y = 5. Esta ecuación diofantina no tiene solución, yaque gcd(2, 4) = 2 no divide a 5 de manera uniforme. ✷11.6.4 Heurística para resolver programas lineales enterosEl problema de la dependencia de datos requiere resolver muchos programas lineales enteros.Ahora veremos varias técnicas para manejar las desigualdades simples y una técnica para aprovechar la similitud encontrada en el análisis de dependencias de datos.Prueba de variables independientesMuchos de los programas lineales enteros de la dependencia de datos consisten en desigualdades que involucran sólo un valor desconocido. Los programas pueden resolverse en formasimple, probando si hay enteros entre los límites superior e inferior de la constante, de maneraindependiente.Ejemplo 11.35: Considere el siguiente ciclo anidado:Maq. Cap_11_AHO A.indd 820 11/10/07 8:19:31 PMPara averiguar si hay una dependencia de datos entre Z[i, j] y Z[ j + 10, i + 9], preguntamossi existen enteros i, j, i y j de tal forma que:La prueba del GCD, que se aplica a las dos igualdades anteriores, determinará que puedehaber una solución entera. Las soluciones enteras a las igualdades se expresan mediante:para cualquier entero t 1 y t 2. Si sustituimos las variables t 1 y t 2 en las desigualdades lineales,obtenemos:Por ende, al combinar los límites inferiores de las últimas dos desigualdades con los límitessuperiores de las primeras dos, deducimos lo siguiente:Como el límite inferior sobre t 2 es mayor que su límite superior, no hay una solución entera y,en consecuencia, tampoco hay dependencia de datos. Este ejemplo muestra que, aun cuandohay igualdades que involucren varias variables, la prueba del GCD puede de todas formas creardesigualdades lineales que involucran a una variable a la vez. ✷Prueba acíclicaOtra heurística simple es averiguar si existe una variable que esté delimitada hacia abajo ohacia arriba por una constante. En ciertas circunstancias, podemos sustituir sin peligro la variable por la constante; las desigualdades simplificadas tienen una solución, si y sólo si las desigualdades originales tienen una solución. Dicho en forma específica, suponga que cada límiteinferior sobre vi es de la siguiente forma:c 0 ≤ civi para cierta ci > 0.mientras todos los límites superiores sobre vi son de la forma:en donde c 1, c 1,…, ci son todos no negativos. Entonces, podemos sustituir la variable vi porsu menor valor entero posible. Si no hay dicho límite inferior, sólo sustituimos vi con −£. De11.6 Analisis de dependencias de datos de arreglos 821Maq. Cap_11_AHO A.indd 821 11/10/07 8:19:32 PM822 Capítulo 11. Optimización para el paralelismo y la localidadmanera similar, si cada restricción que involucra a vi se puede expresar en las dos formas anteriores, pero con las direcciones de las desigualdades invertidas, entonces podemos sustituir lavariable vi con el mayor valor entero posible, o por £ si no hay un límite superior constante.Podemos repetir este paso para simplificar las desigualdades y, en algunos casos, determinar sihay una solución.Ejemplo 11.36: Considere las siguientes desigualdades:La variable v 1 está delimitada de abajo por v 2 y de arriba por v 3. Sin embargo, v 2 está delimitada de abajo sólo por la constante 1, y v 3 está delimitada de arriba sólo por la constante 4.Por ende, si sustituimos v 2 por 1 y v 3 por 4 en las desigualdades, obtenemos lo siguiente:que ahora pueden resolverse fácilmente con la prueba de las variables independientes. ✷La prueba del residuo de cicloAhora vamos a considerar el caso en el que cada variable está delimitada de abajo y de arribapor otras variables. Es muy común en el análisis de dependencias de datos que se dé el caso enel que las restricciones tienen la forma vi ≤ vj + c, lo cual puede resolverse mediante el usode una versión simplificada de la prueba de residuo de ciclo, ideada por Shostack. Un conjunto de estas restricciones puede representarse mediante un grafo dirigido, cuyos nodos estánetiquetados con variables. Hay un flanco de vi a vj etiquetado como c, cada vez que hay unarestricción vi ≤ vj + c.Definimos el peso de una ruta como la suma de las etiquetas de todos los flancos a lo largode la ruta. Cada ruta en el grafo representa una combinación de las restricciones en el sistema.Es decir, podemos inferir que v ≤ v + c cada vez que exista una ruta de v a v con el peso c.Un ciclo en el grafo con el peso c representa a la restricción v ≤ v + c para cada nodo v en elciclo. Si podemos encontrar un ciclo con peso negativo en el grafo, entonces podemos inferirque v < v, lo cual es imposible. En este caso, podemos concluir que no hay solución y, por ende,no hay dependencia.También podemos incorporar a la prueba de residuo de ciclo restricciones de la forma c ≤ vy v ≤ c para la variable v y la constante c. Introducimos al sistema de desigualdades una nuevavariable basura v 0, la cual se agrega a cada límite constante superior e inferior. Desde luego quev 0 debe tener el valor 0, pero como la prueba de residuo de ciclo sólo busca ciclos, los valoresactuales de las variables nunca se vuelven importantes, Para manejar los límites constantes,sustituimos:Maq. Cap_11_AHO A.indd 822 11/10/07 8:19:33 PMv ≤ c por v ≤ v 0 + cc ≤ v por v 0 ≤ v − c.Ejemplo 11.37: Considere las siguientes desigualdades:Los límites constantes superior e inferior sobre v 1 se convierten en v 0 ≤ v 1 − 1 y v 1 ≤ v 0 +10; los límites constantes sobre v 2 y v 3 se manejan de manera similar. Después, al convertir laúltima restricción a v 1 ≤ v 3 − 4, podemos crear el grafo que se muestra en la figura 11.21. Elciclo v 1,v 3,v 0,v 1 tiene un peso de −1, por lo que no hay solución a este conjunto de desigualdades. ✷vvvv 21300 ï4 +40ï1+10ï1+10Figura 11.21: Grafo para las restricciones del ejemplo 11.37MemorizaciónA menudo, los problemas de dependencias de datos similares se resuelven en forma repetitiva,ya que los patrones de acceso simple se repiten a lo largo del programa. Una técnica importantepara agilizar el procesamiento de dependencias de datos es el uso de la memorización. La memorización tabula los resultados a los problemas a medida que se generan. La tabla de solucionesalmacenadas se consulta a medida que se presenta cada problema; el problema debe resolverse sólo si no se puede encontrar el resultado para el problema en la tabla.11.6.5 Solución de programas lineales enteros generalesAhora describiremos un método general para resolver el problema de programación lineal entera. El problema es NP -completo; nuestro algoritmo utiliza un método de bifurcar y delimitarque puede requerir una cantidad de tiempo exponencial, en el peor de los casos. Sin embargo, esraro que la heurística de la sección 11.6.4 no pueda resolver el problema, e incluso si no necesitamos aplicar el algoritmo de esta sección, raras veces debe realizar el paso divide y vencerás.11.6 Analisis de dependencias de datos de arreglos 823Maq. Cap_11_AHO A.indd 823 11/10/07 8:19:34 PM824 Capítulo 11. Optimización para el paralelismo y la localidadEl método es primero comprobar la existencia de soluciones racionales para las desigualdades. Éste es el clásico problema de la programación lineal. Si no hay solución racional paralas desigualdades, entonces las regiones de datos que entran en contacto con los accesos encuestión no se traslapan, y sin duda no hay dependencia de datos. Si hay una solución racional,primero tratamos de probar que hay una solución entera, que casi siempre viene siendo el caso.Si eso falla, entonces dividimos el poliedro delimitado por las desigualdades en dos problemasmás pequeños y utilizamos la recursividad.Ejemplo 11.38: Considere el siguiente ciclo simple:Los elementos que entran en contacto con el acceso Z[i] son Z[1],…, Z[9], mientras que loselementos que entran en contacto con Z[i + 10] son Z[11],…, Z[19]. Los rangos no se traslapany, por lo tanto, no hay dependencias de datos. Dicho de manera más formal, debemos mostrarque no hay dos accesos dinámicos i e i, con 1 ≤ i ≤ 9, 1 ≤ i ≤ 9 e i = i + 10. Si hubieratales enteros i e i, entonces podríamos sustituir i + 10 para i y obtener las cuatro restriccionessobre i: 1 ≤ i ≤ 9 y 1 ≤ i + 10 ≤ 9. Sin embargo, i + 10 ≤ 9 implica que i ≤ −1, lo cualcontradice a 1 ≤ i. Por ende, no existen tales enteros i e i. ✷El Algoritmo 11.39 describe cómo determinar si puede encontrarse una solución enterapara un conjunto de desigualdades lineales basadas en el algoritmo de eliminación de FourierMotzkin.Algoritmo 11.39: Solución divide y vencerás para los problemas de programación linealentera.ENTRADA: Un poliedro convexo Sn sobre las variables v 1,…, vn.SALIDA: “sí” si Sn tiene una solución entera, “no” en cualquier otro caso.MÉTODO: El algoritmo se muestra en la figura 11.22. ✷Las líneas (1) a (3) tratan de encontrar una solución racional a las desigualdades. Si no haysolución racional, entonces no hay solución entera. Si se encuentra una solución racional, estosignifica que las desigualdades definen un poliedro no vacío. Es muy raro que dicho poliedro noincluya soluciones enteras; para que eso ocurra, el poliedro debe ser bastante delgado a lo largode cierta dimensión, y debe caber entre puntos enteros.Por ende, las líneas (4) a (9) tratan de comprobar con rapidez si hay una solución entera.Cada paso del algoritmo de eliminación de Fourier-Motzkin produce un poliedro con una dimensión menor que la anterior. Consideramos a los poliedros en orden inverso. Empezamoscon el poliedro con una variable, y a esa variable le asignamos una solución entera, aproximadamente en medio del rango de valores posibles, si es viable. Después sustituimos el valor porla variable en todos los demás poliedros, reduciendo el número de sus variables desconocidasen uno. Repetimos el mismo proceso hasta haber procesado todos los poliedros, en cuyo casoMaq. Cap_11_AHO A.indd 824 11/10/07 8:19:35 PM 1) aplicar el Algoritmo 11.13 a Sn para proyectar las variables hacia fueravn, vn− 1,…, v 1 en ese orden; 2) hacer que Si sea el poliedro después de proyectar vi + 1 hacia fuera, parai = n − 1, n − 2, …, 0; 3) if S 0 es falsa return “no”; /* No hay solución racional si S 0, que involucra sólo constantes, tiene restricciones que no se pueden cumplir */ 4) for (i = 1; i ≤ n; i++) { 5) if (Si no incluye un valor entero) break; 6) elegir ci, un entero en medio del rango para vi en Si; 7) modificar Si, sustituyendo vi por ci; 8) } 9) if (i == n + 1) return “sí”;10) if (i == 1) return “no”;11) hacer que los límites inferior y superior sobre vi en Si seanli y ui, respectivamente;12) aplicar este algoritmo en forma recursiva a Sn ∪ {vi ≤ [li ]} ySn ∪ {v 1 ≥ [ui ]};13) if (devuelve “sí”) return “sí” else return “no”;Figura 11.22: Búsqueda de una solución entera en las desigualdadesse encuentra una solución entera, o encontramos una variable para la cual no hay soluciónentera.Si no podemos encontrar un valor entero incluso ni para la primera variable, entonces no haysolución entera (línea 10). En caso contrario, todo lo que sabemos es que no hay una soluciónentera que incluya la combinación de enteros específicos que hemos elegido hasta ahora, y elresultado no es concluyente. Las líneas (11) a (13) representan el paso divide y vencerás. Si encontramos que la variable vi tiene una solución racional, pero no entera, dividimos el poliedro endos, en donde el primero requiere que vi debe ser un entero más pequeño que la solución racionalencontrada, y el segundo requiere que vi sea un entero mayor que la solución racional encontrada.Si ninguno de los dos tiene una solución, entonces no hay dependencia.11.6.6 ResumenHemos mostrado que las piezas esenciales de información que un compilador puede deducir delas referencias a los arreglos son equivalentes a ciertos conceptos matemáticos estándar. Dadauna función de acceso F = F, f, B, b	:1. La dimensión de la región de datos a la que se accedió se proporciona mediante el rangode la matriz F. La dimensión del espacio de accesos a la misma ubicación se proporcionamediante la nulidad de F. Las iteraciones cuyas diferencias pertenecen al espacio nulo deF hacen referencia a los mismos elementos del arreglo.11.6 Analisis de dependencias de datos de arreglos 825Maq. Cap_11_AHO A.indd 825 11/10/07 8:19:36 PM826 Capítulo 11. Optimización para el paralelismo y la localidad2. Las iteraciones que comparten la reutilización temporal propia de un acceso se separanmediante vectores en el espacio nulo de F. La reutilización espacial propia puede calcularse de manera similar, preguntando cuando dos iteraciones utilizan la misma fila, envez del mismo elemento. Dos accesos Fi1 + f1 y Fi2 + f2 comparten una localidad quepuede explotarse con facilidad a lo largo de la dirección d, si d es la solución específicapara la ecuación Fd = (f1 − f2). En especial, si d es la dirección correspondiente al ciclomás interno, es decir, el vector [0, 0,…, 0, 1], entonces hay localidad espacial si el arreglose almacena en formato de orden por fila.3. El problema de dependencia de datos (si dos referencias pueden referirse a la mismaubicación) es equivalente a la programación lineal entera. Dos funciones de acceso comparten una dependencia de datos si hay vectores con valor de entero i e i, de tal formaque Bi ≥ 0, Bi ≥ 0 y Fi + f = Fi + f.11.6.7 Ejercicios para la sección 11.6Ejercicio 11.6.1: Busque los GCDs para los siguientes conjuntos de enteros: a) {16, 24, 56}. b) {–45, 105, 240}.! c) {84, 105, 180, 315, 350}.Ejercicio 11.6.2: Para el siguiente ciclo:indique todas lasa) Dependencias verdaderas (escritura seguida de una lectura de la misma ubicación).b) Antidependencias (lectura seguida de una escritura a la misma ubicación).c) Dependencias de salida (escritura seguida de otra escritura a la misma ubicación).Ejercicio 11.6.3: En el recuadro del algoritmo de Euclides, hicimos varias afirmaciones sinprueba. Demuestre cada una de las siguientes aseveraciones:a) El algoritmo de Euclides antes definido siempre funciona. En especial, gcd(b, c) = gcd(a,b), en donde c es el residuo distinto de cero de a/b.b) gcd(a, b) = gcd(a, −b).c) gcd(a1, a2,…, a n) = gcd(gcd(a1, a2), a3, a4,…, a n) para n > 2.!Maq. Cap_11_AHO A.indd 826 11/10/07 8:19:36 PMd) El GCD es en realidad una función sobre conjuntos de enteros; es decir, el orden noimporta. Muestre la ley conmutativa para el GCD: gcd(a, b) = gcd(b, a). Después, muestre la instrucción más difícil, la ley asociativa para el GCD: gcd(gcd(a, b), c) = gcd(a,gcd(b, c)). Por último, muestre que en conjunto, estas leyes implican que el GCD de unconjunto de enteros es el mismo, sin importar el orden en el que se calculen los GCDs depares de enteros.e) Si S y T son conjuntos de enteros, entonces gcd(S ∪T) = gcd(gcd(S), gcd(T)).Ejercicio 11.6.4: Busque otra solución para la segunda ecuación diofantina en el ejemplo11.33.Ejercicio 11.6.5: Aplique la prueba de variables independientes en la siguiente situación. Elanidamiento de ciclos es:y dentro del anidamiento hay una asignación que involucra accesos a arreglos. Determine si haydependencias de datos debido a cada una de las siguientes instrucciones:Ejercicio 11.6.6: En las siguientes dos restricciones:elimine x, sustituyéndola por un límite inferior constante sobre y.Ejercicio 11.6.7: Aplique la prueba de residuo de ciclo al siguiente conjunto de restricciones:Ejercicio 11.6.8: Aplique la prueba de residuo de ciclo al siguiente conjunto de restricciones:11.6 Analisis de dependencias de datos de arreglos 827!Maq. Cap_11_AHO A.indd 827 11/10/07 8:19:37 PM828 Capítulo 11. Optimización para el paralelismo y la localidadEjercicio 11.6.9: Aplique la prueba de residuo de ciclo al siguiente conjunto de restricciones:11.7 Búsqueda del paralelismo sin sincronizaciónHabiendo desarrollado la teoría de los accesos afines a un arreglo, su reutilización de los datosy las dependencias entre ellos, ahora comenzaremos a aplicar esta teoría a la paralelización yoptimización de programas reales. Como vimos en la sección 11.1.4, es importante que busquemos el paralelismo, disminuyendo al mínimo al mismo tiempo la comunicación entre los procesadores. Vamos a empezar por estudiar el problema de paralelizar una aplicación sin permitirningún tipo de comunicación o sincronización entre los procesadores. Esta restricción puedeparecer un ejercicio puramente académico; ¿con qué frecuencia podemos encontrar programasy rutinas que tengan tal forma de paralelismo? De hecho, existen muchos de esos programas enla vida real, y el algoritmo para resolver este problema es útil por derecho propio. Además, losconceptos utilizados para resolver este problema se pueden extender para manejar la sincronización y la comunicación.11.7.1 Un ejemplo introductorioEn la figura 11.23 se muestra un extracto de una traducción en C (con accesos a arreglos estilo Fortran que se retienen por claridad) a partir de un algoritmo multirrejillas de 5000 líneasen Fortran para resolver ecuaciones de Euler tridimensionales. El programa invierte la mayorparte de su tiempo en un pequeño número de rutinas como la que se muestra en la figura. Estípico de muchos programas numéricos. A menudo, éstos consisten en numerosos ciclos for, condistintos niveles de anidamiento y tienen muchos accesos a arreglos, todos los cuales son expresiones afines de índices de ciclos circundantes. Para mantener el ejemplo corto, hemos omitidolas líneas del programa original con características similares.El código de la figura 11.23 opera sobre la variable escalar T y un número de arreglos distintos, con distintas dimensiones. Primero vamos a examinar el uso de la variable T. Como cadaiteración en el ciclo utiliza la misma variable T, no podemos ejecutar las iteraciones en paralelo.Sin embargo, T se utiliza sólo como una forma de contener una subexpresión común, que seutiliza dos veces en la misma iteración. En los primeros dos de los tres anidamientos de ciclosen la figura 11.23, cada iteración del ciclo más interno escribe un valor en T y lo utiliza justodespués dos veces, en la misma iteración. Podemos eliminar las dependencias sustituyendo cadauso de T por la expresión del lado derecho en la asignación anterior de T, sin modificar la semántica del programa. O, podemos sustituir la escalar de T por un arreglo. Así, cada iteración( j, i) utiliza su propio elemento del arreglo T[ j, i].Con esta modificación, el cálculo de un elemento del arreglo en cada instrucción de asignación sólo depende de otros elementos del arreglo con los mismos valores para los últimos doscomponentes ( j e i, respectivamente). Por lo tanto, podemos agrupar todas las operaciones queMaq. Cap_11_AHO A.indd 828 11/10/07 8:19:38 PM11.7 Búsqueda del paralelismo sin sincronización 829Figura 11.23: Extracto de código de un algoritmo multirrejillastrabajan sobre el ( j, i)-ésimo elemento de todos los arreglos en una unidad de cómputo, yejecutarlos en el orden secuencial original. Esta modificación produce (jl − 1) × (il − 1) unidades de cómputo, las cuales son todas independientes. Observe que los anidamientos segundoy tercero en el programa fuente involucran a un tercer ciclo, con el índice k. Sin embargo, comono hay dependencia entre los accesos dinámicos con los mismos valores para j e i, podemosejecutar sin problemas los ciclos en k dentro de los ciclos en j e i; es decir, dentro de una unidadde cómputo.Al saber que estas unidades de cómputo son independientes se permiten varias transformaciones válidas sobre este código. Por ejemplo, en vez de ejecutar el código como estaba originalmente escrito, un uniprocesador puede realizar el mismo cálculo ejecutando las unidadesde operación independiente, una unidad a la vez. El código resultante, que se muestra en lafigura 11.24, ha mejorado la localidad temporal, ya que los resultados producidos se consumende inmediato.Las unidades independientes de cómputo también pueden asignarse a distintos procesadores y ejecutarse en paralelo, sin requerir ningún tipo de sincronización o comunicación. Comohay (jl − 1) × (il − 1) unidades independientes de cómputo, podemos utilizar a lo más (jl − 1) ×(il − 1) procesadores. Al organizar los procesadores como si estuvieran en un arreglo bidimensional, con IDs ( j, i), en donde 2 ≤ j < jl y 2 ≤ i < il, el programa SPMD que va a ejecutar cadaprocesador es sólo el cuerpo en el ciclo interno de la figura 11.24.Maq. Cap_11_AHO A.indd 829 11/10/07 8:19:39 PM830 Capítulo 11. Optimización para el paralelismo y la localidadFigura 11.24: Código de la figura 11.23, transformado para ejecutar los ciclos paralelos másexternosEl ejemplo anterior ilustra el método básico para buscar el paralelismo sin sincronización.Primero dividimos el cómputo en todas las unidades independientes que sea posible. Este particionamiento expone las opciones de programación disponibles. Después asignamos unidadesde cómputo a los procesadores, dependiendo del número de procesadores que tengamos. Porúltimo, generamos un programa SPMD para ejecutarlo en cada procesador.11.7.2 Particionamientos de espacio afínSe dice que un anidamiento de ciclos tiene k grados de paralelismo si tiene, dentro del anidamiento, k ciclos paralelizables; es decir, ciclos tales que no haya dependencias de datos entrelas distintas iteraciones de los ciclos. Por ejemplo, el código en la figura 11.24 tiene 2 grados deparalelismo. Es conveniente asignar las operaciones en un cómputo con k grados de paralelismoa un arreglo de procesadores con k dimensiones.Vamos a suponer al principio que cada dimensión del arreglo de procesadores tiene tantoprocesadores como iteraciones del ciclo correspondiente. Una vez que se han encontrado todaslas unidades de cómputo independientes, debemos asignar estos procesadores “virtuales” a losprocesadores actuales. En la práctica, cada procesador debe ser responsable de un buen númerode iteraciones, ya que de lo contrario no hay suficiente trabajo para amortizar la sobrecarga dela paralelización.Descomponemos el programa para paralelizarlo en instrucciones elementales, como las instrucciones de 3 direcciones. Para cada instrucción, buscamos una partición del espacio afín queasigne cada instancia dinámica de la instrucción, según como la identifican sus índices de ciclo,a un ID de procesador.Maq. Cap_11_AHO A.indd 830 11/10/07 8:19:41 PM11.7 Búsqueda del paralelismo sin sincronización 831Ejemplo 11.40: Como vimos antes, el código de la figura 11.23 tiene dos grados de paralelismo. Vemos el arreglo de procesadores como un espacio bidimensional. Suponga que (p 1, p 2) esel ID de un procesador en el arreglo. El esquema de paralelización descrito en la sección 11.7.1puede describirse mediante funciones simples de partición afín. Todas las instrucciones en el primer anidamiento de ciclos tienen esta misma partición afín:Todas las instrucciones en el segundo y tercer anidamiento de ciclos tienen la misma particiónafín:✷El algoritmo para buscar el paralelismo sin sincronización consiste en tres pasos:1. Buscar, para cada instrucción en el programa, una partición afín que incremente almáximo el grado de paralelismo. Observe que, por lo general, tratamos a la instrucción,en vez del acceso individual, como la unidad de cómputo. La misma partición afín debeaplicarse a cada acceso en la instrucción. Este agrupamiento de accesos tiene sentido,ya que casi siempre hay dependencia entre los accesos de la misma instrucción, de todasformas.2. Asignar las unidades de cómputo independientes resultantes entre los procesadores yelegir una intercalación de los pasos en cada procesador. Esta asignación se controlamediante las consideraciones de localidad.3. Generar un programa SMPD para ejecutarlo en cada procesador.A continuación vamos a ver cómo buscar las funciones de partición afín, cómo generar unprograma secuencial que ejecute las particiones en serie, y cómo generar un programa SPMDque ejecute cada partición en un procesador distinto. Después de ver cómo se maneja el paralelismo con sincronizaciones en las secciones 11.8 a 11.9.9, regresaremos al paso 2 anterioren la sección 11.10 y hablaremos sobre la optimización de la localidad para uniprocesadores ymultiprocesadores.11.7.3 Restricciones de partición de espacioPara no requerir comunicación, a cada par de operaciones que comparten una dependencia dedatos se les debe asignar el mismo procesador. Nos referimos a estas restricciones como “restricciones de partición de espacio”. Cualquier asignación que cumpla con estas restriccionescrea particiones independientes unas de otras. Observe que dichas restricciones pueden cumplirse si colocamos todas las operaciones en una sola partición. Por desgracia, esa “solución” noMaq. Cap_11_AHO A.indd 831 11/10/07 8:19:42 PM832 Capítulo 11. Optimización para el paralelismo y la localidadproduce ningún paralelismo. Nuestra meta es crear todas las particiones independientes que seaposible, al mismo tiempo que se cumpla con las restricciones de partición de espacio; es decir,las operaciones no deben colocarse en el mismo procesador, a menos que sea necesario.Cuando nos limitamos a las particiones afines, en vez de maximizar el número de unidadesindependientes, podemos maximizar el grado (número de dimensiones) de paralelismo. Algunasveces es posible crear unidades más independientes, si podemos usar particiones afines a nivelde pieza. Una partición afín a nivel de pieza divide las instancias de un acceso individual en conjuntos diferentes, y permite una partición afín distinta para cada conjunto. Sin embargo, noconsideraremos dicha opción aquí.De manera formal, una partición afín de un programa no está sincronizada si, y sólo si porcada dos accesos (no necesariamente distintos) que comparten una dependencia, F1 = F1, f1,B1, b1	 en la instrucción s 1 anidada en d 1 ciclos, y F2 = F2, f2, B2, b2	 en la instrucción s2anidada en d2 ciclos, las particiones C1, c1	 y C2, c2	 para las instrucciones s1 y s2, respectivamente, cumplen con las siguientes restricciones de partición de espacio:• Para todas las i1 en Z d1 e i2 en Z d2 tales quea) B1i1 + b1 ≥ 0,b) B2 i2 + b2 ≥ 0, yc) F1i1 + f1 = F2 i2 + f2,se da el caso de que C1 i1 + c1 = C2 i2 + c2.El objetivo del algoritmo de paralelización es encontrar, para cada instrucción, la partición conel rango más alto que cumpla con estas restricciones.En la figura 11.25 se muestra un diagrama que ilustra la esencia de las restricciones departición de espacio. Suponga que hay dos accesos estáticos en dos anidamientos de ciclo conlos vectores índice i1 e i2. Estos accesos son dependientes en el sentido en que acceden por lomenos a un elemento del arreglo en común y, por lo menos, uno de ellos es una escritura. Lafigura muestra accesos dinámicos específicos en los dos ciclos que por casualidad acceden almismo elemento del arreglo, de acuerdo con las funciones de acceso afines F1i1 + f1 y F2i2 + f2.La sincronización es necesaria, a menos que las particiones afines para los dos accesos estáticos,C1 i1 + c1 y C2 i2 + c2, asignen los accesos dinámicos al mismo procesador.Si elegimos una partición afín cuyo rango sea el máximo de los rangos de todas las instrucciones, obtenemos el máximo paralelismo posible. Sin embargo, bajo este particionamientoalgunos procesadores pueden estar inactivos en ciertas ocasiones, mientras que otros procesadores ejecutan instrucciones cuyas particiones afines tienen un rango más pequeño. Esta situaciónpuede ser aceptable si el tiempo requerido para ejecutar esas instrucciones es relativamentecorto. En caso contrario, podemos elegir una partición afín cuyo rango sea más pequeño que elmáximo posible, siempre y cuando ese rango sea mayor que 0.En el ejemplo 11.41 mostramos un pequeño programa, diseñado para ilustrar el poder deesta técnica. Por lo general, las aplicaciones reales son mucho más simples que ésta, pero puedentener condiciones delimitadoras que se asemejen a algunas de las cuestiones que se muestranaquí. Vamos a utilizar este ejemplo a lo largo de este capítulo para demostrar que los programas con accesos afines tienen restricciones de partición de espacio relativamente simples, queMaq. Cap_11_AHO A.indd 832 11/10/07 8:19:43 PM11.7 Búsqueda del paralelismo sin sincronización 833F i + f 11 1C i + c 1 1ArregloC i + cF i + f 22 222 2ID del procesador12Ciclos1iiFigura 11.25: Restricciones de partición de espacioestas restricciones pueden resolverse utilizando las técnicas estándar del álgebra lineal, y queel programa SPMD deseado se puede generar de manera mecánica a partir de las particionesafines.Ejemplo 11.41: Este ejemplo muestra cómo formulamos las restricciones de partición de espacio para el programa que consiste en el anidamiento de ciclos pequeño con dos instrucciones,s 1 y s 2, que se muestra en la figura 11.26.Figura 11.26: Un anidamiento de ciclos que exhibe largas cadenas de operaciones dependientesEn la figura 11.27 mostramos las dependencias de datos en el programa. Es decir, cada puntonegro representa una instancia de la instrucción s 1, y cada punto blanco representa una instanciade la instrucción s 2. El punto ubicado en las coordenadas (i, j) representa la instancia de la instrucción que se ejecuta para esos valores de los índices de ciclo. Sin embargo, tenga en cuenta quela instancia de s 2 se encuentra justo debajo de la instancia de s 1 para el mismo par (i, j), por loque la escala vertical de j es mayor que la escala horizontal de i.Observe que X[i, j] se escribe mediante s 1(i, j); es decir, mediante la instancia de la instrucción s 1 con los valores de índice i y j. Después se lee por s 2 (i, j + 1), de manera que s 1(i, j)Maq. Cap_11_AHO A.indd 833 11/10/07 8:19:43 PM834 Capítulo 11. Optimización para el paralelismo y la localidadss121 345 2 6ijjjj = 1= 4= 2= 3Figura 11.27: Dependencias del código en el ejemplo 11.41debe preceder a s 2 (i, j + 1). Esta observación explica las flechas verticales que van de lospuntos negros a los puntos blancos. De manera similar, Y[i, j] se escribe mediante s 2 (i, j) y selee después por s 1 (i + 1, j). Por ende, s 2 (i, j ) debe preceder a s 1 (i + 1, j ), lo cual explica lasflechas que van de los puntos blancos a los negros.Mediante este diagrama podemos ver con facilidad que este código puede paralelizarse sin sincronización, al asignar cada cadena de operaciones dependientes al mismo procesador. Sin embargo, no es fácil escribir el programa SPMD que implementa este esquema de asignación. Mientrasque los ciclos en el programa original tienen 100 iteraciones cada uno, hay 200 cadenas, en las quela mitad empieza y termina con la instrucción s 1, y la otra mitad empieza y termina con s 2. Laslongitudes de las cadenas varían de 1 a 100 iteraciones.Como hay dos instrucciones, estamos buscando dos particiones afines, una para cada instrucción. Sólo tenemos que expresar las restricciones de partición de espacio para las particiones afines unidimensionales. Estas restricciones se utilizarán más adelante con el método desolución que trata de encontrar todas las particiones afines unidimensionales independientes,para combinarlas y obtener particiones afines multidimensionales. Podemos entonces representar la partición afín para cada instrucción mediante una matriz de 1 × 2 y un vector de1 × 1, para traducir el vector de índices [i, j] en un solo número de procesador. Suponga que[C 11C 12 ], [c 1 ]	, [C 21C 22 ], [c 2 ]	 son las particiones afines unidimensionales para las instrucciones s 1 y s 2, respectivamente.Aplicamos seis pruebas de dependencia de datos:1. El acceso de escritura X[i, j] y él mismo en la instrucción s 1,2. El acceso de escritura X[i, j] con el acceso de lectura X[i, j] en la instrucción s1,3. El acceso de escritura X[i, j] en la instrucción s 1 con el acceso de lectura X[i, j − 1] enla instrucción s 2,4. El acceso de escritura Y[i, j] y él mismo en la instrucción s 2,Maq. Cap_11_AHO A.indd 834 11/10/07 8:19:44 PM11.7 Búsqueda del paralelismo sin sincronización 8355. El acceso de escritura Y[i, j] con el acceso de lectura Y[i, j] en la instrucción s 2,6. El acceso de escritura Y[i, j] en la instrucción s2 con el acceso de lectura Y[i − 1, j ] enla instrucción s 1.Podemos ver que todas las pruebas de dependencia son simples y muy repetitivas. Las únicasdependencias presentes en este código ocurren en el caso (3), entre las instancias de los accesosX[i, j] y X[i, j − 1] y en el caso (6), entre Y[i, j] y Y[i − 1, j].Las restricciones de partición de espacio impuestas por la dependencia de datos entre X[i, j]en la instrucción s 1 y X[i, j − 1] en la instrucción s 2 pueden expresarse en los siguientes términos:Para todas las (i, j) y (i, j) tales que:tenemos que:Es decir, las primeras cuatro condiciones establecen que (i, j) y (i, j ) se encuentran dentrodel espacio de iteraciones del anidamiento de ciclos, y las últimas dos condiciones establecenque los accesos dinámicos X[i, j] y X[i, j − 1] entran en contacto con el mismo elemento delarreglo. Podemos derivar la restricción de partición de espacio para los accesos Y[i − 1, j] enla instrucción s 2 y Y[i, j] en la instrucción s 1 de una forma similar. ✷11.7.4 Resolución de restricciones de partición de espacioUna vez que se han extraído las restricciones de partición de espacio, podemos utilizar lastécnicas estándar del álgebra lineal para buscar las particiones afines que cumplan con las restricciones. Primero vamos a mostrar cómo encontrar la solución para el ejemplo 11.41.Ejemplo 11.42: Podemos encontrar las particiones afines para el ejemplo 11.41 con los siguientes pasos:1. Crear las restricciones de partición de espacio que se muestran en el ejemplo 11.41. Utilizamos los límites de ciclo para determinar las dependencias de datos, pero no se utilizanen el resto del algoritmo para ninguna otra cosa.2. Las variables desconocidas en las igualdades son i, i, j, j, C 11, C 12, c 1, C 21, C 22 y c 2.Hay que reducir el número de variables desconocidas mediante el uso de las igualdadesdebido a las funciones de acceso: i = i y j = j − 1. Para ello, utilizamos la eliminacióngaussiana, la cual reduce las cuatro variables a dos: por decir t 1 = i = i, y t 2 = j + 1 = j.La igualdad para la partición se convierte en:Maq. Cap_11_AHO A.indd 835 11/10/07 8:19:45 PM836 Capítulo 11. Optimización para el paralelismo y la localidad3. La ecuación anterior es válida para todas las combinaciones de t 1 y t 2. Por ende, debecumplirse que:Si realizamos los mismos pasos con la restricción entre los accesos Y[i − 1, j ] y Y[i, j ],obtenemos lo siguiente:Al simplificar todas las restricciones en conjunto, obtenemos las siguientes relaciones:4. Buscar todas las soluciones independientes para las ecuaciones que involucran sólo variables desconocidas en la matriz de coeficientes, ignorando las variables desconocidas enlos vectores constantes de este paso. Sólo hay una elección independiente en la matriz decoeficientes, por lo que las particiones afines que buscamos pueden tener como máximoun rango de uno. Mantenemos la partición tan simple como sea posible, para lo cualestablecemos C 11 = 1. No podemos asignar 0 a C 11, ya que esto creará una matriz de coeficientes con rango 0, la cual asigna todas las iteraciones al mismo procesador. Despuéscomo resultado C 21 = 1, C 22 = −1, C 12 = −1.5. Buscar los términos constantes. Sabemos que la diferencia entre los términos constantes,c 2 − c 1, debe ser −1. No obstante, podemos elegir los valores actuales. Para mantenerlas particiones simples, elegimos c 2 = 0; por lo tanto, c 1 = −1.Hagamos que p sea el ID del procesador que ejecuta la iteración (i, j ). En términos de p,la partición afín es:Es decir, la (i, j )-ésima iteración de s1 se asigna al procesador p = i − j − 1, y la (i, j )-ésimaiteración de s 2 se asigna al procesador p = i − j. ✷Algoritmo 11.43: Buscar una partición afín sin sincronización con el rango más alto paraun programa.ENTRADA: Un programa con accesos a arreglos afines.Maq. Cap_11_AHO A.indd 836 11/10/07 8:19:46 PMSALIDA: Una partición.MÉTODO: Haga lo siguiente:1. Busque todos los pares dependientes de datos de accesos en un programa para cada parde accesos dependientes de datos, F1 = F1, f1, B1, b1 en la instrucción s1 anidada enlos ciclos d 1, y F2 = F2, f2, B2, b2 en la instrucción s 2 anidada en los ciclos d 2. Hagamos que C1, c1 y C2, c2 representen las particiones (actualmente desconocidas)de las instrucciones s 1 y s 2, respectivamente. Las restricciones de partición de espacioestablecen que si:entoncespara todas las i1 e i 2, dentro de sus límites de ciclo respectivos. Vamos a generalizar eldominio de iteraciones para incluir todas las i1 en Zd1 y todas las i 2 en Zd2; es decir, seasume que los límites son de infinito negativo a infinito. Esta suposición tiene sentido,ya que una partición afín no puede hacer uso del hecho de que una variable índice sólopuede asumir un conjunto limitado de valores enteros.2. Para cada par de accesos dependientes, reducimos el número de variables desconocidasen los vectores índice. (a) Observe que Fi + f es el mismo vector que:Es decir, al agregar un componente 1 adicional al final del vector columna i, podemos hacer que el vector columna f sea una última columna adicional de la matriz F.Por lo tanto, podemos rescribir la igualdad de las funciones de acceso F1i1 + f1 =F2i 2 + f2 de la siguiente forma: (b) Las ecuaciones anteriores tendrán en general más de una solución. Sin embargo, aúnpodemos usar la eliminación gaussiana en la resolución de las ecuaciones para loscomponentes de i1 e i 2 de la mejor forma posible. Es decir, tratamos de eliminar todas las variables hasta quedarnos sólo con las que no podemos eliminar. La soluciónresultante para i1 e i 2 tendrá la siguiente forma:11.7 Búsqueda del paralelismo sin sincronización 837Maq. Cap_11_AHO B.indd 837 11/10/07 8:20:34 PM838 Capítulo 11. Optimización para el paralelismo y la localidaden donde U es una matriz triangular superior y t es un vector de variables libres, cuyorango comprende a todos los números enteros. (c) Podemos usar el mismo truco que en el paso (2a) para rescribir la igualdad de las particiones. Al sustituir el vector (i1, i 2, 1) con el resultado del paso (2b), podemos escribirlas restricciones sobre las particiones de la siguiente manera:3. Eliminamos las variables que no son de partición. Las ecuaciones anteriores son válidaspara todas las combinaciones de t si:Rescriba estas ecuaciones en la forma Ax = 0, en donde x es un vector de todos loscoeficientes desconocidos de las particiones afines.4. Buscamos el rango de la partición afín y resolvemos para las matrices de coeficientes.Como el rango de una partición afín es independiente del valor de los términos constantes en la partición, eliminamos todas las variables desconocidas que provienen de losvectores constantes, como c1 o c2, con lo cual sustituimos Ax = 0 por las restriccionessimplificadas Ax = 0. Buscamos las soluciones a Ax = 0, expresándolas como B, unconjunto de vectores básicos que abarcan el espacio nulo de A.5. Buscamos los términos constantes. Derivamos una fila de la partición afín deseada decada vector básico en B, y derivamos los términos constantes usando Ax = 0.✷Observe que el paso 3 ignora las restricciones impuestas por los límites de ciclo en las variables t. Como resultado, las restricciones son sólo más estrictas y el algoritmo debe, por lo tanto,ser seguro. Es decir, colocamos restricciones sobre las Cs y las cs, asumiendo que el valor de tes arbitrario. Es posible que pueda haber otras soluciones para las Cs y cs que sean válidas sóloporque algunos valores de t son imposibles. Si no buscamos estas otras soluciones podríamosperder una optimización, pero esto no puede ocasionar que el programa se cambie a un programa que haga algo distinto de lo que hace el programa original.11.7.5 Un algoritmo simple de generación de códigoEl Algoritmo 11.43 genera particiones afines que dividen los cálculos en particiones independientes. Las particiones pueden asignarse de manera arbitraria a distintos procesadores, yaque son independientes unos de otros. Un procesador puede asignarse a más de una partición,y puede intercalar la ejecución de sus particiones, siempre y cuando las operaciones dentrode cada partición que, por lo general, tienen dependencias de datos, se ejecuten en formasecuencial.Maq. Cap_11_AHO B.indd 838 11/10/07 8:20:37 PMEs muy fácil generar un programa correcto, dada una partición afín. Primero vamos a introducir el Algoritmo 11.45, un método simple que genera código para un solo procesador queejecuta cada una de las particiones independientes en forma secuencial. Dicho código optimizala localidad temporal, ya que los accesos a arreglos que tienen varios usos están muy cercanosen el tiempo. Además, el código puede convertirse con facilidad en un programa SPMD queejecute cada partición en un procesador distinto. Por desgracia, el código generado es ineficiente; a continuación hablaremos sobre las optimizaciones para hacer que el código se ejecute coneficiencia.La idea esencial es la siguiente. Recibimos límites para las variables índice de un anidamiento de ciclos, y hemos determinado, en el Algoritmo 11.43, una partición para los accesos deuna instrucción específica s. Suponga que deseamos generar código secuencial para realizar laacción de cada procesador en forma secuencial. Creamos un ciclo externo para iterar a través delos IDs de los procesadores. Es decir, cada iteración de este ciclo realiza las operaciones asignadas a un ID de procesador específico. Este programa original se inserta como el cuerpo de esteciclo; además se agrega una prueba para proteger cada operación en el código y asegurar quecada procesador sólo ejecute las operaciones que tenga asignadas. De esta forma, garantizamosque el procesador ejecutará todas las instrucciones que tenga asignadas, en el orden secuencialoriginal.Ejemplo 11.44: Vamos a generar código para ejecutar las particiones independientes delejemplo 11.41 en forma secuencial. El programa secuencial original de la figura 11.26 se repiteaquí, como la figura 11.28.Figura 11.28: Repetición de la figura 11.26En el ejemplo 11.41, el algoritmo de particionamiento afín encontró un grado de paralelismo. Por ende, el espacio del procesador se puede representar mediante una sola variable p. Enese ejemplo también seleccionamos una partición afín que, para todos los valores de las variables índice i y j con 1 ≤ i ≤ 100 y 1 ≤ j ≤ 100, asignó:1. La instancia (i, j) de la instrucción s 1 al procesador p = i − j − 1.2. La instancia (i, j ) de la instrucción s 2 al procesador p = i − j.Podemos generar el código en tres pasos:1. Para cada instrucción, se buscan todos los IDs de los procesadores que participen en elcálculo. Combinamos las restricciones 1 ≤ i ≤ 100 y 1 ≤ j ≤ 100 con una de las ecuaciones p = i − j − 1 o p = i − j, y proyectamos i y j hacia fuera para obtener las nuevasrestricciones:11.7 Búsqueda del paralelismo sin sincronización 839Maq. Cap_11_AHO B.indd 839 11/10/07 8:20:38 PM840 Capítulo 11. Optimización para el paralelismo y la localidad (a) −100 ≤ p ≤ 98, si utilizamos la función p = i − j − 1 que obtenemos para la instrucción s 1. (b) −99 ≤ p ≤ 99 si utilizamos p = i − j de la instrucción s 2 .2. Se buscan todos los IDs de los procesadores que participen en cualquiera de las instrucciones. Al tomar la unión de estos rangos, obtenemos −100 ≤ p ≤ 99; estos límites sonsuficientes para cubrir todas las instancias de ambas instrucciones s 1 y s 2.3. Se genera el código para iterar a través de los cálculos en cada partición, en forma secuencial. El código, que se muestra en la figura 11.29, tiene un ciclo externo que itera através de todos los IDs de las particiones que participan en el cálculo (línea (1)). Cadapartición pasa a través del movimiento de generar los índices de todas las iteraciones enel programa secuencial original en las líneas (2) y (3), de manera que pueda elegir lasiteraciones que se supone debe ejecutar el procesador p. Las pruebas de las líneas (4) y(6) se aseguran de que las instrucciones s 1 y s 2 se ejecuten sólo cuando el procesador plas vaya a ejecutar.El código generado, aunque correcto, es en extremo ineficiente. En primer lugar, aun cuandocada procesador ejecuta cálculos de 99 iteraciones como máximo, genera índices de ciclo para100 × 100 iteraciones, un orden de magnitud más de lo necesario. En segundo lugar, cada sumaen el ciclo más interno está protegida por una prueba que crea otro factor constante de sobrecarga. En las secciones 11.7.6 y 11.7.7, respectivamente, trataremos estos dos tipos de ineficiencias. ✷Figura 11.29: Una simple rescritura de la figura 11.28 que itera sobre el espacio de procesadoresAunque el código de la figura 11.29 parece estar diseñado para ejecutarse en un uniprocesador, podríamos tomar los ciclos internos, de las líneas (2) a (8), y ejecutarlos en 200 procesadores distintos, cada uno de los cuales tendría un valor distinto para p, de −100 a 99. O, podríamosparticionar la responsabilidad por los ciclos internos entre cualquier número de procesadores menor que 200, siempre y cuando arregláramos que cada procesador supiera de qué valores de p esresponsable, y ejecutara las líneas (2) a (8) sólo para esos valores de p.Algoritmo 11.45: Generación de código que ejecuta particiones de un programa en forma secuencial.Maq. Cap_11_AHO B.indd 840 11/10/07 8:20:39 PMENTRADA: Un programa P con accesos a arreglos afines. Cada instrucción s en el programatiene límites asociados de la forma Bs i + bs ≥ 0, en donde i es el vector de los índices deciclo para el anidamiento de ciclos en el que aparece la instrucción s. También asociada conla instrucción s, hay una partición Cs i + cs = p, en donde p es un vector m-dimensional devariables que representa a un ID de procesador; m es el máximo, sobre todas las instruccionesen el programa P, del rango de la partición para esa instrucción.SALIDA: Un programa equivalente a P, pero que itera sobre el espacio de procesadores, envez de hacerlo sobre los índices de ciclo.MÉTODO: Haga lo siguiente:1. Para cada instrucción, utilice la eliminación de Fourier-Motzkin para proyectar haciafuera todas las variables índices de ciclo de los límites.2. Use el Algoritmo 11.13 para determinar los límites en los IDs de las particiones.3. Genere ciclos, uno para cada una de las m dimensiones del espacio de procesadores. Hagaque p = [p 1, p 2,…, pm ] sea el vector de variables para estos ciclos; es decir, debe haberuna variable para cada dimensión del espacio de procesadores. Cada variable pi del ciclovaría a través de la unión de los espacios de particiones para todas las instrucciones enel programa P.Observe que la unión de los espacios de particiones no es necesariamente convexa. Paramantener el algoritmo simple, en vez de enumerar sólo aquellas particiones que tienen un cálculo no vacío que realizar, establezca el límite inferior de cada pi al mínimo de todos los límitesinferiores impuestos por todas las instrucciones, y el límite superior de cada pi al máximo detodos los límites superiores impuestos por todas las instrucciones. Por lo tanto, algunos valoresde p pueden no tener operaciones.El código a ejecutar por cada partición es el programa secuencial original. Sin embargo,cada instrucción está protegida por un predicado, de manera que sólo se ejecuten las operaciones que pertenecen a la partición. ✷En breve veremos un ejemplo del Algoritmo 11.45. Sin embargo, tenga en cuenta que todavía estamos lejos del código óptimo para los ejemplos típicos.11.7.6 Eliminación de iteraciones vacíasAhora veremos la primera de las dos transformaciones necesarias para generar un códigoSPMD eficiente. El código ejecutado por cada procesador recorre todas las iteraciones enel programa original y elije las operaciones que se supone debe ejecutar. Si el código tiene kgrados de paralelismo, el efecto es que cada procesador realiza k órdenes de magnitud más detrabajo. El propósito de la primera transformación es estrechar los límites de los ciclos paraeliminar todas las iteraciones vacías.Empezaremos por considerar las instrucciones en el programa, una a la vez. El espacio deiteraciones de una instrucción que debe ejecutar cada partición es el espacio de iteraciones original, más la restricción que impone la partición afín. Podemos generar límites estrechos para11.7 Búsqueda del paralelismo sin sincronización 841Maq. Cap_11_AHO B.indd 841 11/10/07 8:20:40 PM842 Capítulo 11. Optimización para el paralelismo y la localidadcada instrucción mediante la aplicación del Algoritmo 11.13 al nuevo espacio de iteraciones;el nuevo vector de índices es como el vector de índices secuencial original, pero se le agreganlos IDs de los procesadores como índices externos. Recuerde que el algoritmo generará límitesestrechos para cada índice, en términos de los índices de ciclo circundantes.Después de buscar los espacios de iteraciones de las distintas instrucciones, los combinamos,ciclo por ciclo, haciendo a los límites la unión de ellos para cada instrucción. Algunos ciclos terminan con una sola iteración, como se muestra a continuación en el ejemplo 11.46, y podemossimplemente eliminar el ciclo y establecer el índice de ciclo al valor para esa iteración.Ejemplo 11.46: Para el ciclo de la figura 11.30(a), el Algoritmo 11.43 creará la siguientepartición afín:El Algoritmo 11.45 creará el código de la figura 11.30(b). Al aplicar el Algoritmo 11.13 a la instrucción s 1 se produce el siguiente límite: p ≤ i ≤ p, o implemente i = p. De manera similar, elalgoritmo determina que j = p para la instrucción s 2. Por ende, obtenemos el código de la figura11.30(c). La propagación de copia de las variables i y j eliminará la prueba innecesaria y producirá el código de la figura 11.30(d). ✷Ahora regresaremos al ejemplo 11.44 e ilustraremos el paso para combinar varios espaciosde iteraciones provenientes de distintas instrucciones en uno solo.Ejemplo 11.47: Ahora vamos a estrechar los límites de ciclo del código en el ejemplo 11.44. Elespacio de iteraciones ejecutado por la partición p para la instrucción s 1 se define mediante lassiguientes igualdades y desigualdades:Al aplicar el Algoritmo 11.13 a lo anterior se crean las restricciones que se muestran en la figura 11.31(a). El Algoritmo 11.13 genera la restricción p + 2 ≤ i ≤ 100 + p + 1 de i − p − 1 = jy 1 ≤ j ≤ 100, y estrecha el límite superior de p a 98. De igual forma, los límites para cada unade las variables para la instrucción s 2 se muestran en la figura 11.31(b).Los espacios de iteraciones para s 1 y s 2 en la figura 11.31 son similares, pero como se esperade la figura 11.27, ciertos límites difieren por 1 entre los dos. El código en la figura 11.32 se ejecuta a través de esta unión de espacios de iteraciones. Por ejemplo, para i se utiliza mín(1, p + 1)como el límite inferior, y máx(100, 100 + p + 1) como el límite superior. Observe que el ciclo másinterno tiene 2 iteraciones, excepto que sólo tiene una la primera y última vez que se ejecuta. Porlo tanto, la sobrecarga al generar índices de ciclo se reduce por un orden de magnitud. Como elespacio de iteraciones ejecutado es más grande que el de s 1 y s 2, aún son necesarias las condicionales para seleccionar cuándo deben ejecutarse estas instrucciones. ✷Maq. Cap_11_AHO B.indd 842 11/10/07 8:20:40 PM(a) Código inicial.(b) Resultado de aplicar el Algoritmo 11.45.(c) Después de aplicar el Algoritmo 11.13.(d) Código final.Figura 11.30: Código para el ejemplo 11.4611.7 Búsqueda del paralelismo sin sincronización 843Maq. Cap_11_AHO B.indd 843 11/10/07 8:20:41 PM844 Capítulo 11. Optimización para el paralelismo y la localidad(a) Límites para la instrucción s 1.(b) Límites para la instrucción s 2.Figura 11.31: Límites más estrechos en p, i y j para la figura 11.2911.7.7 Eliminación de las pruebas de los ciclos más internosLa segunda transformación es la eliminación de las pruebas condicionales de los ciclos internos.Como se puede ver en los ejemplos anteriores, las pruebas condicionales permanecen si los espacios de iteraciones de las instrucciones en el ciclo se cruzan, pero no por completo. Para evitarla necesidad de pruebas adicionales, dividimos el espacio de iteraciones en subespacios, cada unode los cuales ejecuta el mismo conjunto de instrucciones. Esta optimización requiere la duplicación de código, por lo cual sólo debe usarse para eliminar condicionales en los ciclos internos.Para dividir un espacio de iteraciones de manera que se eliminen las pruebas en los ciclos internos, aplicamos los siguientes pasos repetidas veces hasta eliminar todas las pruebas en los ciclosinternos:1. Seleccionar un ciclo que consista de instrucciones con distintos límites.2. Dividir el ciclo que utilice una condición tal que cierta instrucción se excluya de, por lomenos, uno de sus componentes. Elegimos la condición de entre los límites de los distintos poliedros que se traslapan. Si alguna instrucción tiene todas sus iteraciones sólo enuno de los medios planos de la condición, entonces dicha condición es útil.3. Generar código para cada uno de estos espacios de iteraciones por separado.Ejemplo 11.48: Vamos a eliminar las condicionales del código de la figura 11.32. Las instrucciones s 1 y s 2 se asignan al mismo conjunto de IDs de particiones, excepto para las particionesMaq. Cap_11_AHO B.indd 844 11/10/07 8:20:42 PMFigura 11.32: Código de la figura 11.29 mejorado mediante límites de ciclo más estrechosdelimitadoras en ambos extremos. Por ende, separamos el espacio de particiones en tres subespacios:1. p = −100,2. −99 ≤ p ≤ 98, y3. p = 99.El código para cada subespacio puede entonces especializarse para el (los) valor(es) de pcontenidos. La figura 11.33 muestra el código resultante para cada uno de los tres espacios deiteraciones.Observe que los espacios primero y tercero no necesitan ciclos en i o j, ya que para el valorespecífico de p que define a cada espacio, estos ciclos son degenerados; sólo tienen una iteración.Por ejemplo, en el espacio (1), al sustituir p = −100 en los límites de ciclo se restringe i a 1,y por consecuencia j a 100. Las asignaciones a p en los espacios (1) y (3) son sin duda códigomuerto, y pueden eliminarse.A continuación, dividimos el ciclo con el índice i en el espacio (2). De nuevo, las iteracionesprimera y última del índice de ciclo i son distintas. Por ende, dividimos el ciclo en tres subespacios:a) máx(1, p + 1) ≤ i < p + 2, en donde sólo se ejecuta s 2.b) máx(1, p + 2) ≤ i ≤ mín(100, 100 + p), en donde se ejecutan s 1 y s 2.c) 101 + p < i ≤ mín(101 + p, 100), en donde sólo se ejecuta s 1.Por lo tanto, el anidamiento de ciclos para el espacio (2) en la figura 11.33 puede escribirsecomo en la figura 11.34(a).La figura 11.34(b) muestra el programa optimizado. Hemos sustituido la figura 11.34(a)por el anidamiento de ciclos en la figura 11.33. También propagamos las asignaciones a p, i yj hacia los accesos al arreglo. Al optimizar en el nivel de código intermedio, algunas de estasasignaciones se identificarán como subexpresiones comunes y se volverán a extraer del códigode acceso al arreglo. ✷11.7 Búsqueda del paralelismo sin sincronización 845Maq. Cap_11_AHO B.indd 845 11/10/07 8:20:43 PM846 Capítulo 11. Optimización para el paralelismo y la localidadespacioespacioespacioFigura 11.33: Dividir el espacio de iteraciones sobre el valor de p11.7.8 Transformaciones del código fuenteHemos visto cómo podemos derivar, de simples particiones afines para cada instrucción, programas que son bastante distintos del código fuente original. En los ejemplos que hemos vistohasta ahora no es aparente de qué manera se correlacionan las particiones afines con los cambiosen el nivel de código fuente. Esta sección muestra que podemos razonar acerca de los cambios alcódigo fuente con relativa facilidad, al descomponer las particiones afines en una serie de transformaciones primitivas.Siete transformaciones afines primitivasCada partición afín puede expresarse como una serie de transformaciones afines primitivas, cadauna de las cuales corresponde a una simple modificación en el nivel de código fuente. Hay sietetipos de transformaciones primitivas: las primeras cuatro primitivas se ilustran en la figura 11.35,las últimas tres, también conocidas como transformaciones unimodulares, se ilustran en la figura11.36.La figura muestra un ejemplo para cada primitiva: un código fuente, una partición afín,y el código resultante. También dibujamos las dependencias de datos para el código, antes ydespués de las transformaciones. De los diagramas de dependencias de datos, podemos ver quecada primitiva corresponde a una transformación geométrica simple, e induce una transformación de código bastante simple. Las siete primitivas son:Maq. Cap_11_AHO B.indd 846 11/10/07 8:20:44 PM/* espacio (2) *//* espacio (2a) *//* espacio (2b) *//* espacio (2c) *//* espacio (2) *//* espacio (3); p = 99 *//* espacio (1); p = -100 */(a) División del espacio (2) sobre el valor de i.(b) Código optimizado equivalente a la figura 11.28.Figura 11.34: Código para el ejemplo 11.4811.7 Búsqueda del paralelismo sin sincronización 847Maq. Cap_11_AHO B.indd 847 11/10/07 8:20:46 PM848 Capítulo 11. Optimización para el paralelismo y la localidads1s2s1s2s1s2s1s2s1s2s1s2s1s2s1s2CÓDIGO FUENTE PARTICIÓN CÓDIGO TRANSFORMADOFusiónFisiónReindexadoEscaladoFigura 11.35: Transformaciones afines primitivas (I)Maq. Cap_11_AHO B.indd 848 11/10/07 8:20:48 PMs1s2s1s2CÓDIGO FUENTE PARTICIÓN CÓDIGO TRANSFORMADOInversiónPermutaciónDesplazamientoFigura 11.36: Transformaciones afines primitivas (II)11.7 Búsqueda del paralelismo sin sincronización 849Maq. Cap_11_AHO B.indd 849 11/10/07 8:20:50 PM850 Capítulo 11. Optimización para el paralelismo y la localidadTransformaciones unimodularesUna transformación unimodular se representa mediante sólo una matriz de coeficientesunimodular, sin un vector constante. Una matriz unimodular es una matriz cuadrada,cuyo determinante es ±1. La importancia de una transformación unimodular es que asigna un espacio de iteraciones n-dimensional a otro poliedro n-dimensional, en donde hayuna correspondencia de uno a uno entre las iteraciones de los dos espacios.1. Fusión. La transformación de fusión se caracteriza por la asignación de múltiples índicesde ciclo en el programa original al mismo índice de ciclo. El nuevo ciclo fusiona las instrucciones de distintos ciclos.2. Fisión. La fisión es el inverso de la fusión. Asigna el mismo índice de ciclo para distintasinstrucciones a distintos índices de ciclo en el código transformado. Esto divide el ciclooriginal en varios ciclos.3. Reindexado. El reindexado desplaza las ejecuciones dinámicas de una instrucción enbase a un número constante de iteraciones. La transformación afín tiene un término constante.4. Escalado. Las iteraciones consecutivas en el programa de código fuente se separan mediante un factor constante. La transformación afín tiene un coeficiente no unitario positivo.5. Inversión. Ejecuta las iteraciones en un ciclo en orden inverso. La inversión se caracterizapor tener −1 como coeficiente.6. Permutación. Permuta los ciclos internos y externos. La transformación afín consiste enfilas permutadas de la matriz de identidad.7. Desplazamiento. Itera a través del espacio de iteraciones en los ciclos a cierto ángulo. Latransformación afín es una matriz unimodular, con 1s en la diagonal.Una interpretación geométrica de la paralelizaciónLas transformaciones afines mostradas en todos los ejemplos, excepto el de la fisión, se derivanmediante la aplicación del algoritmo de partición afín sin sincronización a los códigos fuenterespectivos (en la siguiente sección veremos cómo la fisión puede paralelizar el código consincronización). En cada uno de los ejemplos, el código generado tiene un ciclo paralelizable(más externo), cuyas iteraciones pueden asignarse a distintos procesadores y no se requieresincronización.Estos ejemplos muestran que hay una interpretación geométrica simple de cómo funcionala paralelización. Las aristas de dependencia siempre apuntan de una instancia anterior a unaposterior. Así, las dependencias entre las instrucciones separadas que no estén anidadas enun ciclo común siguen el orden léxico; las dependencias entre las instrucciones anidadas en elMaq. Cap_11_AHO B.indd 850 11/10/07 8:20:52 PMmismo ciclo siguen el orden lexicográfico. En sentido geométrico, las dependencias de un ciclobidimensional siempre apuntan dentro del rango [0º, 180º), lo cual significa que el ángulo de ladependencia debe estar por debajo de 180º, pero no debe ser menor que 0º.Las transformaciones afines modifican el orden de las iteraciones de tal forma que todas lasdependencias se encuentran sólo entre las operaciones anidadas dentro de la misma iteración delciclo más externo. En otras palabras, no hay aristas de dependencia en los límites de las iteraciones en el ciclo más externo. Podemos paralelizar los códigos fuente simples al dibujar susdependencias y buscar dichas transformaciones en forma geométrica.11.7.9 Ejercicios para la sección 11.7Ejercicio 11.7.1: Para el siguiente ciclo:a) ¿Cuál es el mayor número de procesadores que pueden usarse en forma efectiva paraejecutar este ciclo?b) Rescriba el código con el procesador p como parámetro.c) Establezca y encuentre una solución a las restricciones de partición de espacio para esteciclo.d) ¿Cuál es la partición afín del rango más alto para este ciclo?Ejercicio 11.7.2: Repita el ejercicio 11.7.1 para los anidamientos de ciclos en la figura11.37.Ejercicio 11.7.3: Rescriba el siguiente código:de manera que consista en un solo ciclo. Rescriba el ciclo en términos de un número de procesador p, de forma que el código pueda particionarse entre 100 procesadores, en donde elprocesador p ejecute la iteración p.Ejercicio 11.7.4: En el siguiente código:11.7 Búsqueda del paralelismo sin sincronización 851Maq. Cap_11_AHO B.indd 851 11/10/07 8:20:53 PM852 Capítulo 11. Optimización para el paralelismo y la localidadFigura 11.37: Código para el ejercicio 11.7.2las únicas restricciones son que la instrucción s que forma el cuerpo del anidamiento de ciclosdebe ejecutar las iteraciones s (i − 1, j ) y s (i, j − 1) antes de ejecutar la iteración s (i, j ). Verifique que éstas sean las únicas restricciones necesarias. Después rescriba el código, de forma queel ciclo externo tenga la variable índice p, y en la p-ésima iteración del ciclo externo se ejecutentodas las instancias de s (i, j ) tales que i + j = p.Ejercicio 11.7.5: Repita el ejercicio 11.7.4, pero haga que en la p-ésima iteración del cicloexterno, se ejecuten las instancias de s tales que i − j = p.Ejercicio 11.7.6: Combine los siguientes ciclos:en un solo ciclo, preservando todas las dependencias.!Maq. Cap_11_AHO B.indd 852 11/10/07 8:20:54 PMEjercicio 11.7.7: Muestre que la siguiente matriz:es unimodular. Describa la transformación que realiza sobre un anidamiento de ciclos bidimensional.Ejercicio 11.7.8: Repita el ejercicio 11.7.7 con la siguiente matriz:11.8 Sincronización entre ciclos paralelosLa mayoría de los programas no tienen paralelismo si no permitimos que los procesadores realicensincronizaciones. Pero al agregar hasta un pequeño número constante de operaciones de sincronización a un programa, se puede exponer más paralelismo. En esta sección veremos primero elparalelismo que se logra mediante un número constante de sincronizaciones, y en la siguientesección veremos el caso general, en donde incrustamos las operaciones de sincronización en losciclos.11.8.1 Un número constante de sincronizacionesLos programas que no tienen paralelismo sin sincronización pueden contener una secuencia deciclos, algunos de los cuales son paralelizables si se consideran en forma independiente. Podemos paralelizar dichos ciclos mediante la introducción de barreras de sincronización, antes ydespués de su ejecución. El ejemplo 11.49 ilustra este punto.Figura 11.38: Dos anidamientos de ciclos secuencialesEjemplo 11.49: En la figura 11.38 hay un programa que representa a un algoritmo de integración ADI (Alternating Direction Implicit, implícito de dirección alternante). No hay paralelismo sin sincronización. Las dependencias en el primer anidamiento de ciclos requieren quecada procesador trabaje en una columna del arreglo X; sin embargo, las dependencias en elsegundo anidamiento de ciclos requieren que cada procesador trabaje en una fila del arreglo X.Para que no haya comunicación, todo el arreglo tiene que residir en el mismo procesador, por11.8 Sincronización entre ciclos paralelos 853Maq. Cap_11_AHO B.indd 853 11/10/07 8:20:56 PM854 Capítulo 11. Optimización para el paralelismo y la localidadlo cual no hay paralelismo. Sin embargo, observamos que ambos ciclos pueden paralelizarse enforma independiente.Una manera de paralelizar el código es hacer que distintos procesadores trabajen en distintas columnas del arreglo en el primer ciclo, que sincronicen y esperen a que todos los procesadores terminen, y después operen sobre las filas individuales. De esta forma, todos los cálculosen el algoritmo pueden paralelizarse con la introducción de sólo una operación de sincronización. Sin embargo, observamos que aunque sólo se realiza una sincronización, esta paralelizaciónrequiere que se transfieran casi todos los datos en la matriz X entre los procesadores. Es posible reducir la cantidad de comunicación al introducir más sincronizaciones, lo cual veremos en la sección 11.9.9. ✷Puede parecer que este método sólo puede aplicarse a los programas que consisten en unasecuencia de anidamientos de ciclos. Sin embargo, podemos crear oportunidades adicionalespara la optimización, a través de las transformaciones de código. Podemos aplicar la fisión deciclos para descomponer los ciclos en el programa original en varios ciclos más pequeños, quedespués se pueden paralelizar de manera individual al separarlos con barreras. Ilustraremosesta técnica con el ejemplo 11.50.Ejemplo 11.50: Considere el siguiente ciclo:Sin conocer los valores en el arreglo A, debemos asumir que el acceso en la instrucción s 2 puedeescribir a cualquiera de los elementos de W. Por ende, las instancias de s 2 deben ejecutarse enforma secuencial, en el orden en el que se ejecutan en el programa original.No hay paralelismo sin sincronización, y el Algoritmo 11.43 sólo asignara todos los cálculosal mismo procesador. Sin embargo, como mínimo, las instancias de la instrucción s 1 puedenejecutarse en paralelo. Podemos paralelizar parte de este código, al hacer que distintos procesadores ejecuten distintas instancias de la instrucción s 1. Después, en un ciclo secuencialseparado, un procesador (por decir, el número 0) ejecuta a s 2, como en el código SPMD que semuestra en la figura 11.39. ✷11.8.2 Grafos de dependencias del programaPara encontrar todo el paralelismo posible mediante un número constante de sincronizaciones,podemos aplicar la fisión al programa original masivamente. Se descomponen los ciclos entodos los ciclos separados que sea posible, y después se paraleliza cada ciclo de manera independiente.Para exponer todas las oportunidades para la fisión de ciclos, utilizamos la abstracción deun grafo de dependencias de programa (Program-Dependence Graph, PDG). Un grafo de depenMaq. Cap_11_AHO B.indd 854 11/10/07 8:20:57 PMX[p] = Y[p] + Z[p]; /* (s1) *//* barrera de sincronizacion */if (p == 0) for (i=1; i<=n; i++) W[A[i]] = X[i]; /* (s2) *)Figura 11.39: Código SPMD para el ciclo en el ejemplo 11.50, en donde p es una variable quecontiene el ID del procesadordencias de un programa es un grafo cuyos nodos son las instrucciones de asignación del programa,y cuyas aristas capturan las dependencias de datos, y las direcciones de la dependencia de datos,entre las instrucciones. Una arista de la instrucción s 1 a la instrucción s 2 existe cada vez quecierta instancia dinámica de s 1 comparte una dependencia de datos con una instancia dinámicaposterior de s 2.Para construir el PDG para un programa, primero buscamos las dependencias de datosentre cada par de accesos estáticos (no necesariamente distintos) en cada par de instrucciones(no necesariamente distintas). Suponga que determinamos que hay una dependencia entre elacceso F1 en la instrucción s 1 y el acceso F2 en la instrucción s 2. Recuerde que una instanciade una instrucción se especifica mediante un vector de índices i = [i 1, i 2,…, i m ], en donde ik esel índice del k-ésimo ciclo más externo en el cual está incrustada la instrucción.1. Si existe un par de instancias dependientes de datos, i1 de s 1 e i2 de s 2, y si i1 se ejecutaantes de i2 en el programa original, lo cual se escribe como i1 ≺s 1 s 2 i2, entonces hay unaarista de s 1 a s 2.2. De manera similar, si existe un par de instancias dependientes de datos, i1 de s 1 e i2 des 2, y si i2 ≺s 1s 2 i2, entonces hay una arista de s 2 a s 1.Observe que es posible para una dependencia de datos entre dos instrucciones s 1 y s 2 generartanto una arista de s 1 a s 2, como una arista de s 2 que regrese a s 1.En el caso especial en el que las instrucciones s 1 y s 2 no son distintas, i1 ≺s 1 s 2 i2 si, y sólosi i1 ≺ i2 (i1 es menor que i2, en sentido lexicográfico). En el caso general, s 1 y s 2 pueden serdistintas instrucciones, que quizá pertenezcan a distintos anidamientos de ciclos.Ejemplo 11.51: Para el programa del ejemplo 11.50, no hay dependencias entre las instancias de la instrucción s 1. Sin embargo, la i-ésima instancia de la instrucción s 2 debe seguir ala i-ésima instancia de la instrucción s 1. Peor aún, como la referencia W[A[i]] puede escribiren cualquier elemento del arreglo W, la i-ésima instancia de s 2 depende de todas las instanciasanteriores de s 2. Es decir, la instrucción s 2 depende de sí misma. El PDG para el programadel ejemplo 11.50 se muestra en la figura 11.40. Observe que hay un ciclo en el grafo, el cualcontiene sólo a s 2. ✷El grafo de dependencias del programa ayuda a determinar si podemos dividir las instrucciones en un ciclo. Las instrucciones conectadas en un ciclo en un PDG no pueden dividirse.11.8 Sincronización entre ciclos paralelos 855Maq. Cap_11_AHO B.indd 855 11/10/07 8:20:58 PM856 Capítulo 11. Optimización para el paralelismo y la localidads s 1 2Figura 11.40: Grafo de dependencias del programa del ejemplo 11.50Si s 1 → s 2 es una dependencia entre dos instrucciones en un ciclo, entonces cierta instanciade s 1 debe ejecutarse antes de cierta instancia de s 2, y viceversa. Observe que esta dependencia mutua ocurre sólo si s 1 y s 2 están incrustadas en cierto ciclo común. Debido a la dependencia mutua, no podemos ejecutar todas las instancias de una instrucción antes de la otray, por lo tanto, no se permite la fisión de ciclos. Por otro lado, si la dependencia s 1 → s 2 esunidireccional, podemos dividir el ciclo y ejecutar todas las instancias de s 1 primero, y despuéstodas las de s 2.s s 1 2s3(a) Un programa.(b) Su grafo de dependencias.Figura 11.41: Programa y grafo de dependencias para el ejemplo 11.52Ejemplo 11.52: La figura 11.41(b) muestra el grafo de dependencias para el programa de lafigura 11.41(a). Las instrucciones s 1 y s 3 pertenecen a un ciclo en el grafo y, por lo tanto, nopueden colocarse en ciclos separados. Sin embargo, podemos dividir la instrucción s 2 y ejecutar todas sus instancias antes de ejecutar el resto de los cálculos, como en la figura 11.42. Elprimer ciclo es paralelizable, pero el segundo no. Podemos paralelizar el primer ciclo colocandobarreras antes y después de su ejecución en paralelo. ✷Maq. Cap_11_AHO B.indd 856 11/10/07 8:20:58 PMFigura 11.42: Agrupamiento de los componentes de un anidamiento de ciclos fuertemente conectados11.8.3 Tiempo jerárquicoAunque la relación ≺s 1s 2 puede ser muy difícil de calcular en general, hay una familia de programas para la cual las optimizaciones de esta sección se aplican comúnmente, y para la cual hayuna manera directa de calcular las dependencias. Suponga que el programa está estructuradopor bloques, que consiste en ciclos y operaciones aritméticas simples, y ninguna otra construcción de control. Una instrucción en el programa es una instrucción de asignación, una secuenciade instrucciones, o una construcción de ciclo, cuyo cuerpo es una instrucción. Por ende, la estructura de control representa una jerarquía. En la parte superior de ésta se encuentra el nodo querepresenta a la instrucción de todo el programa. Una instrucción de asignación es un nodo hoja.Si una instrucción es una secuencia, entonces sus hijos son las instrucciones dentro de la secuencia, distribuidos de izquierda a derecha, de acuerdo con su orden léxico. Si una instrucción esun ciclo, entonces sus hijos son los componentes del cuerpo del ciclo que, por lo general, es unasecuencia de una o más instrucciones.Figura 11.43: Un programa estructurado en forma jerárquicaEjemplo 11.53: La estructura jerárquica del programa en la figura 11.43 se muestra en lafigura 11.44. La naturaleza jerárquica de la secuencia de ejecución se destaca en la figura 11.45.11.8 Sincronización entre ciclos paralelos 857Maq. Cap_11_AHO B.indd 857 11/10/07 8:21:00 PM858 Capítulo 11. Optimización para el paralelismo y la localidadLa instancia individual de s0 precede a todas las demás operaciones, ya que es la primerainstrucción que se ejecuta. A continuación, ejecutamos todas las instrucciones a partir de laprimera iteración del ciclo externo, antes de las que se encuentran en la segunda iteración, y asísucesivamente. Para todas las instancias dinámicas cuyo índice de ciclo i tiene el valor 0, lasinstrucciones s 1, L 2, L 3 y s5 se ejecutan en orden léxico. Podemos repetir el mismo argumentopara generar el resto del orden de ejecución. ✷s0 L1s1 L2 L3 s5Progs2 s3 s4Figura 11.44: Estructura jerárquica del programa en el ejemplo 11.53Figura 11.45: Orden de ejecución del programa en el ejemplo 11.53Podemos resolver el ordenamiento de dos instancias provenientes de dos instrucciones distintas en forma jerárquica. Si las instrucciones comparten ciclos comunes, comparamos losvalores de sus índices de ciclo comunes, empezando con el ciclo más externo. Tan pronto comoencontramos una diferencia entre sus valores índice, ésta determina el ordenamiento. Sólo silos valores índice para los ciclos externos son iguales, tenemos que comparar los índices delsiguiente ciclo interno. Este proceso es similar a la forma en que podríamos comparar el tiempoexpresado en términos de horas, minutos y segundos. Para comparar dos tiempos, primero comparamos las horas, y sólo si se refieren a la misma hora compararíamos entonces los minutos,Maq. Cap_11_AHO B.indd 858 11/10/07 8:21:01 PMy así sucesivamente. Si los valores índice son iguales para todos los ciclos comunes, entoncesresolvemos el orden con base en su colocación léxica relativa. Así, al orden de ejecución paralos programas con ciclos anidados simples que hemos estado viendo se le conoce como “tiempojerárquico”.Hagamos que s 1 sea una instrucción anidada en un ciclo con d 1 niveles, y s 2 en un ciclo cond 2 niveles, compartiendo d ciclos comunes (externos); observe que, sin duda, d ≤ d 1 y d ≤ d 2.Suponga que i = [i 1, i 2,…, id1] es una instancia de s 1 y que j = [ j 1, j 2,…, jd2] es una instanciade s 2.i ≺s1s 2 j si y sólo si se cumple una de las siguientes dos condiciones:1. [i 1, i 2,…, id ] ≺ [ j 1, j 2,…, jd ], o2. [i 1, i 2,…, id ] = [ j 1, j 2,…, jd ], y s 1 aparece antes que s 2, en sentido léxico.El predicado [i 1, i 2,…, id ] ≺ [ j 1, j 2,…, jd ] puede escribirse como una desunión de desigualdades lineales:La arista de un PDG de s 1 a s 2 existe mientras que la condición de dependencia de datos yuna de las cláusulas de desunión puedan volverse verdaderas al mismo tiempo. Por ende, tal veztengamos que resolver hasta d o d + 1 programas enteros lineales, dependiendo de si s 1 apareceantes que s 2 en sentido léxico, para determinar la existencia de una arista.11.8.4 El algoritmo de paralelizaciónAhora presentaremos un algoritmo simple que primero divide el cálculo en todos los ciclos distintos que sea posible, y después los paraleliza en forma independiente.Algoritmo 11.54: Maximizar el grado de paralelismo permitido por O (1) sincronizaciones.ENTRADA: Un programa con accesos a un arreglo.SALIDA: Código SPMD con un número constante de barreras de sincronización.MÉTODO:1. Construya el grafo de dependencias del programa y particione las instrucciones en componentes fuertemente conectados (SCCs). En la sección 10.5.8 vimos que un componentefuertemente conectado es un subgrafo máximo del original, en el cual todos los nodos enel subgrafo pueden llegar a cualquier otro nodo.2. Transforme el código para ejecutar SCCs en un orden topológico, aplicando la fisión sies necesario.3. Aplique el Algoritmo 11.43 a cada SCC para encontrar todo su paralelismo sin sincronización. Se insertan barreras antes y después de cada SCC paralelizado.✷11.8 Sincronización entre ciclos paralelos 859Maq. Cap_11_AHO B.indd 859 11/10/07 8:21:02 PM860 Capítulo 11. Optimización para el paralelismo y la localidadAunque el Algoritmo 11.54 encuentra todos los grados de paralelismo con O (1) sincronizaciones, tiene varias debilidades. En primer lugar, puede introducir sincronizaciones innecesarias. De hecho, si aplicamos este algoritmo a un programa que pueda paralelizarse sinsincronización, el algoritmo paralelizará cada instrucción de manera independiente, e introducirá una barrera de sincronización entre los ciclos paralelos que ejecutan cada instrucción. Ensegundo lugar, aunque puede haber sólo un número constante de sincronizaciones, el esquemade paralelización puede transferir muchos datos entre los procesadores con cada sincronización.En algunos casos, el costo de la comunicación hace que el paralelismo sea demasiado costoso,e incluso hasta podemos estar mejor si ejecutamos el programa en forma secuencial, en ununiprocesador. En las siguientes secciones, veremos formas de incrementar la localidad de losdatos, y en consecuencia reducir la cantidad de comunicación.11.8.5 Ejercicios para la sección 11.8Ejercicio 11.8.1: Aplique el Algoritmo 11.54 al código de la figura 11.46.Figura 11.46: Código para el ejercicio 11.8.1Ejercicio 11.8.2: Aplique el Algoritmo 11.54 al código de la figura 11.47.Figura 11.47: Código para el ejercicio 11.8.2Ejercicio 11.8.3: Aplique el Algoritmo 11.54 al código de la figura 11.48.Maq. Cap_11_AHO B.indd 860 11/10/07 8:21:03 PMFigura 11.48: Código para el ejercicio 11.8.311.9 CanalizacionesEn las canalizaciones, una tarea se descompone en varias etapas a realizar en distintos procesadores. Por ejemplo, una tarea que se calcula mediante un ciclo de n iteraciones puede estructurarsecomo una canalización de n etapas. Cada etapa se asigna a un procesador distinto; cuando unprocesador termina con su etapa, los resultados se pasan como entrada para el siguiente procesador en la canalización.A continuación, empezaremos por explicar el concepto de las canalizaciones con más detalle.Después mostraremos un algoritmo numérico real, conocido como sobrerrelajación sucesiva, parailustrar las condiciones bajo las cuales se puede aplicar la canalización, en la sección 11.9.2. Másadelante, en la sección 11.9.6, definimos de manera formal las restricciones que deben resolverse,y en la sección 11.9.7 describimos un algoritmo para resolverlas. A los programas que tienen variassoluciones independientes para las restricciones de partición de tiempo se les considera que susciclos más externos son ciclos completamente permutables; dichos ciclos pueden canalizarse confacilidad, como vimos en la sección 11.9.8.11.9.1 ¿Qué es la canalización?Nuestros primeros intentos por paralelizar los ciclos particionaron las iteraciones de un anidamiento de ciclos, de tal forma que dos iteraciones que compartían datos se asignaban al mismo procesador. La canalización permite a los procesadores compartir datos, pero por lo general sólo en forma“local”, en donde los datos se pasan de un procesador a otro que está adyacente en el espacio deprocesadores. He aquí un ejemplo simple.Ejemplo 11.55: Considere el siguiente ciclo:Este código suma la i-ésima fila de Y y la agrega al i-ésimo elemento de X. El ciclo interior,que corresponde a la suma, debe ejecutarse en forma secuencial debido a la dependencia de11.9 Canalizaciones 861Maq. Cap_11_AHO B.indd 861 11/10/07 8:21:04 PM862 Capítulo 11. Optimización para el paralelismo y la localidadTiempo ProcesadoresFigura 11.49: Ejecución canalizada del ejemplo 11.55 con m = 4 y n = 3datos; 6 sin embargo, las distintas tareas de suma son independientes. Podemos paralelizar estecódigo al hacer que cada procesador ejecute una suma por separado. El procesador i accede ala fila i de Y y actualiza el i-ésimo elemento de X.De manera alternativa, podemos estructurar los procesadores para ejecutar la suma en unacanalización, y derivar el paralelismo al traslapar la ejecución de las sumas, como se muestra enla figura 11.49. Dicho en forma más específica, cada iteración del ciclo interno puede tratarsecomo una etapa de una canalización: la etapa j toma un elemento de X que se generó en la etapa anterior, lo suma a un elemento de Y y pasa el resultado a la siguiente etapa. Observe queen este caso, cada procesador accede a una columna, en vez de una fila de Y. Si Y se almacena enformato ordenado por columnas, hay una ganancia en la localidad al particionar de acuerdo a lascolumnas, en vez de hacerlo en base a las filas.Podemos iniciar una nueva tarea tan pronto como el primer procesador termina con la primera etapa de la tarea anterior. Al principio, la canalización está vacía y sólo el primer procesadorestá ejecutando la primera etapa. Una vez que termina, los resultados se pasan al segundo procesador, mientras que el primer procesador empieza con la segunda tarea, y así sucesivamente.De esta forma, la canalización se llena en forma gradual hasta que todos los procesadores estánocupados. Cuando el primer procesador termina con la última tarea, la canalización empiezaa drenarse, y cada vez más procesadores se vuelven inactivos hasta que el último procesadortermina la última tarea. En el estado estable, pueden ejecutarse n tareas en forma concurrente,en una canalización de n procesadores. ✷Es interesante contrastar la canalización con el paralelismo simple, en donde distintos procesadores ejecutan distintas tareas:• La canalización sólo puede aplicarse a anidamientos con una profundidad de dos, por lomenos. Podemos tratar a cada iteración del ciclo externo como una tarea, y a las iteraciones en el ciclo interno como etapas de esa tarea.• Las tareas ejecutadas sobre una canalización pueden compartir dependencias. La información que pertenece a la misma etapa de cada tarea se guarda en el mismo procesador; así,los resultados generados por la i-ésima etapa de una tarea pueden usarse por la i-ésimaetapa de las tareas subsiguientes, sin que esto afecte a la comunicación. De manera similar,6 Recuerde que no aprovechamos la supuesta propiedad conmutativa y asociativa de la suma.Maq. Cap_11_AHO B.indd 862 11/10/07 8:21:05 PMcada elemento de datos de entrada utilizado por una etapa individual de distintas tareasdebe residir sólo en un procesador, como lo ilustra el ejemplo 11.55.• Si las tareas son independientes, entonces la paralelización simple tiene un mejor uso delos procesadores, ya que éstos pueden ejecutar instrucciones al mismo tiempo, sin tenerque pagar por la sobrecarga de llenar y vaciar la canalización. Sin embargo, como semuestra en el ejemplo 11.55, el patrón de accesos de datos en un esquema canalizado esdistinto del de la paralización simple. Puede ser preferible la canalización, si ésta reducela comunicación.11.9.2 Sobrerrelajación sucesiva (Successive Over-Relaxation, SOR):un ejemploLa sobrerrelajación sucesiva (Successive Over-Relaxation, SOR) es una técnica para acelerar laconvergencia de los métodos de relajación para resolver conjuntos de ecuaciones lineales simultáneas. En la figura 11.50(a) se muestra una plantilla bastante simple, que ilustra su patrón deacceso de datos. Aquí, el nuevo valor de un elemento en el arreglo depende de los valores de loselementos en su alrededor. Dicha operación se realiza repetidas veces, hasta cumplir con ciertocriterio de convergencia.ji(a) Código fuente original.(b) Dependencias de datos en el código.Figura 11.50: Un ejemplo de sobrerrelajación sucesiva (SOR)En la figura 11.50(b) se muestra una imagen de las dependencias de datos clave. No mostramos las dependencias que pueden inferirse mediante las dependencias ya incluidas en lafigura. Por ejemplo, la iteración [i, j ] depende de las iteraciones [i, j − 1], [i, j − 2], y asísucesivamente. De las dependencias queda claro que no hay paralelismo sin sincronización.Como la cadena más larga de dependencias consiste en O (m + n) aristas, al introducir lasincronización debemos tener la capacidad de encontrar un grado de paralelismo y ejecutar lasO (mn) operaciones en un tiempo unitario representado por O (m + n).11.9 Canalizaciones 863Maq. Cap_11_AHO B.indd 863 11/10/07 8:21:06 PM864 Capítulo 11. Optimización para el paralelismo y la localidadEn especial, observamos que las iteraciones que se encuentran a lo largo de las diagonales 7de150º en la figura 11.50(b) no comparten dependencias. Sólo dependen de las iteraciones quese encuentran a lo largo de las diagonales cercanas al origen. Por lo tanto, podemos paralelizareste código al ejecutar las iteraciones sobre cada diagonal en orden, empezando en el origen yprocediendo hacia fuera. A las iteraciones a lo largo de cada diagonal se les conoce como frentede onda (wavefront), y a un esquema de paralelización de este tipo se le denomina formación defrentes de ondas (wavefronting).11.9.3 Ciclos completamente permutablesPrimero introduciremos la noción de la permutabilidad completa, un concepto útil para la canalización y otras optimizaciones. Los ciclos son completamente permutables si pueden permutarse enforma arbitraria, sin cambiar la semántica del programa original. Una vez que los ciclos se colocanen una forma completamente permutable, podemos canalizar el código con facilidad y aplicartransformaciones como el uso de bloques, para mejorar la localidad de los datos.El código de la SOR, como está escrito en la figura 11.50(a), no es completamente permutable. Como se muestra en la sección 11.7.8, la permutación de dos ciclos significa que las iteraciones en el espacio de iteraciones original se ejecutan columna por columna, en vez de hacerlofila por fila. Por ejemplo, el cómputo original en la iteración [2, 3] se ejecutaría antes del de laiteración [1, 4], violando las dependencias que se muestran en la figura 11.50(b).Sin embargo, podemos transformar el código para hacerlo completamente permutable. Alaplicar al código la siguiente transformación afín:se produce el código que se muestra en la figura 11.51(a). Este código transformado es completamente permutable, y su versión permutada se muestra en la figura 11.51(c). Tambiénmostramos el espacio de iteraciones y las dependencias de datos de estos dos programas en lasfiguras 11.51(b) y (d), respectivamente. De la figura podemos ver con facilidad que este ordenpreserva el orden relativo entre cada par de accesos dependiente de datos.Al permutar ciclos, modificamos en forma drástica el conjunto de las operaciones que seejecutan en cada iteración del ciclo más externo. El hecho de tener este grado de libertad en laprogramación significa que hay mucho descuido en el orden de las operaciones en el programa.El descuido en la programación se traduce en oportunidades para la paralelización. Más adelanteen esta sección mostraremos que si un ciclo tiene k ciclos externos completamente permutables,al introducir sólo O (n) sincronizaciones podemos obtener O (k − 1) grados de paralelismo (n es elnúmero de iteraciones en un ciclo).11.9.4 Canalización de ciclos completamente permutablesUn ciclo con k ciclos externos completamente permutables puede estructurarse como una canalización con O (k − 1) dimensiones. En el ejemplo de la SOR, k = 2, por lo que podemosestructurar los procesadores como una canalización lineal.7 Es decir, las secuencias de puntos que se forman al moverse 1 hacia abajo y 2 a la derecha, en forma repetida.Maq. Cap_11_AHO B.indd 864 11/10/07 8:21:07 PMjiji(a) El código de la figura 11.50, transformado por(b) Dependencias de datos del código en (a).(c) Una permutación de los ciclos en (a).(d) Dependencias de datos del código en (b).Figura 11.51: Versión completamente permutable del código de la figura 11.5011.9 Canalizaciones 865Maq. Cap_11_AHO B.indd 865 11/10/07 8:21:08 PM866 Capítulo 11. Optimización para el paralelismo y la localidadPodemos canalizar el código de la SOR en dos formas distintas, las cuales se muestran enlas figuras 11.52(a) y (b), y corresponden a las dos permutaciones posibles que se muestranen las figuras 11.51(a) y (c), respectivamente. En cada caso, cada columna del espacio deiteraciones constituye una tarea, y cada fila constituye una etapa. Asignamos la etapa i alprocesador i, por lo cual cada procesador ejecuta el ciclo interno del código. Si ignoramos lascondiciones delimitadoras, un procesador puede ejecutar la iteración i sólo hasta después deque el procesador p − 1 ha ejecutado la iteración i − 1.(b) Procesadores asignados a las columnas.(a) Procesadores asignados a las filas.Figura 11.52: Dos implementaciones de canalización del código de la figura 11.51Suponga que cada procesador requiere exactamente la misma cantidad de tiempo paraejecutar una iteración, y que la sincronización ocurre en forma instantánea. Ambos esquemascanalizados ejecutarían las mismas iteraciones en paralelo; la única diferencia es que tienendistintas asignaciones de procesadores. Todas las iteraciones ejecutadas en paralelo se encuentran a lo largo de las diagonales de 135º en el espacio de iteraciones de la figura 11.51(b), quecorresponde a las diagonales de 150º en el espacio de iteraciones del código original; vea lafigura 11.50(b).No obstante, en la práctica los procesadores con cachés no siempre ejecutan el mismo códigoen la misma cantidad de tiempo, y el tiempo para la sincronización también varía. A diferenciadel uso de barreras de sincronización, lo cual obliga a todos los procesadores a operar en pasossincronizados, la canalización requiere que los procesadores se sincronicen y se comuniquencomo máximo dos procesadores más. Por ende, la canalización tiene frentes de onda relajados,lo cual permite que ciertos procesadores se adelanten impulsivamente, al tiempo que otros seretrasen por unos momentos. Esta flexibilidad reduce el tiempo que invierten los procesadoresesperando a los demás procesadores, y mejora el rendimiento en paralelo.Los esquemas de canalización antes mostrados son sólo dos de las muchas formas en lasque puede canalizarse el cómputo. Como dijimos antes, una vez que un ciclo es completamenteMaq. Cap_11_AHO B.indd 866 11/10/07 8:21:09 PMpermutable, tenemos mucha libertad en la forma en la que queramos paralelizar el código. Elprimer esquema de canalización asigna la iteración [i, j ] al procesador i; el segundo asigna laiteración [i, j ] al procesador j. Podemos crear canalizaciones alternativas al asignar la iteración[i, j ] al procesador c0 i + c 1 j, siempre y cuando c0 y c 1 sean constantes positivas. Dicho esquemacrearía canalizaciones con frentes de ondas relajados entre 90º y 180º, ambos exclusivos.11.9.5 Teoría generalEl ejemplo que acabamos de completar ilustra la siguiente teoría general de las canalizaciones: sipodemos idear cuando menos dos ciclos más externos distintos para un anidamiento de ciclos ycumplir con todas las dependencias, entonces podemos canalizar el cómputo. Un ciclo con k ciclos más externos, completamente permutables, tiene k − 1 grados de paralelismo canalizado.Los ciclos que no pueden canalizarse no tienen ciclos más externos alternativos. El ejemplo11.56 muestra una instancia de ese tipo. Para respetar todas las dependencias, cada iteraciónen el ciclo más externo debe ejecutar precisamente el cómputo que se encuentra en el códigooriginal. Sin embargo, dicho código puede contener aún paralelismo en los ciclos internos, locual puede explotarse mediante la introducción de por lo menos n sincronizaciones, en donden es el número de iteraciones en el ciclo más externo.s s 1 2Figura 11.53: Un ciclo externo secuencial (a) y su PDG (b)Ejemplo 11.56: La figura 11.53 es una versión más compleja del problema que vimos en elejemplo 11.50. Como se muestra en el grafo de dependencias del programa en la figura 11.53(b),las instrucciones s 1 y s 2 pertenecen al mismo componente fuertemente conectado. Como noconocemos el contenido de la matriz A, debemos suponer que el acceso en la instrucción s 2puede leer de cualquiera de los elementos de X. Hay una dependencia verdadera de la instrucción s 1 a la instrucción s 2, y una antidependencia de la instrucción s 2 a la instrucción s 1.11.9 Canalizaciones 867Maq. Cap_11_AHO B.indd 867 11/10/07 8:21:11 PM868 Capítulo 11. Optimización para el paralelismo y la localidadNo hay oportunidad para la canalización en ninguno de los casos, ya que todas las operacionesque pertenecen a la iteración i en el ciclo externo deben preceder a las que están en la iteracióni + 1. Para encontrar más paralelismo, repetimos el proceso de paralelización en el ciclo interno. Las iteraciones en el segundo ciclo pueden paralelizarse sin sincronización. Por lo tanto, serequieren 200 barreras, con una antes y una después de cada ejecución del ciclo interno. ✷11.9.6 Restricciones de partición de tiempoAhora nos enfocaremos en el problema de buscar paralelismo canalizado. Nuestro objetivo esconvertir un cómputo en un conjunto de tareas canalizables. Para buscar paralelismo canalizado, no resolvemos directamente lo que se va a ejecutar en cada procesador, como hicimos con laparalelización de ciclos. En vez de ello, hacemos la siguiente pregunta fundamental: ¿Cuáles sontodas las posibles secuencias de ejecución que respetan a las dependencias de datos originalesen el ciclo? Es obvio que la secuencia de ejecución original satisface todas las dependencias dedatos. La pregunta es si hay transformaciones afines que puedan crear un programa alternativo, en donde las iteraciones del ciclo más externo ejecuten un conjunto distinto de operacionesdel programa original, y aún así se cumpla con todas las dependencias. Si podemos encontrardichas transformaciones, podemos canalizar el ciclo. El punto clave es que si hay libertad enla programación de las operaciones, hay paralelismo; más adelante explicaremos los detallesacerca de cómo derivar el paralelismo canalizado a partir de dichas transformaciones.Para encontrar reordenamientos aceptables del ciclo externo, es conveniente encontrar transformaciones afines unidimensionales, una para cada instrucción, que asignen los valores índicede ciclo originales a un número de iteración en el ciclo más externo. Las transformaciones sonválidas si la asignación puede cumplir con todas las dependencias de datos en el programa. Las“restricciones de partición de tiempo”, que se muestran a continuación, sólo indican que si unaoperación es dependiente de la otra, entonces a la primera se le debe asignar una iteración en elciclo más externo, no antes que la de la segunda. Si se les asigna la misma iteración, entoncesse comprende que la primera se ejecutará después de la segunda dentro de la iteración.Una asignación de particiones afines de un programa es una partición de tiempo válida si,y sólo si para cada dos accesos (no necesariamente distintos) que comparten una dependencia,por decir:en la instrucción s 1, que está anidada en d 1 ciclos, y:en la instrucción s 2, anidada en d 2 ciclos, las asignaciones de particiones unidimensionales C1,c1 y C2, c2 para las instrucciones s 1 y s 2, respectivamente, cumplen con las siguientes restricciones de partición de tiempo:• Para todas las i1 en Zd 1 y todas las i 2 en Zd 2 tales que: a) i1 ≺s 1s 2 i 2,Maq. Cap_11_AHO B.indd 868 11/10/07 8:21:12 PM b) B1i1 + b1 ≥ 0, c) B2i 2 + b2 ≥ 0, y d) F1i1 + f1 = F2i 2 + f2,se da el caso en el que C1i1 + c1 ≤ C2i 2 + c2.Esta restricción, que se ilustra en la figura 11.54, es bastante parecida a las restricciones departición de espacio. Es una relajación de las restricciones de partición de espacio, en cuanto aque si dos iteraciones se refieren a la misma ubicación, no necesariamente tienen que asignarsea la misma partición; sólo requerimos que se preserve el orden de ejecución relativo originalentre las dos iteraciones. Es decir, las restricciones aquí tienen ≤ en donde las restricciones departición de espacio tienen =.F i + f 11 1C i + c 1 11i 2iArregloC i + cF i + f 22 222 2Pasos de tiempo1<Figura 11.54: Restricciones de partición de tiempoSabemos que existe por lo menos una solución a las restricciones de partición de tiempo.Podemos asignar las operaciones en cada iteración del ciclo más externo de vuelta a la mismaiteración, y se cumplirán todas las dependencias de datos. Esta solución es la única para lasrestricciones de partición de tiempo, para los programas que no pueden canalizarse. Por otrolado, si podemos encontrar varias soluciones independientes a las restricciones de partición detiempo, el programa puede canalizarse. Cada solución independiente corresponde a un cicloen el anidamiento más externo completamente permutable. Como es lógico, sólo hay una solución independiente para las restricciones de sincronización extraídas del programa del ejemplo11.56, en donde no hay paralelismo canalizado, y que hay dos soluciones independientes parael ejemplo del código de la SOR.Ejemplo 11.57: Vamos a considerar el ejemplo 11.56, y en especial las dependencias de datosde las referencias al arreglo X en las instrucciones s 1 y s 2. Como el acceso no es afín en la ins11.9 Canalizaciones 869Maq. Cap_11_AHO B.indd 869 11/10/07 8:21:13 PM870 Capítulo 11. Optimización para el paralelismo y la localidadtrucción s 2, para aproximar el acceso modelamos la matriz X simplemente como una variableescalar en el análisis de dependencias que involucra a la instrucción s 2. Hagamos que (i, j ) seael valor índice de una instancia dinámica de s 1 y hagamos que i sea el valor índice de unainstancia dinámica de s 2. Hagamos que las asignaciones de cómputo de las instrucciones s 1 ys 2 sean [C 11, C 12 ], c 1 y [C 21 ], c 2, respectivamente.Primero vamos a considerar las restricciones de partición de tiempo impuestas por las dependencias de la instrucción s 1 a s 2. Por ende, i ≤ i, la (i, j )-ésima iteración transformada des 1 no debe ser posterior a la i-ésima iteración transformada de s 2; es decir,Al expandir, obtenemos lo siguiente:Como j puede ser arbitrariamente grande, independiente de i e i, debe ser que C 12 = 0. Porende, una posible solución a las restricciones es:yLos argumentos similares acerca de la dependencia de datos de s 2 a s 1 y de s 2 de vuelta así misma producirán una respuesta similar. En esta solución específica, la i-ésima iteración delciclo externo, que consiste en la instancia i de s 2 y en todas las instancias (i, j ) de s 1, se asignantodas al salto temporal i. Otras opciones válidas de C 11, C 21, c 1 y c 2 producen asignacionessimilares, aunque podría haber saltos temporales en los que no ocurra nada. Es decir, todas lasformas de programar el ciclo externo requieren que las iteraciones se ejecuten en el mismo ordenque en el código original. Esta instrucción es válida ya sea si todas las 100 iteraciones se ejecutanen el mismo procesador, en 100 procesadores distintos, o en cualquier cosa intermedia. ✷Ejemplo 11.58: En el código de la SOR que se muestra en la figura 11.50(a), la referencia deescritura X [ j + 1] comparte una dependencia consigo misma y con las tres referencias de lectura en el código. Estamos buscando la asignación del cómputo [C 1, C 2], c para la instrucciónde asignación tal que:si hay una dependencia de (i, j ) a (i, j ). Por definición, (i, j ) ≺ (i, j ); es decir, ya sea quei < i o (i = i ∧ j < j )).Vamos a considerar tres de los pares de dependencias de datos:1. La dependencia verdadera del acceso de escritura X [ j + 1] al acceso de lectura X [ j + 2].Como las instancias deben acceder a la misma ubicación, j + 1 = j + 2 o j = j + 1. Alsustituir j = j + 1 en las restricciones de sincronización, obtenemos:Maq. Cap_11_AHO B.indd 870 11/10/07 8:21:14 PMComo j = j + 1, j > j, las restricciones de precedencia se reducen a i < i. Por lo tanto,2. La antidependencia del acceso de lectura X [ j + 2 ] al acceso de escritura X [ j + 1]. Aquí,j + 2 = j + 1, o j = j − 1. Al sustituir j = j − 1 en las restricciones de sincronización,obtenemos:Cuando i = i, obtenemos:Cuando i < i, ya que C 2 ≥ 0, obtenemos:3. La dependencia de salida del acceso X [ j + 1] de vuelta a sí mismo. Aquí j = j. Las restricciones de sincronización se reducen a:Como sólo i < i es relevante, obtenemos de nuevo:El resto de las dependencias no producen nuevas restricciones. En total, hay tres restricciones:He aquí dos soluciones independientes para estas restricciones:La primera solución preserva el orden de ejecución de las iteraciones en el ciclo más externo.Tanto el código de la SOR original en la figura 11.50(a) como el código transformado que semuestra en la figura 11.51(a) son ejemplos de dicho arreglo. La segunda solución coloca las iteraciones a lo largo de las diagonales de 135º en el mismo ciclo externo. El código que se muestraen la figura 11.51(b) es un ejemplo de un código con esa composición de ciclo más externo.Observe que existen muchos otros posibles pares de soluciones independientes. Por ejemplo,también serían soluciones independientes para las mismas restricciones. Elegimos los vectoresmás simples para simplificar la transformación de código. ✷11.9 Canalizaciones 871Maq. Cap_11_AHO B.indd 871 11/10/07 8:21:15 PM872 Capítulo 11. Optimización para el paralelismo y la localidad11.9.7 Resolución de restricciones de partición de tiempomediante el Lema de FarkasComo las restricciones de partición de tiempo son similares a las restricciones de partición deespacio, ¿podemos usar un algoritmo similar para resolverlas? Por desgracia, la ligera diferencia entre los dos problemas se traduce en una gran diferencia técnica entre los dos métodos desolución. El Algoritmo 11.43 simplemente resuelve para C1, c1, C2 y c2, de tal forma que paratodas las i1 en Zd1 y todas las i2 en Zd 2, si:entonces:Las desigualdades lineales debido a los límites de ciclo se utilizan sólo para determinar si dosreferencias comparten una dependencia de datos, y no se utilizan en ningún otro caso.Para buscar soluciones a las restricciones de partición de tiempo, no podemos ignorar lasdesigualdades lineales i ≺ i; por lo general, ignorarlas sólo permitiría la solución trivial decolocar todas las iteraciones en la misma partición. Por ende, el algoritmo para encontrar soluciones a las restricciones de partición de tiempo debe manejar tanto las igualdades como lasdesigualdades.El problema general que deseamos resolver es: dada una matriz A, encontrar un vector cde manera que, para todos los vectores x tales que Ax ≥ 0, se dé el caso de que cT x ≥ 0.En otras palabras, estamos buscando c de tal forma que el producto interno de c y cualquiercoordenada en el poliedro definida por las desigualdades Ax ≥ 0 siempre produzca una respuesta no negativa.Para este problema utilizamos el Lema de Farkas. Hagamos que A sea una matriz m × nde números reales, y que c sea un n-vector real, distinto de cero. El lema de Farkas estableceque el sistema fundamental de desigualdades:tiene una solución x con valor real, o que el sistema dual:tiene una solución y con valor real, pero nunca ambos.El sistema dual puede manejarse mediante el uso de la eliminación de Fourier-Motzkin paraproyectar a lo lejos las variables de y. Para cada c que tiene una solución en el sistema dual, ellema garantiza que no hay soluciones para el sistema fundamental. Dicho de otra forma, podemos probar la negación del sistema fundamental; es decir, podemos demostrar que cTx ≥ 0para todas las x tales que Ax ≥ 0, encontrando una solución y al sistema dual: ATy = c yy ≥ 0.Algoritmo 11.59: Buscar un conjunto de asignaciones válidas de partición de tiempo afines,muy independientes, para un ciclo secuencial externo.Maq. Cap_11_AHO B.indd 872 11/10/07 8:21:16 PMAcerca del Lema de FarkasLa prueba del lema puede encontrarse en muchos libros de texto estándar sobre programación lineal. El Lema de Farkas, que se demostró por primera vez en 1901, es uno delos teoremas de la alternativa. Estos teoremas son todos equivalentes, pero a pesar de losintentos a través de los años, no se ha encontrado una prueba simple e intuitiva de estelema, o de alguno de sus equivalentes.ENTRADA: Un anidamiento de ciclos con accesos a arreglos.SALIDA: Un conjunto máximo de asignaciones de partición de tiempo linealmente independientes.MÉTODO: Los siguientes pasos constituyen el algoritmo:1. Buscar todos los pares dependientes de datos de accesos en un programa.2. Para cada par de accesos dependientes de datos, F1 = F1, f1, B1, b1 en la instruccións 1, anidados en d 1 ciclos, y F2 = F2, f2, B2, b2 en la instrucción s 2, anidados en d 2ciclos, hagamos que C1, c 1 y C2, c 2 sean las asignaciones de partición de tiempo (desconocidas) de las instrucciones s 1 y s 2, respectivamente. Recuerde que las restriccionesde partición de tiempo establecen que: • Para todas las i1 en Zd1 y todas las i 2 en Zd2 tales que:a) i1 ≺s 1 s 2 i 2,b) B1i1 + b1 ≥ 0,c) B2i 2 + b2 ≥ 0, yd) F1i1 + f1 = F2i 2 + f2, se da el caso de que C1i1 + c1 ≤ C2i 2 + c2.Como i1 ≺s 1 s 2 i 2 es una unión disyuntiva de un número de cláusulas, podemos crear unsistema de restricciones para cada cláusula y resolver cada una de ellas por separado,como se muestra a continuación: (a) De manera similar al paso (2a) en el Algoritmo 11.43, se aplica la eliminación gaussiana a las ecuaciones: para reducir el vector: a cierto vector de variables desconocidas, x.11.9 Canalizaciones 873Maq. Cap_11_AHO B.indd 873 11/10/07 8:21:17 PM874 Capítulo 11. Optimización para el paralelismo y la localidad (b) Hacemos que c sea todas las variables desconocidas en las asignaciones de partición.Se expresan las restricciones de desigualdad lineales debido a las asignaciones departición como: para cierta matriz D. (c) Se expresan las restricciones de precedencia en las variables índice de ciclo y en loslímites de ciclo como: para cierta matriz A. (d) Se aplica el Lema de Farkas. Buscar x para satisfacer las dos restricciones anterioreses equivalente a buscar y de tal forma que:y Observe que cTD aquí es cT en la instrucción del Lema de Farkas, y estamos usandola forma negada del lema. (e) De esta forma, se aplica la eliminación de Fourier-Motzkin para proyectar a lo lejoslas variables y, y se expresan las restricciones en los coeficientes c como Ec ≥ 0. (f) Hacemos que Ec ≥ 0 sea el sistema sin los términos constantes.3. Buscar un conjunto máximo de soluciones linealmente independientes a Ec ≥ 0, usandoel Algoritmo B.1 del apéndice B. El método de ese algoritmo complejo es llevar la cuentadel conjunto actual de soluciones para cada una de las instrucciones, y después buscarcada vez más soluciones independientes, insertando restricciones que obliguen a que lasolución sea linealmente independiente durante por lo menos una instrucción.4. De cada solución de c que se haya encontrado, derivar una asignación de partición detiempo afín. Los términos constantes se derivan usando Ec ≥ 0.✷Ejemplo 11.60: Las restricciones para el ejemplo 11.57 pueden escribirse así:Maq. Cap_11_AHO B.indd 874 11/10/07 8:21:18 PMEl lema de Farkas establece que estas restricciones son equivalentes a:yAl resolver este sistema, obtenemos:yObserve que estas restricciones se satisfacen mediante la solución específica que obtuvimos enel ejemplo 11.57. ✷11.9.8 Transformaciones de códigoSi existen k soluciones independientes a las restricciones de partición de tiempo de un anidamiento de ciclos, entonces es posible transformar el anidamiento de ciclos para que tenga k ciclos másexternos completamente permutables, los cuales pueden transformarse para crear k − 1 gradosde canalización, o para crear k − 1 ciclos internos paralelizables. Además. Podemos aplicar eluso de bloques a los ciclos completamente permutables para mejorar la localidad de datos de losuniprocesadores, así como para reducir la sincronización entre los procesadores, en una ejecuciónen paralelo.Explotación de ciclos completamente permutablesPodemos crear fácilmente un anidamiento de ciclos con k ciclos más externos completamente permutables, a partir de k soluciones independientes a las restricciones de partición de tiempo. Para ello, sólo debemos hacer que la k-ésima solución sea la k-ésima fila de la nueva transformación. Una vez que se crea la transformación afín, podemos usar el Algoritmo 11.45 paragenerar el código.Ejemplo 11.61: Las soluciones encontradas en el ejemplo 11.58 para nuestro ejemplo de laSOR fueron:Si hacemos que la primera solución sea la primera fila y que la segunda solución sea la segundafila, obtenemos la siguiente transformación:lo cual produce el código de la figura 11.51(a).Si en vez de eso hacemos que la segunda solución sea la primera fila, obtenemos la siguientetransformación:lo cual produce el código de la figura 11.51(c). ✷11.9 Canalizaciones 875Maq. Cap_11_AHO B.indd 875 11/10/07 8:21:19 PM876 Capítulo 11. Optimización para el paralelismo y la localidadEs fácil ver que dichas transformaciones producen un programa secuencial válido. La primera fila particiona todo el espacio de iteraciones de acuerdo con la primera solución. Lasrestricciones de sincronización garantizan que dicha descomposición no viole ninguna dependencia de datos. Después, particionamos las iteraciones en cada uno de los ciclos más externos,de acuerdo con la segunda solución. De nuevo, esto debe ser válido, ya que estamos tratandosólo con subconjuntos del espacio de iteraciones original. Lo mismo se aplica para el resto delas filas en la matriz. Como podemos ordenar las soluciones en forma arbitraria, los ciclos soncompletamente permutables.Explotación de las canalizacionesPodemos transformar con facilidad un ciclo con k ciclos más externos completamente permutables, en un código con k − 1 grados de paralelismo de canalización.Ejemplo 11.62: Vamos a regresar a nuestro ejemplo de la SOR. Una vez que los ciclos setransforman para ser completamente permutables, sabemos que la iteración [i 1, i 2 ] puede ejecutarse, siempre y cuando se hayan ejecutado las iteraciones [i 1, i 2 − 1] y [i 1 − 1, i 2 ]. Podemosgarantizar este orden en una canalización de la siguiente manera. Asignamos la iteración i 1 alprocesador p 1. Cada procesador ejecuta iteraciones en el ciclo interno, en el orden secuencialoriginal, con lo cual se garantiza que la iteración [i 1, i 2 ] se ejecute después de [i 1, i 2 − 1]. Además, es necesario que el procesador p espere la señal del procesador p − 1 de que ha ejecutadola iteración [p − 1, i 2 ] antes de que ejecute la iteración [p, i 2 ]. Esta técnica genera el códigocanalizado de las figuras 11.52(a) y (b), a partir de los ciclos completamente permutables delas figuras 11.51(a) y (c), respectivamente. ✷En general, dados k ciclos más externos completamente permutables, la iteración con losvalores índice (i 1,…, ik ) pueden ejecutarse sin violar las restricciones de dependencia de datos,siempre y cuando se hayan ejecutado las siguientes iteraciones:Por lo tanto, podemos asignar las particiones de las primeras k − 1 dimensiones del espacio deiteraciones a O (nk− 1) procesadores, de la siguiente manera. Cada procesador es responsable de unconjunto de iteraciones cuyos índices concuerdan en las primeras k − 1 dimensiones, y varían sobretodos los valores del k-ésimo índice. Cada procesador ejecuta las iteraciones en el k-ésimo ciclo demanera secuencial. El procesador correspondiente a los valores [p 1, p 2,…, p k − 1] para los primerosk − 1 índices de ciclo puede ejecutar la iteración i en el k-ésimo ciclo, siempre y cuando reciba unaseñal de los procesadores:de que han ejecutado su i-ésima iteración en el k-ésimo ciclo.Frentes de ondaTambién es fácil generar k − 1 ciclos internos paralelizables a partir de un ciclo con k ciclos másexternos completamente permutables. Aunque es preferible la canalización, incluimos aquí estainformación para tener una idea más completa.Maq. Cap_11_AHO B.indd 876 11/10/07 8:21:20 PMParticionamos el cómputo de un ciclo con k ciclos más externos completamente permutablesmediante el uso de una nueva variable índice i, en donde i se define como cierta combinaciónde todos los índices en el anidamiento de ciclos permutable k. Por ejemplo, i = i 1 + . . . + i kes una de esas combinaciones.Creamos un ciclo secuencial más externo que itere a través de las i particiones en ordenincremental; el cómputo anidado dentro de cada partición se ordena como antes. Se garantizaque los primeros k − 1 ciclos dentro de cada partición son paralelizables. Por intuición, si seproporciona un espacio de iteraciones bidimensional, esta transformación agrupa las iteraciones a lo largo de diagonales de 135º como una ejecución del ciclo más externo. Esta estrategiagarantiza que las iteraciones dentro de cada iteración del ciclo más externo no tengan dependencia de datos.Uso de bloquesUn ciclo con k niveles, completamente permutable, puede dividirse en bloques de k dimensiones.En vez de asignar las iteraciones a los procesadores con base en el valor de los índices externoo interno, podemos juntar bloques de iteraciones en una unidad. Los bloques son útiles paramejorar la localidad de los datos, así como para minimizar la sobrecarga de la canalización.Suponga que tenemos un anidamiento de ciclos bidimensional, completamente permutable,como en la figura 11.55(a), y deseamos descomponer el cómputo en b × b bloques. El orden deejecución del código en bloques se muestra en la figura 11.56, y el código equivalente está enla figura 11.55(b).Si asignamos cada bloque a un procesador, entonces toda la acción de pasar datos de unaiteración a otra que esté dentro de un bloque no requiere comunicación entre los procesadores.De manera alternativa, podemos aumentar la granularidad de la canalización al asignar unacolumna de bloques a un procesador. Observe que cada procesador se sincroniza con sus predecesores y sucesores sólo en los límites de los bloques. Así, otra ventaja del uso de bloques es quelos programas sólo tienen que comunicar los datos a los que acceden en los límites del bloque consus bloques vecinos. Los valores interiores para un bloque los maneja un solo procesador.Ejemplo 11.63: Ahora vamos a usar un algoritmo numérico real (descomposición de Cholesky) para ilustrar cómo el Algoritmo 11.59 maneja los anidamientos de ciclos individuales sólocon paralelismo de canalización. El código, que se muestra en la figura 11.57, implementa a unalgoritmo O (n 3), que opera sobre un arreglo de datos bidimensional. El espacio de iteracionesejecutado es una pirámide triangular, ya que j sólo itera hasta el valor del índice i del ciclointerno, y k sólo itera hasta el valor de j. El ciclo tiene cuatro instrucciones, todas anidadas endistintos ciclos.Al aplicar el Algoritmo 11.59 a este programa encontramos tres dimensiones de tiempoválidas. Anida todas las operaciones, algunas de las cuales estaban anidadas originalmente enanidamientos de ciclos de 1 y 2 niveles, en un anidamiento de ciclos tridimensional, completamente permutable. En la figura 11.58 se muestra el código, junto con las asignaciones.La rutina de generación de código protege la ejecución de las operaciones con los límitesde ciclo originales, para asegurar que los nuevos programas sólo ejecuten operaciones que seencuentren en el código original. Podemos canalizar el código asignando la estructura tridimensional a un espacio de procesadores bidimensional. Las iteraciones (i 2, j 2, k 2) se asignan11.9 Canalizaciones 877Maq. Cap_11_AHO B.indd 877 11/10/07 8:21:21 PM878 Capítulo 11. Optimización para el paralelismo y la localidad(a) Un anidamiento de ciclos simple.(b) Una versión en bloques de este anidamiento de ciclos.Figura 11.55: Un anidamiento de ciclos bidimensional y su versión en bloquesjij(a) Antes. (b) Después.iFigura 11.56: Orden de ejecución, antes y después de usar bloques en un anidamiento de ciclosde 2 nivelesMaq. Cap_11_AHO B.indd 878 11/10/07 8:21:22 PMFigura 11.57: Descomposición de Cholesky/* inicio del codigo para el procesador (i2,j2) */// Asignacion: i2 = i, j2 = j, k2 = k// Asignacion: i2 = i, j2 = j, k2 = j// Asignacion: i2 = i, j2 = i, k2 = m// Asignacion: i2 = i, j2 = i, k2 = i/* fin del codigo para el procesador (i2,j2) */Figura 11.58: La figura 11.57 escrita como un anidamiento de ciclos completamente permutable11.9 Canalizaciones 879Maq. Cap_11_AHO B.indd 879 11/10/07 8:21:23 PM880 Capítulo 11. Optimización para el paralelismo y la localidadal procesador con el ID (i 2, j 2). Cada procesador ejecuta el ciclo más interno, el ciclo con elíndice k 2. Antes de ejecutar la k-ésima iteración, el procesador espera las señales de los procesadores con los IDs (i 2 − 1, j 2) y (i 2, j 2 − 1). Después de ejecutar su iteración, envía unaseñal a los procesadores (i 2 + 1, j 2) y (i 2, j 2 + 1). ✷11.9.9 Paralelismo con sincronización mínimaEn las últimas tres secciones, hemos descrito tres poderosos algoritmos de paralelización: el Algoritmo 11.43 encuentra todo el paralelismo que no requiera sincronizaciones, el Algoritmo 11.54encuentra todo el paralelismo que requiera sólo un número constante de sincronizaciones, y elAlgoritmo 11.59 encuentra todo el paralelismo canalizable que requiera O (n) sincronizaciones, endonde n es el número de iteraciones en el ciclo más externo. Como primera aproximación, nuestra meta es paralelizar todo el cómputo que sea posible, e introducir al mismo tiempo la menorcantidad de sincronización que sea necesaria.El Algoritmo 11.64, que se muestra a continuación, encuentra todos los grados de paralelismo en un programa, empezando con la granularidad más grande del paralelismo. En lapráctica, para paralelizar un código para un multiprocesador, no es necesario explotar todos losniveles de paralelismo, sólo los más externos que sea posible hasta paralelizar todo el cómputo,y hasta que se utilicen todos los procesadores por completo.Algoritmo 11.64: Encontrar todos los grados de paralelismo en un programa, en donde todoel paralelismo debe tener la mayor granularidad posible.ENTRADA: Un programa que se va a paralelizar.SALIDA: Una versión paralelizada del mismo programa.MÉTODO: Haga lo siguiente:1. Encuentre el máximo grado de paralelismo que no requiera sincronización: aplique elAlgoritmo 11.43 al programa.2. Encuentre el máximo grado de paralelismo que requiere O (1) sincronizaciones: aplique elAlgoritmo 11.54 a cada una de las particiones de espacio descubiertas en el paso 1. Si nose encuentra ningún paralelismo sin sincronización, el cómputo completo se deja en unapartición.3. Encuentre el máximo grado de paralelismo que requiera O (n) sincronizaciones. Aplique el Algoritmo 11.59 a cada una de las particiones descubiertas en el paso 2, para encontrar el paralelismo canalizado. Después aplique el Algoritmo 11.54 a cada una de lasparticiones asignadas a cada procesador, o al cuerpo del ciclo secuencial si no se encuentracanalización.4. Encuentre el máximo grado de paralelismo con grados de sincronizaciones cada vez mayores: aplique en forma recursiva el paso 3 al cómputo que pertenezca a cada una de lasparticiones de espacio generadas por el mapa anterior. ✷Maq. Cap_11_AHO B.indd 880 11/10/07 8:21:25 PMEjemplo 11.65: Vamos ahora a regresar al ejemplo 11.56. Los pasos 1 y 2 del Algoritmo 11.54no encuentran paralelismo; es decir, necesitamos más que un número constante de sincronizaciones para paralelizar este código. En el paso 3, al aplicar el Algoritmo 11.59 determinamosque sólo hay un ciclo externo válido, que es el mismo del código original en la figura 11.53. Porlo tanto, el ciclo no tiene paralelismo canalizado. En la segunda parte del paso 3, aplicamos elAlgoritmo 11.54 para paralelizar el ciclo interno. Tratamos el código dentro de una particióncomo un programa completo, en donde la única diferencia es que el número de partición se trata como una constante simbólica. En este caso, encontramos que el ciclo interno es paralelizabley, por lo tanto, el código puede paralelizarse con n barreras de sincronización. ✷El Algoritmo 11.64 encuentra todo el paralelismo en un programa, en cada nivel de sincronización. El algoritmo prefiere esquemas de paralelización que tienen menos sincronización,pero menos sincronización no significa que se minimiza la comunicación. Aquí veremos dosextensiones del algoritmo para lidiar con sus debilidades.Consideración del costo de comunicaciónEl paso 2 del Algoritmo 11.64 paraleliza cada componente fuertemente conectado, sin importarque se encuentre paralelismo sin sincronización o no. Sin embargo, puede ser posible paralelizarvarios componentes sin sincronización ni comunicación. Una solución es buscar masivamente elparalelismo sin sincronización entre los subconjuntos del grafo de dependencias del programaque comparten la mayor parte de los datos.Si es necesaria la comunicación entre los componentes fuertemente conectados, observamosque cierta comunicación es más costosa que otras. Por ejemplo, el costo de transportar unamatriz es mucho más alto que sólo tener que comunicarse entre los procesadores adyacentes.Suponga que s 1 y s 2 son instrucciones en dos componentes fuertemente conectados separados,que acceden a los mismos datos en las iteraciones i1 e i 2, respectivamente. Si no podemosencontrar las asignaciones de partición C1, c1 y C2, c2 para las instrucciones s 1 y s 2, respectivamente, de forma que:en vez de ello tratamos de satisfacer la restricción:en donde δ es una constante pequeña.Intercambio de comunicación por sincronizaciónAlgunas veces es mejor realizar más sincronización para minimizar la comunicación. El ejemplo11.66 trata acerca de esto. Por lo tanto, si no podemos paralelizar un código sólo mediante11.9 Canalizaciones 881Maq. Cap_11_AHO B.indd 881 11/10/07 8:21:26 PM882 Capítulo 11. Optimización para el paralelismo y la localidadcomunicación entre los componentes fuertemente conectados de la proximidad, debemos tratarde canalizar el cómputo en vez de paralelizar cada componente por separado. Como se muestraen el ejemplo 11.66, la canalización puede aplicarse a una secuencia de ciclos.Ejemplo 11.66: Para el algoritmo de integración ADI en el ejemplo 11.49, hemos mostradoque al optimizar los anidamientos de ciclos primero y segundo en forma independiente, se encuentra paralelismo en cada uno de ellos. Sin embargo, dicho esquema requeriría transponer lamatriz entre los ciclos, incurriendo en un tráfico de datos determinado por O (n 2). Si utilizamosel Algoritmo 11.59 para buscar paralelismo canalizado, encontramos que podemos convertirtodo el programa completo en un anidamiento de ciclos completamente permutable, como enla figura 11.59. Entonces podemos aplicar bloques para reducir la sobrecarga de comunicación.Este esquema incurriría en O (n) sincronizaciones, pero requeriría de mucha menos comunicación. ✷Figura 11.59: Un anidamiento de ciclos completamente permutable para el código del ejemplo11.4911.9.10 Ejercicios para la sección 11.9Ejercicio 11.9.1: En la sección 11.9.4, hablamos sobre la posibilidad de utilizar diagonalesaparte de los ejes horizontal y vertical, para canalizar el código de la figura 11.51. Escriba código que sea análogo a los ciclos de la figura 11.52 para las diagonales: (a) 135º (b) 120º.Ejercicio 11.9.2: La figura 11.55(b) puede simplificarse si b divide a n de manera uniforme.Rescriba el código bajo esa suposición.Figura 11.60: Cálculo del triángulo de PascalMaq. Cap_11_AHO B.indd 882 11/10/07 8:21:26 PMEjercicio 11.9.3: En la figura 11.60 hay un programa para calcular las primeras 100 filas deltriángulo de Pascal. Es decir, P[i, j ] se convertirá en el número de formas en que se puedenelegir j objetos de i, para 0 ≤ j ≤ i < 100.a) Rescriba el código como un solo anidamiento de ciclos completamente permutable.b) Use 100 procesadores en una canalización para implementar este código. Escriba el código para cada procesador p, en términos de p, e indique la sincronización necesaria.c) Rescriba el código usando bloques cuadrados de 10 iteraciones en un lado. Como las iteraciones forman un triángulo, sólo habrá 1 + 2 + ... + 10 = 55 bloques. Muestre el códigopara un procesador (p 1, p 2) asignado al p 1-ésimo bloque en la dirección i, y al p 2-ésimobloque en la dirección j, en términos de p 1 y p 2.Figura 11.61: Código para el ejercicio 11.9.4Ejercicio 11.9.4: Repita el ejercicio 11.9.2 para el código de la figura 11.61. Sin embargo, observe que las iteraciones para este problema forman un cubo tridimensional cuyo lado es iguala 100. Por ende, los bloques para la parte (c) deben ser 10 × 10 × 10, y hay 1000 de ellos.Ejercicio 11.9.5: Vamos a aplicar el Algoritmo 11.59 a un ejemplo simple de las restriccionesde partición de tiempo. En lo que sigue, suponga que el vector i1 es (i 1, j 1), y que el vector i 2es (i 2, j 2); técnicamente, ambos vectores están transpuestos. La condición i1 ≺s 1 s 2 i 2 consiste enlas siguientes desuniones:i. i 1 < i 2, oii. i 1 = i 2 y j 1 < j 2.Las otras igualdades y desigualdades son:11.9 Canalizaciones 883!!Maq. Cap_11_AHO B.indd 883 11/10/07 8:21:27 PM884 Capítulo 11. Optimización para el paralelismo y la localidadPor último, la desigualdad de partición de tiempo, con las variables desconocidas c 1, d 1, e 1,c 2, d 2 y e 2 es:a) Resuelva las restricciones de partición de tiempo para el caso i; es decir, en donde i 1 < i 2.En especial, elimine todas las i 1, j 1, i 2 y j 2 que pueda, y establezca las matrices D y Acomo en el Algoritmo 11.59. Después, aplique el Lema de Farkas a las desigualdades resultantes de las matrices.b) Repita la parte (a) para el caso ii, en donde i 1 = i 2 y j 1 < j 2.11.10 Optimizaciones de localidadEl rendimiento de un procesador, ya sea que forme parte de un multiprocesador o no, es muysensible al comportamiento de su caché. Los fallos en la caché pueden requerir decenas de ciclodel reloj, por lo que las proporciones altas de fallos de caché pueden provocar un bajo rendimiento del procesador. En el contexto de un multiprocesador con un bus común de memoria, lacontención en el bus puede aumentar el efecto negativo de la mala localidad de los datos.Como veremos más adelante, incluso si sólo deseamos mejorar la localidad de los uniprocesadores, el algoritmo de particionamiento afín para la paralelización es útil como un medioen la identificación de oportunidades para las transformaciones de ciclo. En esta sección describiremos tres técnicas para mejorar la localidad de los datos en uniprocesadores y multiprocesadores.1. Para mejorar la localidad temporal de los resultados calculados, tratamos de usar los resultados tan pronto como se generan. Para ello, dividimos un cálculo en particiones independientes y ejecutamos todas las operaciones dependientes en cada partición lo más cercaposible.2. La contracción de arreglos reduce las dimensiones de un arreglo y reduce el número deubicaciones de memoria a las que se accede. Podemos aplicar la contracción de arreglossi sólo se utiliza una ubicación del arreglo en un momento dado.3. Además de mejorar la localidad temporal de los resultados calculados, también debemosoptimizar la localidad espacial de los resultados calculados, y la localidad temporal yespacial de los datos de sólo lectura. En vez de ejecutar cada partición, una después dela otra, intercalamos varias de las particiones, de manera que las reutilizaciones entreparticiones ocurran lo más cerca posible.Maq. Cap_11_AHO B.indd 884 11/10/07 8:21:29 PM11.10.1 Localidad temporal de los datos calculadosEl algoritmo de particionamiento afín junta todas las operaciones dependientes; al ejecutarestas operaciones en serie, mejoramos la localidad temporal de los datos calculados. Vamos aregresar al ejemplo con varias rejillas que vimos en la sección 11.7.1. Al aplicar el Algoritmo11.43 para paralelizar el código de la figura 11.23, encontramos dos grados de paralelismo. Elcódigo en la figura 11.24 contiene dos ciclos externos que iteran a través de la partición independiente en serie. Este código transformado tiene una localidad temporal mejorada, ya quelos resultados calculados se utilizan de inmediato en la misma iteración.Por ende, incluso si nuestra meta es optimizar la ejecución secuencial, es beneficioso utilizarla paralelización para buscar estas operaciones relacionadas y juntarlas. El algoritmo que utilizamos aquí es similar al Algoritmo 11.64, el cual busca todas las granularidades del paralelismo,empezando con el ciclo más externo. Como vimos en la sección 11.9.9, el algoritmo paralelizalos componentes fuertemente conectados por separado, si no podemos encontrar paralelismo sinsincronización en cada nivel. Esta paralelización tiende a incrementar la comunicación. Por ende,combinamos masivamente los componentes fuertemente conectados, paralelizados por separado,si comparten la reutilización.11.10.2 Contracción de arreglosLa optimización de la contracción de arreglos proporciona otra muestra de la concesión entreel almacenamiento y el paralelismo, que se presentó por primera vez en el contexto del paralelismo a nivel de instrucción, en la sección 10.2.3. Así como al usar más registros se permiteun mayor paralelismo a nivel de instrucción, el uso de más memoria permite más paralelismo anivel de ciclo. Como se muestra en el ejemplo con varias rejillas en la sección 11.7.1, al expandiruna variable escalar temporal en un arreglo se permite que distintas iteraciones mantengandistintas instancias de las variables temporales, y que se ejecuten al mismo tiempo. Por elcontrario, cuando tenemos una ejecución secuencial que opera sobre un elemento del arreglo enun momento dado en serie, podemos contraer el arreglo, sustituirlo con un escalar y hacer quecada iteración utilice la misma ubicación.En el programa multirrejillas transformado que se muestra en la figura 11.24, cada iteracióndel ciclo interno produce y consume un elemento distinto de AP, AM, T y una fila de D. Si estos arreglos no se utilizan fuera del extracto de código, las iteraciones pueden utilizar en formaserial el mismo almacenamiento de datos, en vez de colocar los valores en distintos elementosy filas, respectivamente. La figura 11.62 muestra el resultado de reducir la dimensionalidad delos arreglos. Este código se ejecuta más rápido que el original, ya que lee y escribe menos datos.En especial, en el caso cuando un arreglo se reduce a una variable escalar, podemos asignar lavariable a un registro y eliminar la necesidad de acceder a la memoria en conjunto.A medida que se utiliza menos almacenamiento, hay menos paralelismo disponible. Lasiteraciones en el código transformado en la figura 11.62 ahora comparten las dependencias dedatos, y ya no pueden ejecutarse en paralelo. Para paralelizar el código en P procesadores,podemos expandir cada una de las variables escalares por un factor de P y hacer que cadaprocesador acceda a su propia copia privada. Así, la cantidad por la cual se expande el almacenamiento está directamente correlacionada a la cantidad de paralelismo explotado.11.10 Optimizaciones de localidad 885Maq. Cap_11_AHO B.indd 885 11/10/07 8:21:29 PM886 Capítulo 11. Optimización para el paralelismo y la localidadFigura 11.62: Código de la figura 11.23 después de particionar (figura 11.24) y contracción dearreglosHay tres razones por las que es común buscar oportunidades para la contracción de arreglos:1. Los lenguajes de programación de mayor nivel para aplicaciones científicas, como Matlaby Fortran 90, soportan operaciones a nivel de arreglo. Cada subexpresión de operacionescon arreglos produce un arreglo temporal. Como los arreglos pueden ser extensos, cadaoperación con arreglos como una multiplicación o suma requeriría leer y escribir en muchas ubicaciones de memoria, y a la vez requeriría muy pocas operaciones aritméticas.Es importante que reordenemos las operaciones, de manera que los datos se consuman amedida que se produzcan, y que contraigamos estos arreglos en variables escalares.2. Las supercomputadoras construidas en las décadas de los 80 y 90 son todas máquinasde vectores, por lo que muchas aplicaciones científicas desarrolladas en esa época se hanoptimizado para tales máquinas. Aun cuando existen los compiladores de vectorización,muchos programadores todavía escriben su código para operar sobre vectores a la vez.El ejemplo de código multirrejillas de este capítulo es un ejemplo de este estilo.3. El compilador también introduce las oportunidades para la contracción. Como se ilustramediante la variable T en el ejemplo multirrejillas, un compilador expandiría los arreglos para mejorar la paralelización. Tenemos que contraerlos cuando no sea necesaria laexpansión de espacio.Ejemplo 11.67: La expresión de arreglos Z = W + X + Y se traduce a:Maq. Cap_11_AHO B.indd 886 11/10/07 8:21:30 PMSi rescribimos el código de la siguiente manera:podemos aumentar su velocidad de manera considerable. Desde luego que al nivel del códigoen C, no tendríamos ni siquiera que utilizar la variable temporal T, pero podríamos escribir laasignación a Z[i] como una sola instrucción. Sin embargo, aquí estamos tratando de modelarel nivel de código intermedio en el cual un procesador de vectores se encargaría de las operaciones. ✷Algoritmo 11.68: Contracción de arreglos.ENTRADA: Un programa transformado por el Algoritmo 11.64.SALIDA: Un programa equivalente con dimensiones reducidas del arreglo.MÉTODO: Una dimensión de un arreglo puede contraerse a un solo elemento si:1. Cada partición independiente utiliza sólo un elemento del arreglo.2. La partición no utiliza el valor del elemento al momento de entrar a la partición.3. El valor del elemento no está vivo al salir de la partición.Identifique las dimensiones que pueden contraerse (aquellas que cumplen con las tres condiciones anteriores) y sustitúyalas con un solo elemento. ✷El Algoritmo 11.68 asume que el programa se ha transformado primero mediante el Algoritmo 11.64 para juntar todas las operaciones dependientes en una partición y ejecutar lasparticiones en forma secuencial. Busca aquellas variables de arreglo cuyos rangos vivos de suselementos en distintas iteraciones están desunidas. Si esas variables no están vivas después delciclo, contrae el arreglo y hace que el procesador opere en la misma ubicación escalar. Despuésde cualquier contracción, puede ser necesario expandir los arreglos en forma selectiva, para darcabida al paralelismo y a otras optimizaciones de localidad.El análisis del estado de vida que se requiere aquí es más complejo que el descrito en lasección 9.2.5. Si el arreglo se declara como variable global, o si es un parámetro, se requiere unanálisis entre procedimientos para asegurar que no se utilice el valor en la salida. Además, debemos calcular el estado de vida de los elementos individuales del arreglo; sería muy imprecisotratar al arreglo en forma conservadora como un escalar.11.10.3 Intercalación de particionesA menudo, las distintas particiones en un ciclo leen los mismos datos, o leen y escriben en lasmismas líneas de caché. En esta sección y en las dos que le siguen, veremos cómo optimizar lalocalidad cuando hay reutilización entre las particiones.11.10 Optimizaciones de localidad 887Maq. Cap_11_AHO B.indd 887 11/10/07 8:21:31 PM888 Capítulo 11. Optimización para el paralelismo y la localidadReutilización en los bloques más internosAdoptamos el modelo simple, en el cual los datos pueden encontrarse en la caché si se reutilizadentro de un pequeño número de iteraciones. Si el ciclo más interno tiene un límite extenso odesconocido, sólo la reutilización a través de las iteraciones del ciclo más interno se traduce enun beneficio para la localidad. Los bloques crean ciclos internos con límites pequeños conocidos, permitiendo explotar la reutilización dentro y a través de bloques completos de cómputo.Por ende, los bloques tienen el efecto de sacar provecho en más dimensiones de reutilización.Ejemplo 11.69: Considere el código de multiplicación de matrices que se muestra en la figura11.5 y su versión con bloques en la figura 11.7. La multiplicación de matrices tiene reutilizacióna lo largo de cada dimensión de su espacio de iteraciones tridimensional. En el código original,el ciclo más interno tiene n iteraciones, en donde n es desconocida y puede ser grande. Nuestromodelo simple supone que sólo los datos reutilizados a través de las iteraciones en el ciclo másinterno se encuentra en la caché.En la versión con bloques, los tres ciclos más internos ejecutan un bloque tridimensionalde cómputo, con B iteraciones en cada lado. El tamaño del bloque B lo elige el compilador demanera que sea lo bastante pequeño como para que todas las líneas de caché leídas y escritasdentro del bloque de cómputo puedan caber en la caché. Por ende, los datos reutilizados a través de las iteraciones en el tercer ciclo más externo pueden encontrarse en la caché. ✷Nos referimos al conjunto más interno de ciclos con pequeños límites conocidos como el bloquemás interno. Es conveniente que el bloque más interno incluya todas las dimensiones del espaciode iteraciones que acarrean la reutilización, si es posible. No es tan importante maximizar laslongitudes de cada lado del bloque. Para el ejemplo de multiplicación de matrices, el bloqueo tridimensional reduce la cantidad de datos a los que se accede para cada matriz, por un factor de B 2.Si hay reutilización, es mejor acomodar los bloques de mayor dimensión con lados más cortos, quelos bloques de menor dimensión con lados más largos.Podemos optimizar la localidad del anidamiento de ciclos más interno, completamentepermutable, bloqueando el subconjunto de ciclos que comparten la reutilización. Tambiénpodemos generalizar la noción del bloqueo para explotar las reutilizaciones que se encuentranentre las iteraciones de ciclos paralelos externos. Observamos que los bloques principalmenteintercalan la ejecución de un pequeño número de instancias en el ciclo más interno. En la multiplicación de matrices, cada instancia del ciclo más interno calcula un elemento de la respuestatipo arreglo; hay n 2 de ellos. Los bloques intercalan la ejecución de un bloque de instancias,calculando B iteraciones de cada instancia a la vez. De manera similar, podemos intercalar lasiteraciones en ciclos paralelos para aprovechar las reutilizaciones entre ellos.A continuación definimos dos primitivas que pueden reducir la distancia entre reutilizaciones, a través de distintas iteraciones. Aplicamos estas primitivas en forma repetida, empezandodesde el ciclo más externo hasta que todas las reutilizaciones se muevan, adyacentes unas aotras, en el bloque más interno.Intercalación de ciclos internos en un ciclo paraleloConsidere el caso en el que un ciclo paralelizable externo contiene un ciclo interno. Para explotar la reutilización a través de las iteraciones del ciclo externo, intercalamos las ejecucionesMaq. Cap_11_AHO B.indd 888 11/10/07 8:21:32 PMde un número fijo de instancias del ciclo interno, como se muestra en la figura 11.63. Al crearbloques internos bidimensionales, esta transformación reduce la distancia entre la reutilizaciónde iteraciones consecutivas del ciclo externo.(a) Programa fuente. (b) Código transformado.Figura 11.63: Intercalación de 4 instancias del ciclo internoEl paso que convierte un cicloense conoce como seccionamiento (stripmining). En el caso en el que el ciclo externo en la figura11.63 tiene un pequeño límite conocido, no es necesario seccionarlo, sino solo permutar los dosciclos en el programa original.Intercalación de instrucciones en un ciclo paraleloConsidere el caso en el que un ciclo paralelizable contiene una secuencia de instrucciones s 1,s 2,…, sm. Si algunas de estas instrucciones son también ciclos, las instrucciones de iteracionesconsecutivas aún pueden estar separadas por muchas operaciones. Podemos explotar la reutilización entre iteraciones al intercalar de nuevo sus ejecuciones, como se muestra en la figura11.64. Esta transformación distribuye un ciclo seccionado a través de las instrucciones. De nuevo, si el ciclo externo tiene un pequeño número fijo de iteraciones, no es necesario seccionar elciclo, sino sólo distribuir el ciclo original a través de todas las instrucciones.Utilizamos si(j) para denotar la ejecución de la instrucción s 1 en la iteración j. En vez delorden de ejecución secuencial original que se muestra en la figura 11.65(a), el código se ejecutaen el orden mostrado en la figura 11.65(b).Ejemplo 11.70: Ahora vamos a regresar al ejemplo multirrejillas y mostraremos cómo explotamos la reutilización entre las iteraciones de los ciclos paralelos externos. Observamos que lasreferencias DW [1, k, j, i], DW [1, k − 1, j, i] y DW [1, k + 1, j, i] en los ciclos más internos del código en la figura 11.62 tienen una localidad espacial bastante mala. Del análisis de reutilización,11.10 Optimizaciones de localidad 889Maq. Cap_11_AHO B.indd 889 11/10/07 8:21:32 PM890 Capítulo 11. Optimización para el paralelismo y la localidad(a) Programa fuente. (b) Código transformado.Figura 11.64: La transformación de intercalación de instruccionescomo vimos en la sección 11.5, el ciclo con el índice i acarrea la localidad espacial y el ciclo conel índice k acarrea la reutilización de grupo. El ciclo con el índice k ya es el ciclo más interno,por lo que nos interesa intercalar las operaciones en DW a partir de un bloque de particionescon valores consecutivos de i. Aplicamos la transformación para intercalar las instrucciones en el ciclo y obtener el códigode la figura 11.66, y después aplicamos la transformación para intercalar los ciclos internosy obtener el código de la figura 11.67. Observe que, a medida que intercalamos B iteracionesdel ciclo con el índice i, debemos expandir las variables AP, AM, T en arreglos que guarden Bresultados a la vez. ✷11.10.4 Reunión de todos los conceptosEl Algoritmo 11.71 optimiza la localidad para un uniprocesador, y el Algoritmo 11.72 optimizatanto el paralelismo como la localidad para un multiprocesador.Algoritmo 11.71: Optimización de la localidad de datos en un uniprocesador.ENTRADA: Un programa con accesos afines a los arreglos.SALIDA: Un programa equivalente que maximiza la localidad de los datos.MÉTODO: Realice los siguientes pasos:1. Aplique el Algoritmo 11.64 para optimizar la localidad temporal de los resultados calculados.2. Aplique el Algoritmo 11.68 para contraer los arreglos en donde sea posible.3. Determine el subespacio de iteraciones que pueden compartir los mismos datos o líneasde caché, usando la técnica descrita en la sección 11.5. Para cada instrucción, identifiquelas dimensiones de los ciclos paralelos externos que tienen reutilización de datos.4. Para cada ciclo paralelo externo que acarree la reutilización, mueva un bloque de lasiteraciones hacia el bloque más interno, mediante la aplicación de las primitivas de intercalación en forma repetida.Maq. Cap_11_AHO B.indd 890 11/10/07 8:21:34 PM(a) Orden original.(b) Orden transformado.Figura 11.65: Distribución de un ciclo seccionado5. Aplique los bloques al subconjunto de dimensiones en el anidamiento de ciclos completamente permutable más interno que acarree la reutilización.6. Use bloques en el anidamiento de ciclos completamente permutable externo para nivelesmás altos de jerarquías de memoria, como la caché de tercer nivel o la memoria física.7. Expanda las escalares y los arreglos en donde sea necesario, mediante las longitudes delos bloques.✷Algoritmo 11.72: Optimización del paralelismo y la localidad de los datos para multiprocesadores.ENTRADA: Un programa con accesos afines a los arreglos.SALIDA: Un programa equivalente que maximiza el paralelismo y la localidad de los datos.MÉTODO: Haga lo siguiente:1. Use el Algoritmo 11.64 para paralelizar el programa y crear un programa SPMD.11.10 Optimizaciones de localidad 891Maq. Cap_11_AHO B.indd 891 11/10/07 8:21:34 PM892 Capítulo 11. Optimización para el paralelismo y la localidad/* Termina el codigo que va a ejecutar el procesador (j,i ´ ) */Figura 11.66: Extracto de la figura 11.23 después del particionamiento, la contracción de arreglos y los bloques2. Aplique el Algoritmo 11.71 al programa SPMD producido en el paso 1 para optimizar sulocalidad.✷11.10.5 Ejercicios para la sección 11.10Ejercicio 11.10.1: Realice la contracción de arreglos en las siguientes operaciones con vectores:Ejercicio 11.10.2: Realice la contracción de arreglos en las siguientes operaciones con vectores:Maq. Cap_11_AHO B.indd 892 11/14/07 3:43:38 PM/* Termina el codigo que va a ejecutar el procesador (j,i ´ ) */Figura 11.67: Extracto de la figura 11.23 después del particionamiento, la contracción de arreglos y los bloquesEjercicio 11.10.3: Seccione el siguiente ciclo externo:en secciones con anchura de 10.11.11 Otros usos de las transformaciones afinesHasta ahora nos hemos enfocado en la arquitectura de las máquinas con memoria compartida, perola teoría de las transformaciones afines de ciclos tiene muchas otras aplicaciones. Podemos aplicar transformaciones afines a otras formas de paralelismo, incluyendo las máquinas con memoria11.11 Otros usos de las transformaciones afi nes 893Maq. Cap_11_AHO B.indd 893 11/14/07 3:43:44 PM894 Capítulo 11. Optimización para el paralelismo y la localidaddistribuida, las instrucciones vectoriales, las instrucciones SIMD (Single Instruction MultipleData; Una sola instrucción, múltiples datos), así como las máquinas emisoras de múltiples instrucciones. El análisis de reutilización presentado en este capítulo también es útil para la preobtención de datos, la cual es una técnica efectiva para mejorar el rendimiento de la memoria.11.11.1 Máquinas con memoria distribuidaPara las máquinas con memoria distribuida, en donde los procesadores se comunican enviándose mensajes entre sí; es aún más importante que a los procesadores se les asignen unidadesgrandes e independientes de cómputo, como las generadas por el algoritmo de particionamientoafín. Además del particionamiento del cómputo, todavía quedan varias cuestiones adicionalesde compilación:1. Repartición de datos. Si los procesadores utilizan distintas porciones de un arreglo, sólotienen que repartir suficiente espacio para contener la porción utilizada. Podemos usar laproyección para determinar la sección de arreglos utilizados por cada procesador. La entrada es el sistema de desigualdades lineales que representan los límites de ciclo, las funciones de acceso a los arreglos y las particiones afines que asignan las iteraciones a los IDsde los procesadores. Proyectamos los índices de ciclo para alejarlos y buscamos para cadaID de procesador el conjunto de ubicaciones de arreglos utilizadas.2. Código de comunicación. Debemos generar código explícito para enviar y recibir datoshacia, y desde, otros procesadores. En cada punto de sincronización: (a) Se determinan los datos que residen en un procesador, y que otros procesadoresnecesitan. (b) Se genera el código que busca todos los datos a enviar y los empaqueta en un búfer. (c) De manera similar, se determinan los datos que necesita el procesador, se desempaquetan los mensajes recibidos y se mueven los datos a las ubicaciones de memoriaapropiadas. De nuevo, si todos los accesos son afines, el compilador puede realizar estas tareas, usando el framework afín.3. Optimización. No es necesario que todas las comunicaciones se lleven a cabo en los puntos de sincronización. Es preferible que cada procesador envíe los datos tan pronto comoestén disponibles, y que cada procesador no empiece esperando los datos hasta que senecesiten. Dichas optimizaciones deben balancearse mediante el objetivo de no generardemasiados mensajes, ya que hay una sobrecarga no trivial asociada con el procesamiento de cada mensaje.Las técnicas aquí descritas tienen otras aplicaciones también. Por ejemplo, un sistema embebido de propósito especial puede utilizar coprocesadores para descargar parte de sus cálculos.O, en vez de demandar la obtención de datos en la caché, un sistema embebido puede utilizarun controlador separado para cargar y descargar datos hacia, y fuera de, la caché o de otrosbúferes de datos, mientras que el procesador opera con otros datos. En estos casos pueden utilizarse técnicas similares para generar el código que desplace los datos.Maq. Cap_11_AHO B.indd 894 11/10/07 8:21:41 PM11.11.2 Procesadores que emiten múltiples instruccionesTambién podemos usar transformaciones afines de ciclos para optimizar el rendimiento de lasmáquinas que emiten varias instrucciones. Como vimos en la sección 10.5, el rendimiento deun ciclo canalizado por software se limita mediante dos factores: los ciclos en las restriccionesde precedencia y el uso del recurso crítico. Al modificar la composición del ciclo más interno,podemos mejorar estos límites.En primer lugar, tal vez podamos usar transformaciones de ciclos para crear ciclos másinternos paralelizables, con lo cual se eliminan por completo los ciclos de precedencia. Suponga que un programa tiene dos ciclos, en donde el externo es paralelizable y el interno no loes. Podemos permutar los dos ciclos para hacer el ciclo interno paralelizable, y así crear másoportunidades para el paralelismo a nivel de instrucción. Observe que no es necesario que lasiteraciones en el ciclo más interno sean completamente paralelizables. Basta con que el ciclode dependencias en el ciclo sea lo bastante corto como para que se utilicen por completo todoslos recursos de hardware.También podemos relajar el límite debido al uso de los recursos, mejorando el balance deuso dentro de un ciclo. Suponga que un ciclo sólo utiliza el sumador, y que otro ciclo sólo utilizael multiplicador. O, suponga que un ciclo está limitado por memoria y que otro está limitadopor cómputo. Es conveniente fusionar cada par de ciclos en estos ejemplos, para poder utilizartodas las unidades funcionales al mismo tiempo.11.11.3 Instrucciones con vectores y SIMDAdemás de la cuestión sobre múltiples instrucciones, hay otras dos formas importantes de paralelismo a nivel de instrucción: las operaciones con vectores y SIMD. En ambos casos, la cuestiónde sólo una instrucción hace que la misma operación se aplique a un vector de datos.Como dijimos antes, muchas de las primeras supercomputadoras utilizaban instrucciones convectores. Las operaciones con vectores se llevan a cabo en forma canalizada; los elementos enel vector se obtienen en forma serial y los cómputos sobre los distintos elementos se traslapan.En las máquinas de vectores avanzadas, las operaciones con vectores pueden encadenarse: amedida que se producen los elementos de los resultados con vectores, se consumen de inmediatopor las operaciones de otra instrucción con vectores, sin tener que esperar a que todos los resultados estén listos. Además, en máquinas avanzadas con hardware para dispersar/recolectar, loselementos de los vectores no necesitan ser contiguos; se utiliza un vector índice para especificarla ubicación de los elementos.Las instrucciones SIMD especifican que se debe realizar la misma operación en ubicacionescontiguas de memoria. Estas instrucciones cargan datos de la memoria en paralelo, los almacenan en registros amplios y realizan cálculos con ellos usando hardware en paralelo. Muchasaplicaciones de medios, grafos y procesamiento de señales digitales pueden beneficiarse de estasoperaciones. Los procesadores de medios de bajo rendimiento pueden alcanzar un paralelismo anivel de instrucción con sólo emitir una instrucción SIMD a la vez. Los procesadores de mayorrendimiento pueden combinar la emisión de instrucciones SIMD con varias instrucciones paralograr un mayor rendimiento.La generación de instrucciones con vectores y SIMD comparte muchas similitudes con laoptimización de la localidad. A medida que encontramos particiones independientes que operan11.11 Otros usos de las transformaciones afi nes 895Maq. Cap_11_AHO B.indd 895 11/10/07 8:21:41 PM896 Capítulo 11. Optimización para el paralelismo y la localidadsobre ubicaciones contiguas de memoria, seccionamos esas iteraciones e intercalamos esas operaciones en los ciclos más internos.La generación de instrucciones SIMD presenta dos dificultades adicionales. En primer lugar,ciertas máquinas requieren que los datos SIMD que se obtienen de memoria estén alineados. Porejemplo, podrían requerir que los operandos SIMD de 256 bytes se coloquen en direcciones quesean múltiplos de 256. Si el ciclo de origen opera sólo sobre un arreglo de datos, podemos generarun ciclo principal que opere sobre datos alineados y código adicional antes y después del ciclopara manejar esos elementos en el límite. Sin embargo, para los ciclos que operan sobre más deun arreglo, tal vez no sea posible alinear todos los datos al mismo tiempo. En segundo lugar,los datos utilizados por iteraciones consecutivas en un ciclo pueden no estar contiguos. Algunosejemplos incluyen muchos algoritmos importantes de procesamiento de señales digitales, comolos decodificadores de Viterbi y las transformadas rápidas de Fourier. Tal vez se requieran operaciones adicionales para revolver los datos alrededor, para sacar provecho de las instruccionesSIMD.11.11.4 PreobtenciónNinguna optimización de la localidad de los datos puede eliminar todos los accesos a la memoria; por una parte, los datos usados por primera vez deben obtenerse de memoria. Para ocultar la latencia de las operaciones de memoria, se han adoptado instrucciones de preobtención(prefetch) en muchos procesadores de alto rendimiento. La preobtención es una instrucción demáquina que indica al procesador la probabilidad de usar ciertos datos pronto, y que es conveniente cargar esos datos en la caché, si no están ya presentes.El análisis de reutilización descrito en la sección 11.5 puede usarse para estimar cuando hayprobabilidad de fallos en la caché. Hay dos consideraciones importantes a la hora de generarinstrucciones de preobtención. Si se va a acceder a ubicaciones contiguas de memoria, debemosemitir sólo una instrucción de preobtención para cada línea de caché. Las instrucciones de preobtención deben emitirse con la suficiente anticipación como para que los datos estén en la cachépara cuando se vayan a utilizar. Sin embargo, no debemos emitir instrucciones de preobtencióncon demasiada anticipación. Éstas instrucciones pueden desplazar datos que podrían ser necesarios todavía; además, los datos preobtenidos podrían vaciarse antes de usarlos.Ejemplo 11.73: Considere el siguiente código:Suponga que la máquina destino tiene una instrucción de preobtención que puede obtener dospalabras de datos a la vez, y que la latencia de una instrucción de preobtención requiere untiempo aproximado de ejecución equivalente a seis iteraciones del ciclo anterior. El código depreobtención para el ejemplo anterior se muestra en la figura 11.68.Desenrollamos el ciclo más interno dos veces, por lo que puede emitirse una instrucciónde preobtención para cada línea de caché. Utilizamos el concepto de canalización de softwarepara preobtener los datos seis iteraciones antes de usarlos. El prólogo obtiene los datos que seMaq. Cap_11_AHO B.indd 896 11/10/07 8:21:42 PMFigura 11.68: Código modificado para la preobtención de datosutilizan en las primeras seis iteraciones. El ciclo de estado estable preobtiene seis iteracionespor adelantado, a medida que realiza su cómputo. El epílogo no emite instrucciones de preobtención, sino que sólo ejecuta las iteraciones restantes. ✷11.12 Resumen del capítulo 11♦ Paralelismo y localidad de los arreglos: Las oportunidades más importantes para las optimizaciones basadas en el paralelismo y la localidad provienen de los ciclos que acceden aarreglos. Estos ciclos tienen por lo regular dependencias limitadas entre los accesos a loselementos de un arreglo, y tienden a acceder a los arreglos en un patrón regular, permitiendo un uso eficiente de la caché para una buena localidad.♦ Accesos afines: Casi toda la teoría y las técnicas para la optimización del paralelismo yla localidad suponen que los accesos a arreglos son afines: las expresiones para los índicesde arreglos son funciones lineales de los índices de ciclo.♦ Espacios de iteraciones: Un anidamiento de ciclos con d ciclos anidados define un espaciode iteraciones d-dimensional. Los puntos en el espacio son los d tuplas de valores quepueden asumir los índices de ciclo durante la ejecución del anidamiento de ciclos. En elcaso afín, los límites sobre cada índice de ciclo son funciones lineales de los índices del ciclo externo, por lo que el espacio de iteraciones es un poliedro.♦ Eliminación de Fourier-Motzkin: Una manipulación clave de los espacios de iteracioneses reordenar los ciclos que definen el espacio de iteraciones. Para ello se requiere la proyección de un espacio de iteraciones, representado por un poliedro, en un subconjunto desus dimensiones. El algoritmo de Fourier-Motzkin sustituye los límites superior e inferioren una variable dada por las desigualdades entre los mismos límites.♦ Dependencias de datos y accesos a arreglos: Un problema central que debemos resolverpara poder manipular los ciclos de las optimizaciones de paralelismo y localidad es si dosaccesos a un arreglo tienen una dependencia de datos (pueden tocar el mismo elemento delarreglo). Cuando los accesos y los límites de ciclo son afines, el problema puede expresarse11.12 Resumen del capítulo 11 897Maq. Cap_11_AHO B.indd 897 11/10/07 8:21:43 PM898 Capítulo 11. Optimización para el paralelismo y la localidadcomo si hubiera soluciones a una ecuación con matrices y vectores dentro del poliedro quedefine el espacio de iteraciones.♦ Rango de una matriz y reutilización de datos: La matriz que describe un acceso a unarreglo puede indicarnos varias cosas importantes acerca de ese acceso. Si el rango de lamatriz es lo más grande posible (el mínimo del número de filas y el número de columnas),entonces el acceso nunca toca el mismo elemento dos veces a medida que iteran los ciclos.Si el arreglo se almacena en formato de orden por fila (columna), entonces el rango de lamatriz con la última (primera) fila eliminada nos indica si el acceso tiene buena localidad; es decir, se accede a los elementos en una línea de caché individual casi al mismotiempo.♦ Dependencia de datos y ecuaciones diofantinas: Sólo porque dos accesos al mismo arregloestán en contacto con la misma región del arreglo, no significa que en realidad accedana un elemento en común. La razón es que cada arreglo puede omitir ciertos elementos;por ejemplo, uno accede a los elementos pares y el otro a los elementos impares. Parapoder estar seguros de que hay una dependencia de datos, debemos resolver una ecuacióndiofantina (sólo soluciones enteras).♦ Solución de ecuaciones lineales diofantinas: La técnica clave es calcular el máximo comúndivisor (GCD) de los coeficientes de las variables. Sólo si ese GCD divide el términoconstante, habrá soluciones enteras.♦ Restricciones de partición de espacio: Para paralelizar la ejecución de un anidamiento deciclos, debemos asignar las iteraciones del ciclo a un espacio de procesadores, que puede tener una o más dimensiones. Las restricciones de partición de espacio indican que si dosaccesos en dos iteraciones distintas comparten una dependencia de datos (es decir, accedenal mismo elemento del arreglo), entonces deben asignarse al mismo procesador. Siempre ycuando la asignación de iteraciones a los procesadores sea afín, podemos formular el problema en términos de matriz-vector.♦ Transformaciones primitivas de código: Las transformaciones que se utilizan para paralelizar los programas con accesos afines a los arreglos son combinaciones de siete primitivas: fusión de ciclos, fisión de ciclos, reindexado (sumar una constante a los índicesde ciclo), escalado (multiplicar los índices de ciclo por una constante), inversión (de uníndice de ciclo), permutación (del orden de los ciclos), y desplazamiento (rescribir losciclos de forma que la línea de pasaje a través del espacio de iteraciones ya no sea a lolargo de uno de los ejes).♦ Sincronización de operaciones en paralelo: Algunas veces podemos obtener más paralelismo si insertamos operaciones de sincronización entre los pasos de un programa. Porejemplo, los anidamientos de ciclos consecutivos pueden tener dependencias de datos,pero las sincronizaciones entre los ciclos pueden permitir la paralelización de éstos porseparado.♦ Canalización: Esta técnica de paralelización permite a los procesadores compartir datos,al pasar ciertos datos en forma síncrona (por lo general, elementos de un arreglo) de unprocesador a un procesador adyacente en el espacio de procesadores. El método puedemejorar la localidad de los datos a los que accede cada procesador.Maq. Cap_11_AHO B.indd 898 11/10/07 8:21:44 PM♦ Restricciones de partición de tiempo: Para descubrir las oportunidades para la canalización, debemos descubrir soluciones a las restricciones de partición de tiempo. Éstas nosindican que cuando dos accesos a un arreglo pueden tocar el mismo elemento del arreglo,entonces el acceso en la iteración que ocurre primero debe asignarse a una etapa en lacanalización que ocurra no después que la etapa a la cual se asignó el segundo acceso.♦ Resolución de restricciones de partición de tiempo: El Lema de Farkas proporciona unatécnica poderosa para buscar todas las asignaciones de particiones de tiempo afines quese permiten mediante un anidamiento de ciclos dado, con accesos a arreglos. En esencia,la técnica consiste en sustituir la formulación fundamental de las desigualdades linealesque expresan las restricciones de partición de tiempo por su doble.♦ Uso de bloques: Esta técnica descompone cada uno de los diferentes ciclos en un anidamiento de ciclos, en dos ciclos cada uno. La ventaja es que si hacemos esto, podremostrabajar con secciones pequeñas (bloques) de un arreglo multidimensional, un bloquea la vez. Eso, a su vez, mejora la localidad del programa, dejando que todos los datosnecesarios residan en la caché mientras se trabaja sobre un solo bloque.♦ Seccionamiento (Stripmining): De manera similar al bloqueo, esta técnica descomponesólo un subconjunto de los ciclos de un anidamiento de ciclos en dos ciclos cada uno. Unaposible ventaja es que se accede al arreglo multidimensional una “sección” a la vez, locual puede conducir a la mejor utilización posible de la caché.11.13 Referencias para el capítulo 11Para explicaciones detalladas de arquitecturas de microprocesadores, el lector puede consultarel texto de Hennessy y Patterson [9].Lamport [13] y Kuck, Muraoka y Chen [6] introdujeron el concepto del análisis de dependencias de datos. Las primeras pruebas de dependencia de datos utilizaron la heurística parademostrar que un par de referencias son independientes al determinar si no hay soluciones alas ecuaciones diofantinas y los sistemas de desigualdades lineales reales: [5, 6, 26]. Maydan,Hennessy y Lam [18] formularon la prueba de dependencia de datos como programación linealentera y mostraron que el problema puede resolverse con exactitud y eficiencia en la práctica.El análisis de dependencias de datos que se describe en este capítulo se basa en el trabajo querealizaron Maydan, Hennessy y Lam [18], y Pugh y Wonnacott [23], que a su vez utilizan técnicas de la eliminación de Fourier-Motzkin [7] y del algoritmo de Shostak [25].Las décadas de 1970 y 1980 vieron el uso de las transformaciones de ciclo para mejorar lavectorización y la paralelización: fusión de ciclos [3], fisión de ciclos [1], seccionamiento (stripmining) [17], e intercambio de ciclos [28]. Hubo tres proyectos experimentales importantes de paralelización/vectorización en ese tiempo: Parafrase, que dirigió Kuck en la Universidad de IllinoisUrbana-Champaign [21], el proyecto PFC, que dirigió Kennedy en la Universidad Rice [4], y elproyecto PTRAN de Allen en IBM Research [2].McKellar y Coffman [19] hablaron por primera vez sobre el uso de bloques para mejorar lalocalidad de los datos. Lam, Rothbert y Wolf [12] proporcionaron el primer análisis empírico aprofundidad del uso de bloques en las cachés para las arquitecturas modernas. Wolf y Lam [27]11.13 Referencias para el capítulo 11 899Maq. Cap_11_AHO B.indd 899 11/10/07 8:21:44 PM900 Capítulo 11. Optimización para el paralelismo y la localidadutilizaron las técnicas del álgebra lineal para calcular la reutilización de los datos en los ciclos.Sarkar y Gao [24] presentaron la optimización de la contracción de arreglos.Lamport [13] fue el primero en modelar los ciclos como espacios de iteraciones, y utilizó lahiperplaneación (un caso especial de una transformación afín) para encontrar paralelismo en losmultiprocesadores. Las transformaciones afines tienen sus raíces en el diseño de algoritmos conarreglos sistólicos [11]. Diseñados como algoritmos paralelos implementados directamente enVLSI, los arreglos sistólicos requieren la minimización de la comunicación junto con la paralelización. Se desarrollaron técnicas algebraicas para asignar el cómputo en coordenadas de espacioy tiempo. Feautrier [8] presentó el concepto de un programa afín y el uso del Lema de Farkasen las transformaciones afines. El algoritmo de transformaciones afines que se describe en estecapítulo está basado en el trabajo de Lim y colaboradores [15, 14, 16].Porterfield [22] propuso uno de los primeros algoritmos de compiladores para preobtenerdatos. Mowry, Lam y Gupta [20] aplicaron el análisis de la reutilización para minimizar la sobrecarga de las instrucciones de preobtención y ganar una mejora general en el rendimiento. 1. Abu-Sufah, W., D. J. Kuck y D. H. Lawrie, “On the performance enhancement of pagingsytems through program analysis and transformations”, IEEE Trans. on ComputingC-30:5 (1981), pp. 341-356. 2. Allen, F. E., M. Burke, P. Charles, R. Cytron y J. Ferrante, “An overview of thePTRAN analysis system for multiprocessing”, J. Parallel and Distributed Computing5:5 (1988), pp. 617-640. 3. Allen, F. E. y J. Cocke, “A Catalogue of optimizing transformations”, en Design andOptimization of Compilers (R. Rustin, ed.), pp. 1-30, Prentice-Hall, 1972. 4. Allen, R. y K. Kennedy, “Automatic translation of Fortran programs to vector form”,ACM Transactions on Programming Languages and Systems 9:4 (1987), pp. 491-542. 5. Banerjee, U., Data Dependence in Ordinary Programs, tesis de Maestría, Departamentode Ciencias Computacionales, Universidad de Illinois Urbana-Champaign, 1976. 6. Banerjee, U., Speedup of Ordinary Programs, tesis Ph. D., Departamento de CienciasComputacionales, Universidad de Illinois Urbana-Champaign, 1979. 7. Dantzig, G. y B. C. Eaves, “Eliminación de Fouier-Motzkin y su doble”, J. Combinatorial Theory, A(14) (1973), pp. 288-297. 8. Feautrier, P., “Some efficient solutions to the affine scheduling problem: I. One-dimensional time”, International J. Parallel Programming 21:5 (1992), pp. 313-348. 9. Hennessy, J. L. y D. A. Patterson, Computer Architecture: A Quantitative Approach,Tercera edición, Morgan Kaufman, San Francisco, 2003.10. Kuck, D., Y. Muraoka y S. Chen, “On the number of operations simultaneously executable in Fortran-like programs and their resulting speedup”, IEEE Transactions onComputers C-21:12 (1972), pp. 1293-1310.Maq. Cap_11_AHO B.indd 900 11/10/07 8:21:44 PM11. Kung, H. T. y C. E. Leiserson, “Systolic arrays (for VLSI)”, en Duff, I. S. y G. W.Stewart (eds.), Sparse Matrix Proceedings, pp. 256-282. Sociedad para las Matemáticasindustriales y aplicadas, 1978.12. Lam, M. S., E. E. Rothberg y M. E. Wolf, “The cache performance and optimization ofblocked algorithms”, Proc. Sixth International Conference on Architectural Support forProgramming Languages and Operating Systems (1991), pp. 63-74.13. Lamport, L., “The parallel execution of DO loops”, Comm. ACM 17:2 (1974), pp. 83-93.14. Lim, A. W., G. I. Cheong y M. S. Lam, “An affine partitioning algorithm to maximizeparallelism and minimize communication”, Proc. 13th International Conference on Supercomputing (1999), pp. 228-237.15. Lim, A. W. y M. S. Lam, “Maximizing parallelism and minimizing synchronizationwith affine transforms”, Proc. 24th ACM SIGPLAN-SIG ACT Symposium on Principles of Programming Languages (1997), pp. 201-214.16. Lim, A. W., S.-W. Liao y M. S. Lam, “Blocking and array contraction across arbitrarily nested loops using affine partitioning”, Proc. ACM SIGPLAN Symposium onPrinciples and Practice of Parallel Programming (2001), pp. 103-112.17. Loveman, D. B., “Program improvement by source-to-source transformation”, J. ACM24:1 (1977), pp. 121-145.18. Maydan, D. E., J. L. Hennessy y M. S. Lam, “An efficient method for exact dependenceanalysis”, Proc. ACM SIGPLAN 1991 Conference on Programming Language Designand Implementation, pp. 1-14.19. McKeller, A. C. y E. G. Coffman, “The organization of matrices and matrix operationsin a paged multiprogramming environment”, Comm. ACM, 12:3 (1969), pp. 153-165.20. Mowry, T. C., M. S. Lam y A. Gupta, “Design and evaluation of a compiler algoritmfor prefetching”, Proc. Fifth Internacional Conference on Architectural Support for Programming Languages and Operating Systems (1992), pp. 62-73.21. Padua, D. A. y M. J. Wolfe, “Advanced compiler optimizations for supercomputers”,Comm. ACM, 29:12 (1986), pp. 1184-1201.22. Porterfield, A., Software Methods for Improving Cache Performance on SupercomputerApplications, tesis Ph. D., Departamento de Ciencias Computacionales, UniversidadRice, 1989.23. Pugh, W. y D. Wonnacott, “Eliminating false positives using the omega test”, Proc.ACM SIGPLAN 1992 Conference on Programming Language Design and Implementation, pp. 140-151.24. Sarkar, V. y G. Gao, “Optimization of array accesses by collective loop transformations”,Proc. 5th International Conference on Supercomputing (1991), pp. 194-205.11.13 Referencias para el capítulo 11 901Maq. Cap_11_AHO B.indd 901 11/10/07 8:21:45 PM902 Capítulo 11. Optimización para el paralelismo y la localidad25. R. Shostak, “Deciding linear inequalities by computing loop residues”, J. ACM, 28:4(1981), pp. 769-779.26. Towle, R. A., Control and Data Dependence for Program Transformation, tesis Ph. D.,Departamento de Ciencias Computacionales, Universidad de Illinois Urbana-Champaign, 1976.27. Wolf, M. E. y M. S. Lam, “A data locality optimizing algorithm”, Proc. SIGPLAN 1991Conference on Programming Language Design and Implementation, pp. 30-44.28. Wolfe, M. J., Techniques for Improving the Inherent Parallelism in Programs, tesis deMaestría, Departamento de Ciencias Computacionales, Universidad de Illinois UrbanaChampaign, 1978.Maq. Cap_11_AHO B.indd 902 11/10/07 8:21:45 PMEn este capítulo destacaremos la importancia del análisis interprocedural, al hablar sobrevarios problemas importantes de optimización que no pueden resolverse con el análisis intraprocedural. Empezaremos por describir las formas comunes del análisis interprocedural y explicaremos las dificultades en su implementación. Después describiremos las aplicaciones paraeste análisis. Para los lenguajes de programación muy populares como C y Java, el análisis dealias de apuntadores es la clave para cualquier análisis interprocedural. Por lo tanto, durantela mayor parte de este capítulo hablaremos sobre las técnicas necesarias para calcular los aliasde apuntadores. Para empezar presentaremos Datalog, una notación que oculta en gran partela complejidad de un análisis eficiente de apuntadores. Después describiremos un algoritmopara este análisis, y mostraremos cómo utilizar la abstracción de los diagramas de decisionesbinarias (BDDs) para implementar el algoritmo con eficiencia.La mayoría de las optimizaciones de los compiladores, incluyendo las descritas en los capítulos 9, 10 y 11, se realizan sobre los procedimientos, uno a la vez. A dicho análisis se le conocecomo intraprocedural. Estos análisis suponen de manera conservadora que los procedimientosinvocados pueden alterar el estado de todas las variables visibles para los procedimientos y quepueden crear todos los efectos adicionales posibles, como la modificación de cualquiera de lasvariables visibles para el procedimiento, o la generación de excepciones que provoquen la limpieza de la pila (stack unwinding) de llamadas. Por ende, el análisis intraprocedural es bastantesimple, aunque impreciso. Algunas optimizaciones no necesitan del análisis interprocedural,mientras que otras casi no pueden producir información útil sin él.Un análisis interprocedural opera a través de un programa completo, haciendo que fluyainformación del emisor a los procesos llamados, y viceversa. Una técnica bastante simple peroútil es la de poner los procedimentos en línea; es decir, sustituir la invocación de un procedimiento por el cuerpo del mismo procedimiento, con modificaciones adecuadas para tomar encuenta el paso de los parámetros y el valor de retorno. Este método se aplica sólo si conocemosel destino de la llamada al procedimiento.Si los procedimientos se invocan de manera indirecta a través de un apuntador, o medianteel mecanismo despachador de métodos prevaleciente en la programación orientada a objetos,Capítulo 12Análisis interprocedural903Maq. Cap_12_AHO.indd 903 11/10/07 1:07:23 AM904 Capítulo 12. Análisis interproceduralel análisis de los apuntadores o referencias del programa puede, en algunos casos, determinarlos destinos de las invocaciones indirectas. Si hay un destino único, pueden aplicarse los procedimientos en línea.Aun si se determina un destino único para cada invocación a un procedimiento, el uso delos procedimientos en línea debe aplicarse con prudencia. En general, no es posible poner enlínea los procedimientos recursivos de manera directa, e inclusive sin la recursividad, el uso deprocedimientos en línea puede expandir el código en forma exponencial.12.1 Conceptos básicosEn esta sección presentaremos los grafos de llamadas: grafos que nos indican qué procedimientos pueden llamar a qué otros procedimientos. También expondremos la idea de “sensibilidad alcontexto”, en donde se requieren análisis de flujo de datos para conocer cuál ha sido la secuencia de llamadas a procedimientos. Es decir, el análisis sensible al contexto incluye (una sinopsisde) la secuencia actual de los registros de activación en la pila, junto con el punto actual en elprograma, cuando hay que diferenciar entre distintos “lugares” en el programa.12.1.1 Grafos de llamadasUn grafo de llamadas para un programa es un conjunto de nodos y aristas de manera que:1. Hay un nodo para cada procedimiento en el programa.2. Hay un nodo para cada sitio de llamada; es decir, un lugar en el programa en donde seinvoca a un procedimiento.3. Si el sitio de llamada c puede llamar al procedimiento p, entonces hay una arista que vadel nodo para c hasta el nodo para p.Muchos programas escritos en lenguajes como C y Fortran hacen las llamadas a los procedimientos de manera directa, por lo que el destino de llamada de cada invocación puededeterminarse en forma estática. En ese caso, cada sitio de llamada tiene una arista que va a unsolo procedimiento en el grafo de llamadas. No obstante, si el programa incluye el uso deun parámetro de procedimiento o apuntador de función, por lo general, no se conoce el destino sino hasta que se ejecuta el programa y, de hecho, puede variar de una invocación a otra.Entonces, un sitio de llamada puede indicar enlaces que van a muchos o a todos los procedimientos en el grafo de llamadas.Las llamadas indirectas son la norma para los lenguajes de programación orientados a objetos. En especial, cuando hay redefinición de métodos en las subclases, un uso del método mpuede referirse a cualquiera de varios métodos distintos, dependiendo de la subclase del objetoreceptor al cuál se aplicó. El uso de dichas invocaciones a métodos virtuales significa que debemos conocer el tipo del receptor antes de poder determinar qué método se invocó.Ejemplo 12.1: La figura 12.1 muestra un programa en C que declara a pf como un apuntadorglobal a una función cuyo tipo es “de entero a entero”. Hay dos funciones de este tipo, fun1 yfun2, y una función principal que no es del tipo al que pf apunta. La figura muestra tres sitiosde llamada, denotados como c1, c2 y c3; las etiquetas no forman parte del programa.Maq. Cap_12_AHO.indd 904 11/10/07 1:07:26 AMEl análisis más simple del objeto al que pf podría apuntar sólo observaría los tipos de lasfunciones. Las funciones fun1 y fun2 son del mismo tipo al que pf apunta, mientras que mainno. Por ende, en la figura 12.2(a) se muestra un grafo de llamadas conservador. Un análisis máscuidadoso del programa observaría que se hace que pf apunte a fun2 en main, y que apunte afun1 en fun2. Pero no hay otras asignaciones para ningún apuntador, por lo que, en especial,no hay forma de que pf apunte a main. Este razonamiento produce el mismo grafo de llamadasque el de la figura 12.2(a).Un análisis aún más preciso indicaría que en c3 sólo es posible que pf apunte a fun2, yaque esa llamada va precedida de inmediato por esa asignación a pf. De manera similar, en c2sólo es posible que pf apunte a fun1. Como resultado, la llamada inicial a fun1 sólo puedeprovenir de fun2, y fun1 no modifica a pf, por lo que cada vez que estamos dentro de fun1,pf apunta a fun1. En especial, en c1 podemos estar seguros de que pf apunta a fun1. Así, lafigura 12.2(b) es un grafo de llamadas correcto y más preciso. ✷En general, la presencia de referencias o apuntadores a funciones o métodos requiere queobtengamos una aproximación estática de los valores potenciales de todos los parámetros deprocedimientos, apuntadores de funciones y tipos de objetos receptores. Para realizar unaaproximación precisa, es necesario el análisis interprocedural. El análisis es iterativo, y empiezacon los destinos observables de manera estática. A medida que se descubren más destinos, elanálisis incorpora las nuevas aristas en el grafo de llamadas y repite el descubrimiento de másdestinos hasta que se llega a una convergencia12.1 Conceptos básicos 905Figura 12.1: Un programa con un apuntador de funciónMaq. Cap_12_AHO.indd 905 11/10/07 1:07:26 AM906 Capítulo 12. Análisis interprocedural12.1.2 Sensibilidad al contextoEl análisis interprocedural es retador, ya que el comportamiento de cada procedimiento depende del contexto en el que se llame. El ejemplo 12.2 utiliza el problema de la propagación constante interprocedural en un pequeño programa, para ilustrar la importancia de los contextos.Ejemplo 12.2: Considere el fragmento de programa en la figura 12.3. La función f se invocaen tres sitios de llamada: c1, c2 y c3. La constante 0 se pasa como el parámetro actual en c1,y la constante 243 se pasa en c2 y c3 en cada iteración; se devuelven las constantes 1 y 244,respectivamente. Así, la función f se invoca con una constante en cada uno de los contextos,pero el valor de la constante es dependiente del contexto.Como veremos más adelante, no es posible saber que a t1, t2 y t3 se les asignan valoresconstantes (por lo tanto a X[i]), a menos que reconozcamos que cuando se llama en el contextoc1, f devuelve 1, y cuando se llama en los otros dos contextos, f devuelve 244. Un análisis simple concluiría que f puede devolver 1 o 244 de cualquier llamada. ✷Un método simple pero bastante impreciso para el análisis interprocedural, conocido comoanálisis insensible al contexto, es tratar a cada instrucción de llamada y retorno como operaciones “goto”. Creamos un súper grafo de control de flujo en donde, además de las aristas decontrol de flujo intraprocedural normales, se crean aristas adicionales que conectan:1. Cada sitio de llamada con el inicio del procedimiento al que llama.2. Las instrucciones de retorno que regresan a los sitios de llamada.11En realidad, el retorno es a la instrucción que sigue después del sitio de llamada.Figura 12.2: Grafos de llamadas derivados de la figura 12.1CCCCCCFUNMAINFUNFUNMAINFUN(a) (b)Maq. Cap_12_AHO.indd 906 11/10/07 1:07:27 AMSe agregan instrucciones de asignación para asignar cada parámetro actual a su parámetroformal correspondiente, y para asignar el valor devuelto a la variable que recibe el resultado.Entonces podemos aplicar un análisis estándar que se pretende usar dentro de un procedimiento al súper grafo de control de flujo, para encontrar los resultados interprocedural insensiblesal contexto. Aunque es simple, este modelo abstrae la importante relación entre los valoresde entrada y salida en las invocaciones a procedimientos, lo cual hace que el análisis sea impreciso.Ejemplo 12.3: El súper grafo de control de flujo para el programa en la figura 12.3 se muestraen la figura 12.4. El bloque B 6 es la función f. El bloque B 3 contiene el sitio de llamada c1;establece el parámetro formal v a 0 y después salta al inicio de f, en B 6. De manera similar, B 4y B 5 representan los sitios de llamada c2 y c3, respectivamente. En B 4, al cual se llega desdeel final de f (bloque B 6), tomamos el valor de retorno de f y lo asignamos a t1. Después establecemos el parámetro formal v a 243 y llamamos a f de nuevo, saltando a B 6. Observe que nohay arista de B 3 a B 4. El control debe fluir a través de f en el camino de B 3 a B 4.B 5 es similar a B 4. Recibe el retorno de f, asigna el valor de retorno a t2 e inicia la tercerallamada a f. El bloque B 7 representa el retorno de la tercera llamada y la asignación a X[i ].Si tratamos a la figura 12.4 como si fuera el grafo de flujo de un solo procedimiento, entonces concluiríamos que al llegar a B 6, v puede tener el valor 0 o 243. Por ende, lo más quepodemos concluir acerca de valret es que se le asigna 1 o 244, pero ningún otro valor. Demanera similar, sólo podemos concluir acerca de t1, t2 y t3 que pueden ser 1 o 244. Por ende,X[i] parece ser 3, 246, 489 o 732. En contraste, un análisis sensible al contexto separaría los resultados para cada uno de los contextos de llamada y produciría la respuesta intuitiva descritaen el ejemplo 12.2: t1 siempre es 1, t2 y t3 siempre son 244, y X[i ] es 489. ✷12.1 Conceptos básicos 907Figura 12.3: Un fragmento de programa que ilustra la necesidad del análisis sensibleal contextoMaq. Cap_12_AHO.indd 907 11/10/07 1:07:28 AM908 Capítulo 12. Análisis interprocedural12.1.3 Cadenas de llamadasEn el ejemplo 12.2, podemos diferenciar entre los contextos con sólo conocer el sitio de llamadaque llama al procedimiento f. En general, un contexto de llamada se define mediante el contenido de toda la pila de llamadas. A la cadena de sitios de llamada en la pila se le conoce comocadena de llamadas.Ejemplo 12.4: La figura 12.5 es una ligera modificación de la figura 12.3. Aquí hemos sustituido las llamadas a f por llamadas a g, que a su vez llama a f con el mismo argumento. Hayun sitio de llamada adicional, c4, en donde g llama a f.Hay tres cadenas de llamadas a f : (c1, c4), (c2, c4) y (c3, c4). Como podemos ver en esteejemplo, el valor de v en la función f no depende del sitio inmediato o del último sitio c4 en lacadena de llamadas. En vez de ello, las constantes se determinan mediante el primer elementoen cada una de las cadenas de llamadas. ✷El ejemplo 12.4 ilustra que la información relevante para el análisis puede introducirse demanera anticipada en la cadena de llamadas. De hecho, algunas veces es necesario considerarFigura 12.4: El grafo de control de flujo para la figura 12.3, en donde las llamadas a funcionesse tratan como control de flujoIIFINGOTO,II8;I=TTTTTTTTVALRETFVALRETVCVCVTVALRETTVALRETCVBBBBBBB1234567Maq. Cap_12_A12.1 Conceptos básicos 909Figura 12.5: Fragmento de programa que ilustra las cadenas de llamadasFigura 12.6: Programa recursivo que requiere el análisis de las cadenas de llamadascompletastoda la cadena de llamadas completa para calcular la respuesta más precisa, como se ilustraen el ejemplo 12.5.Maq. Cap_12_AHO.indd 909 11/10/07 1:07:30 AM910 Capítulo 12. Análisis interproceduralEjemplo 12.5: Este ejemplo ilustra cómo la habilidad para razonar acerca de las cadenas dellamadas sin límites puede producir resultados más precisos. En la figura 12.6 podemos verque si g se llama con un valor positivo c, entonces g se invocará de manera recursiva c veces.Cada vez que se llama a g, el valor de su parámetro v disminuye por 1. Entonces, el valor delparámetro v de g en el contexto cuya cadena de llamadas es c2(c4)n es 234 − n. Por ende, elefecto de g es incrementar 0 o cualquier argumento negativo por 1, y devolver 2 en cualquierargumento de 1 o mayor.Hay tres posibles cadenas de llamadas para f. Si empezamos con la llamada en c1, entoncesg llama a f de inmediato, por lo que (c1, c5) es una de esas cadenas. Si empezamos en c2 o c3,entonces llamamos a g un total de 243 veces, y después llamamos a f. Estas cadenas de llamadas son (c2, c4, c4, . . . , c5) y (c3, c4, c4, . . . , c5), en donde en cada caso hay 242 c4s enla secuencia. En el primero de estos contextos, el valor del parámetro v de f es 0, mientras queen los otros dos contextos es 1. ✷Al diseñar un análisis sensible al contexto, tenemos una elección en la precisión. Por ejemplo, en vez de calificar los resultados mediante la cadena de llamadas completa, podemos optarpor sólo diferenciar un contexto de otro en base a sus k sitios de llamada más inmediatos. Estatécnica se conoce como análisis de contexto limitado por k. El análisis insensible al contextoes tan sólo un caso especial del análisis de contexto limitado por k, en donde k es 0. Podemosencontrar todas las constantes en el ejemplo 12.2, usando un análisis limitado por 1, y todas lasconstantes en el ejemplo 12.4 usando un análisis limitado por 2. Sin embargo, ningún análisislimitado por k puede encontrar todas las constantes en el ejemplo 12.5, siempre y cuando laconstante 243 se sustituya por dos constantes distintas y arbitrariamente grandes.En vez de elegir un valor fijo k, otra posibilidad es ser completamente sensible al contexto para todas las cadenas de llamadas acíclicas, que son cadenas que no contienen ciclosrecursivos. Para las cadenas de llamadas con recursividad, podemos colapsar todos los ciclos recursivos para poder limitar la cantidad de los distintos contextos analizados. En elejemplo 12.5, las llamadas iniciadas en el sitio de llamada c2 pueden aproximarse mediantela siguiente cadena de llamadas: (c2, c4*, c5). Observe que, con este esquema, aun para losprogramas sin recursividad, el número de distintos contextos de llamada puede ser exponencialen el número de procedimientos en el programa.12.1.4 Análisis sensible al contexto basado en la clonaciónOtro método para el análisis sensible al contexto es clonar el procedimiento en forma conceptual, uno para cada contexto único de interés. Después podemos aplicar un análisis insensible alcontexto al grafo de llamadas clonado. Los ejemplos 12.6 y 12.7 muestran el equivalente de unaversión clonada de los ejemplos 12.4 y 12.5, respectivamente. En realidad no necesitamos clonarel código; tan sólo podemos usar una representación interna eficiente para llevar la cuenta delos resultados del análisis de cada clon.Ejemplo 12.6: La versión clonada de la figura 12.5 se muestra en la figura 12.7. Como cadacontexto de llamada se refiere a un clon distinto, no hay confusión. Por ejemplo, g1 recibe0 como entrada y produce 1 como salida, y g2 y g3 reciben 243 como entrada y producen 244como salida. ✷Maq. Cap_12_AHO.indd 910 11/10/07 1:07:31 AMEjemplo 12.7: La versión clonada del ejemplo 12.5 se muestra en la figura 12.8. Para elprocedimiento g, creamos un clon para representar todas las instancias de g que se llaman porprimera vez de los sitios c1, c2 y c3. En este caso, el análisis determinaría que la invocación enel sitio de llamada c1 devuelve 1, suponiendo que el análisis puede deducir que con v = 0, laprueba v > 1 falla. Sin embargo, este análisis no maneja la recursividad lo bastante bien comopara producir las constantes para los sitios de llamada c2 y c3. ✷12.1.5 Análisis sensible al contexto basado en el resumenEl análisis interprocedural basado en el resumen es una extensión del análisis basado en regiones. Básicamente, en un análisis basado en el resumen cada procedimiento se representamediante una descripción concisa (“resumen”) que encapsula cierto comportamiento observable del procedimiento. El propósito principal del resumen es evitar reanalizar el cuerpo de unprocedimiento en cada sitio de llamada que pueda invocar al procedimiento.Primero vamos a considerar el caso en donde no hay recursividad. Cada procedimiento semodela como una región con un solo punto de entrada, en donde cada par emisor-receptor12.1 Conceptos básicos 911Figura 12.7: Versión clonada de la figura 12.5Maq. Cap_12_AHO.indd 911 11/10/07 1:07:32 AM912 Capítulo 12. Análisis interproceduralFigura 12.8: Versión clonada de la figura 12.6Maq. Cap_12_AHO.indd 912 11/10/07 1:07:33 AMcomparte una relación de región exterior-interior. La única diferencia en comparación con laversión intraprocedural es que, en el caso interprocedural, la región de un procedimiento puedeanidarse dentro de varias regiones externas distintas.El análisis consiste en dos partes:1. Una fase ascendente, que calcula una función de transferencia para resumir el efecto deun procedimiento.2. Una fase descendente, que propaga la información del emisor para calcular los resultadosde los receptores.Para obtener resultados completamente sensibles al contexto, la información de los distintoscontextos de llamada debe propagarse hacia abajo, a los receptores, en forma individual. Paraun cálculo más eficiente pero menos preciso, puede combinarse la información de todos los emisores, usando un operador de reunión, y después propagarse hacia abajo, a los receptores.Ejemplo 12.8: Para una propagación de constantes, cada procedimiento se resume mediante una función de transferencia que especifica cómo propagaría las constantes a través de sucuerpo. En el ejemplo 12.2, podemos resumir f como una función que, dada una constante ccomo un parámetro actual para v, devuelve la constante c + 1. Con base en esta información,el análisis determinaría que t1, t2 y t3 tienen los valores constantes 1, 244 y 244, respectivamente. Observe que este análisis no sufre la imprecisión debido a las cadenas de llamadasinalcanzables.Recuerde que el ejemplo 12.4 extiende al ejemplo 12.2 al hacer que g llame a f. Por ende,podríamos concluir que la función de transferencia para g es igual que la función de transferencia para f. De nuevo concluimos que t1, t2 y t3 tienen los valores constantes 1, 244 y 244,respectivamente.Ahora, vamos a considerar cuál es el valor del parámetro v en la función f para el ejemplo12.2. Como primera parte, podemos combinar todos los resultados para todos los contextos dellamada. Como v puede tener los valores 0 o 243, podemos simplemente concluir que v no esuna constante. Esta conclusión es justa, ya que no hay constante que pueda sustituir a v en elcódigo.Si deseamos resultados más precisos, podemos calcular resultados específicos para los contextos de interés. La información debe pasarse hacia abajo, desde el contexto de interés paradeterminar la respuesta sensible al contexto. Este paso es similar a la pasada descendente enel análisis basado en regiones. Por ejemplo, el valor de v es 0 en el sitio de llamada c1, y 243en los sitios c2 y c3. Para obtener la ventaja de la propagación de constantes dentro de f,debemos capturar esta distinción mediante la creación de dos clones, en donde el primero estáespecializado para el valor de entrada 0 y el segundo con el valor 243, como se muestra en lafigura 12.9. ✷Con el ejemplo 12.8 podemos ver que, al final, si deseamos compilar el código de maneradistinta en los distintos contextos, de todas formas tenemos que clonar el código. La diferenciaes que en el método basado en la clonación, ésta se realiza antes del análisis, con base en lascadenas de llamadas. En el método basado en el resumen, la clonación se realiza después delanálisis, usando los resultados del mismo como base.12.1 Conceptos básicos 913Maq. Cap_12_AHO.indd 913 11/10/07 1:07:35 AM914 Capítulo 12. Análisis interproceduralAun si no se aplica la clonación, en el método basado en el resumen las inferencias acerca delefecto de un procedimiento llamado se hacen con precisión, sin el problema de los caminos norealizables.En vez de clonar una función, también podríamos poner el código en línea. Esto tiene elefecto adicional de eliminar la sobrecarga de las llamadas a los procedimientos también.Podemos manejar la recursividad calculando la solución de punto fijo. En presencia de la recursividad, primero buscamos los componentes fuertemente conectados en el grafo de llamadas.En la fase de abajo hacia arriba, no visitamos un componente fuertemente conectado a menosque se hayan visitado todos sus sucesores. Para un componente fuertemente conectado que nosea trivial, calculamos en forma iterativa las funciones de transferencia para cada procedimiento en el componente hasta llegar a una convergencia; es decir, actualizamos en forma iterativalas funciones de transferencia hasta que no ocurran más cambios.12.1.6 Ejercicios para la sección 12.1Ejercicio 12.1.1: En la figura 12.10 hay un programa en C con dos apuntadores a funciones,p y q. N es una constante que podría ser menor o mayor que 10. Observe que el programaresulta en una secuencia infinita de llamadas, pero eso no es motivo de preocupación para losfines de este problema. a) Identifique todos los sitios de llamada en este programa. b) Para cada sitio de llamada, ¿a dónde puede apuntar p? ¿A dónde puede apuntar q ? c) Dibuje el grafo de llamadas para este programa. ! d) Describa todas las cadenas de llamadas para f y g.Figura 12.9: Resultado de propagar todos los posibles argumentos constantes a la función fMaq. Cap_12_AHO.indd 914 11/10/07 1:07:35 AMEjercicio 12.1.2: En la figura 12.11 hay una función id que viene siendo la “función identidad”; devuelve exactamente lo que recibe como argumento. También vemos un fragmento decódigo que consiste en una bifurcación, a la cual le sigue una asignación que suma x + y.a) Al examinar el código, ¿qué podemos averiguar acerca del valor de z al final?b) Construya el grafo de flujo para el fragmento de código, tratando las llamadas a id comocontrol de flujo.c) Si ejecutamos un análisis de propagación de constantes, como en la sección 9.4, en sugrafo de flujo del inciso (b), ¿qué valores constantes se determinan?d) ¿Cuáles son todos los sitios de llamada en la figura 12.11?e) ¿Cuáles son todos los contextos en los que se llama id?f) Rescriba el código de la figura 12.11, clonando una nueva versión de id para cada contexto en el que se llame.g) Construya el grafo de flujo de su código de (f), tratando las llamadas como control deflujo.12.1 Conceptos básicos 915Figura 12.10: Programa para el ejercicio 12.1.1Maq. Cap_12_AHO.indd 915 11/10/07 1:07:36 AM916 Capítulo 12. Análisis interproceduralh) Realice un análisis de propagación de constantes en su grafo de flujo de (g). ¿Qué valoresconstantes se determinan ahora?12.2 ¿Por qué análisis interprocedural?Debido a la dificultad del análisis interprocedural, vamos ahora a señalar el problema importante acerca de cuándo y cómo es conveniente usar este análisis. Aunque utilizamos la propagación de constantes para ilustrar el análisis interprocedural, esta optimización interproceduralno está disponible con facilidad ni es muy benéfica cuando ocurre. La mayoría de los beneficiosde la propagación de constantes se pueden obtener con sólo realizar un análisis intraproceduraly poner en línea las llamadas a procedimientos de las secciones de código que se ejecutan conmás frecuencia.Sin embargo, existen muchas razones por las que el análisis interprocedural es esencial.A continuación describiremos varias aplicaciones importantes de este análisis.12.2.1 Invocación de métodos virtualesComo dijimos antes, los programas orientados a objeto tienen muchos métodos pequeños. Sisólo optimizamos un método a la vez, entonces hay pocas oportunidades de optimización. Alresolver la invocación a un método se habilita la optimización. Un lenguaje como Java cargasus clases en forma dinámica. Como resultado, no sabemos en tiempo de compilación a cuálesde los (tal vez) muchos métodos llamados m hace referencia un uso de “m” en una invocacióncomo x.m().Muchas implementaciones de Java utilizan un compilador “just-in-time” para compilar susbytecodes en tiempo de ejecución. Una optimización común es perfilar la ejecución y determinar cuáles son los tipos comunes de receptores. Así, podemos poner en línea los métodos quese invocan con más frecuencia. El código incluye una comprobación dinámica sobre el tipo yejecuta los métodos en línea si el objeto en tiempo de ejecución tiene el tipo esperado.Es posible otro método para resolver los usos del nombre m de un método, siempre y cuando todo el código fuente esté disponible en tiempo de compilación. Entonces, es posible realizarun análisis interprocedural para determinar los tipos de los objetos. Si el tipo para una variablex resulta ser único, entonces puede resolverse un uso de x.m ().Figura 12.11: Fragmento de código para el ejercicio 12.12Maq. Cap_12_AHO.indd 916 11/10/07 1:07:37 AMSabemos con exactitud a qué método hace referencia en este contexto. En ese caso, podemos poner el código en línea para este método m, y el compilador ni siquiera tiene que incluiruna prueba para el tipo de x.12.2.2 Análisis de alias de apuntadoresAun si no deseamos ejecutar las versiones interprocedurales de los análisis comunes de flujosde datos, como las definiciones de alcance, estos análisis pueden de hecho beneficiarse del análisis de apuntadores interprocedural. Todos los análisis presentados en el capítulo 9 se aplicansólo a las variables escalares locales que no pueden tener alias. Sin embargo, es común el usode apuntadores, en especial en lenguajes como C. Al saber si los apuntadores pueden ser alias(si pueden apuntar a la misma ubicación) podemos mejorar la precisión de las técnicas delcapítulo 9.Ejemplo 12.9: Considere la siguiente secuencia de tres instrucciones, que podrían formar unbloque básico: *p = 1; *q = 2; x = *p;Sin saber si p y q pueden apuntar a la misma ubicación (es decir, si pueden o no ser alias) nopodemos concluir que x es igual a 1 al final del bloque. ✷12.2.3 ParalelizaciónComo vimos en el capítulo 11, la manera más efectiva de paralelizar una aplicación es buscarla granularidad más gruesa del paralelismo, como la que se encuentra en los ciclos más externosde un programa. Para esta tarea, el análisis interprocedural es de gran importancia. Existeuna diferencia considerable entre las optimizaciones escalares (las que se basan en valores devariables simples, como vimos en el capítulo 9) y la paralelización. En la paralelización, sólouna dependencia de datos espuria puede hacer que todo un ciclo completo no sea paralelizable,y reducir en gran parte la efectividad de la optimización. Dicha amplificación de imprecisionesno se ve en las optimizaciones escalares. En la optimización escalar, sólo debemos buscar lamayoría de las oportunidades de optimización. Pasar por alto una o dos oportunidades pocasveces hace la gran diferencia.12.2.4 Detección de errores de software y vulnerabilidadesEl análisis interprocedural no sólo es importante para optimizar código. Pueden usarse las mismas técnicas en el análisis del software existente para muchos tipos de errores de codificación.Estos errores pueden hacer que el software sea desconfiable; los errores de codificación que loshackers pueden explotar para tomar el control de (o de alguna otra forma dañar a) un sistemacomputacional pueden representar riesgos importantes de vulnerabilidad en la seguridad.12.2 ¿Por qué análisis interprocedural? 917Maq. Cap_12_AHO.indd 917 11/10/07 1:07:38 AM918 Capítulo 12. Análisis interproceduralEl análisis estático es útil para detectar ocurrencias de muchos patrones de error comunes.Por ejemplo, un elemento de datos debe protegerse mediante un candado. Como otro ejemplo,la deshabilitación de una interrupción en el sistema operativo debe ir seguida de una rehabilitación de la interrupción. Como las inconsistencias que abarcan los límites de los procedimientosson una fuente considerable de errores, el análisis interprocedural es de gran importancia. PREfix y Metal son dos herramientas prácticas que utilizan el análisis interprocedural con efectividad para buscar muchos errores de programación en programas extensos. Dichas herramientasbuscan errores en forma estática y pueden mejorar de manera considerable la confiabilidaddel software. Sin embargo, estas herramientas son tanto incompletas como poco sólidas, en elsentido de que tal vez no encuentren todos los errores, y no todas las advertencias reportadasson errores reales. Por desgracia, para reportar todos los errores potenciales el gran número deadvertencias falsas convertiría a las herramientas en algo inutilizable. Sin embargo, aun cuandoestas herramientas no son perfectas, su uso sistemático ha demostrado mejorar en gran medidala confiabilidad del software.Al tratarse de vulnerabilidades en la seguridad, es muy conveniente que encontremos todoslos errores potenciales en un programa. En el año 2006, dos de las formas “más populares” deintrusiones que utilizaron los hackers para poner en peligro un sistema fueron:1. La falta de validación de entrada en las aplicaciones Web: la inyección de código SQL esuna de las formas más populares de dicha vulnerabilidad, en donde los hackers obtienenel control de una base de datos al manipular las entradas que aceptan las aplicacionesWeb.2. Los desbordamientos de búfer en los programas en C y C++. Como C y C++ no comprueban si los accesos a los arreglos están dentro de los límites, los hackers pueden escribir cadenas bien elaboradas en áreas no planeadas y así obtener el control de la ejecucióndel programa.En la siguiente sección veremos cómo podemos usar el análisis interprocedural para protegerlos programas contra dichas vulnerabilidades.12.2.5 Inyección de código SQLLa inyección de código SQL se refiere a la vulnerabilidad en la que los hackers pueden manipular la entrada del usuario para una aplicación Web y obtener acceso no planeado a la basede datos. Por ejemplo, los bancos desean que sus usuarios puedan hacer transacciones en línea,siempre y cuando suministren su contraseña correcta. Una arquitectura común para dicho sistema es hacer que el usuario introduzca cadenas en un formulario Web, y después hacer queesas cadenas formen parte de una consulta de base de datos escrita en el lenguaje SQL. Si losdesarrolladores de sistemas no son cuidadosos, las cadenas que proporciona el usuario puedenalterar el significado de la instrucción SQL en formas inesperadas.Ejemplo 12.10: Suponga que un banco ofrece a sus clientes acceso a una relación como:DatosCta(nombre, contrasenia, saldo)Es decir, esta relación es una tabla de tripletas, cada una de las cuales consiste en el nombrede un cliente, la contraseña y el saldo de la cuenta. La intención es que los clientes puedan verMaq. Cap_12_AHO.indd 918 11/10/07 1:07:38 AMel saldo de su cuenta, sólo si pueden proporcionar su nombre y contraseña correctos. Que unhacker vea el saldo de una cuenta no es lo peor que podría ocurrir, pero este ejemplo simplees típico de las situaciones más complicadas, en donde el hacker podría realizar pagos desdela cuenta.El sistema podría implementar una consulta de saldo de la siguiente manera:1. Los usuarios invocan a un formulario Web, en el cual pueden introducir su nombre ycontraseña.2. El nombre se copia a una variable n y la contraseña a una variable p.3. Más adelante, tal vez en algún otro procedimiento, se ejecuta la siguiente secuenciaSQL: SELECT saldo FROM DatosCta WHERE nombre = ’:n’ and contrasenia = ’:p’Para los lectores que no estén familiarizados con SQL, esta consulta dice: “Buscar en la tablaDatosCta una fila con el primer componente (nombre) igual a la cadena actual en la variablen y el segundo componente (contrasenia) igual a la cadena actual en la variable p; imprimir eltercer componente (saldo) de esa fila”. Observe que SQL utiliza comillas sencillas, no dobles,para delimitar cadenas, y que los signos de punto y coma en frente de n y p indican que sonvariables del lenguaje circundante.Suponga que el hacker, que desea buscar el saldo de la cuenta de Charles Dickens, suministralos siguientes valores para las cadenas n y p:n = Charles Dickens’ −− p = que importaEl efecto de estas extrañas cadenas es convertir la consulta en lo siguiente:SELECT saldo FROM DatosCtaWHERE nombre = ’Charles Dickens’ −−’ and contrasenia = ’que importa’En muchos sistemas de bases de datos, los símbolos −− son un token para introducir comentarios, y tienen el efecto de convertir en comentario lo que vaya después en esa línea. Comoresultado, la consulta ahora pide al sistema de base de datos que imprima el saldo para cadapersona cuyo nombre sea ’Charles Dickens’, sin importar la contraseña que aparezca con esenombre en un registro formado por las variables nombre-contrasenia-saldo. Es decir, eliminando los comentarios la consulta sería:SELECT saldo FROM DatosCta WHERE nombre = ’Charles Dickens’ ✷En el ejemplo 12.10, las cadenas “malas” se mantienen en dos variables, que pueden pasarse entre un procedimiento y otro. Sin embargo, en casos más realistas, estas cadenas podríancopiarse varias veces, o combinarse con otras para formar la consulta completa. No podemosesperar detectar los errores de codificación que crean vulnerabilidades de inyección de códigoSQL sin realizar un análisis interprocedural completo de todo el programa.12.2 ¿Por qué análisis interprocedural? 919Maq. Cap_12_AHO.indd 919 11/10/07 1:07:39 AM920 Capítulo 12. Análisis interprocedural12.2.6 Desbordamiento de búferUn ataque por desbordamiento de búfer ocurre cuando los datos cuidadosamente planeados quesuministra el usuario escriben más allá del búfer destinado, y manipulan la ejecución del programa. Por ejemplo, un programa en C podría leer una cadena s del usuario, y después copiarlaen un búfer b, mediante la siguiente llamada a una función: strcpy(b,s);Si la cadena s es más larga que el búfer b, entonces se modificarán los valores de las ubicacionesque no sean parte de b. Es probable que eso en sí haga que el programa funcione de maneraincorrecta, o que por lo menos produzca la respuesta incorrecta, ya que se habrán modificadociertos datos que utiliza el programa.O peor aún, el hacker que seleccionó la cadena s puede elegir un valor que haga algo másque provocar un error. Por ejemplo, si el búfer está en la pila en tiempo de ejecución, entoncesestá cerca de la dirección de retorno para su función. Un valor de s elegido con dolo puedesobrescribir la dirección de retorno, y cuando la función regrese, irá a un lugar elegido por elhacker. Si los hackers tienen un conocimiento detallado del sistema operativo y el hardwaredel entorno, pueden llegar a ejecutar un comando que les otorgue el control de la máquina. Enciertas situaciones, incluso pueden tener la habilidad de hacer que la dirección de retorno falsatransfiera el control a cierto código que forma parte de la cadena s, con lo cual se permite lainserción de cualquier tipo de programa en el código en ejecución.Para evitar desbordamientos de búfer, hay que demostrar en forma estática que cada operación de escritura de arreglos se encuentra dentro de los límites, o debe realizarse en formadinámica una comprobación apropiada de los límites del arreglo. Como estas comprobacionesde los límites tienen que insertarse a mano en los programas en C y C++, es fácil olvidar insertar la prueba o hacerla mal. Se han desarrollado herramientas de heurística para comprobarsi por lo menos cierta prueba, aunque no sea necesariamente correcta, se ha realizado antes dellamar a una instrucción strcpy.La comprobación dinámica de los límites es inevitable, ya que es imposible determinar enforma estática el tamaño de la entrada del usuario. Todo lo que puede hacer un análisis estáticoes asegurar que las comprobaciones dinámicas se hayan insertado en forma apropiada. Porende, una estrategia razonable es hacer que el compilador inserte la comprobación dinámica delos límites en cada operación de escritura, y usar el análisis estático como medio para optimizartodas las comprobaciones de límites que sean posibles. Ya no es necesario atrapar todas laspotenciales violaciones; además, sólo debemos optimizar aquellas regiones de código que seejecutan con frecuencia.Insertar la comprobación de límites en programas en C no es algo trivial, incluso si no nosimporta el costo. Un apuntador podría apuntar a la parte media de cierto arreglo, sin quenosotros conociéramos la extensión de ese arreglo. Se han desarrollado técnicas para llevarel registro de la extensión del búfer al que apunta cada apuntador en forma dinámica. Estainformación permite al compilador insertar comprobaciones de límites de arreglos para todoslos accesos. Algo muy importante es que no es conveniente detener un programa cada vezque se detecta un desbordamiento de búfer. De hecho, estos desbordamientos ocurren en lapráctica, y es probable que un programa falle si deshabilitamos todos los desbordamientos debúfer. La solución es extender el tamaño del arreglo en forma dinámica para darles cabida.Maq. Cap_12_AHO.indd 920 11/10/07 1:07:39 AMPodemos usar el análisis interprocedural para agilizar el costo de las comprobaciones delos límites de los arreglos dinámicos. Por ejemplo, suponga que sólo nos interesa atrapar losdesbordamientos de búfer que involucran a las cadenas de entrada del usuario; podemos usarel análisis estático para determinar qué variables pueden guardar el contenido que proporciona elusuario. Al igual que la inyección de código SQL, es útil poder rastrear una entrada a medidaque se copia a través de los procedimentos para eliminar las comprobaciones de límites innecesarias.12.3 Una representación lógica del flujo de datosHasta este momento, nuestra representación de los problemas y soluciones del flujo de datospuede determinarse como “teoría de conjuntos”. Es decir, representamos la información comoconjuntos y calculamos los resultados usando operadores como la unión y la intersección. Porejemplo, cuando presentamos el problema de las definiciones de alcance en la sección 9.2.4,calculamos ENT[B ] y SAL[B ] para un bloque B, y los describimos como conjuntos de definiciones. Representamos el contenido del bloque B mediante sus conjuntos gen y eliminar.Para lidiar con la complejidad del análisis interprocedural, ahora vamos a presentar unanotación más general y concisa basada en la lógica. En vez de decir algo como “la definición Destá en ENT[B ]”, vamos a usar una notación como ent(B, D) para indicar lo mismo. Al haceresto, podemos expresar “reglas” concisas acerca de inferir los hechos del programa. Tambiénnos permite implementar estas reglas con eficiencia, de una forma que generalice el métodode vector de bits para las operaciones teóricas de conjuntos. Por último, el método lógico nospermite combinar lo que parecen ser varios análisis independientes en un algoritmo integrado.Por ejemplo, en la sección 9.5 describimos la eliminación de la redundancia parcial medianteuna secuencia de cuatro análisis de flujo de datos y otros dos pasos intermedios. En la notaciónlógica, todos estos pasos podrían combinarse en una colección de reglas lógicas que se resuelvenal mismo tiempo.12.3.1 Introducción a DatalogDatalog es un lenguaje que utiliza una notación parecida a Prolog, pero cuya semántica es mucho más simple. Para empezar, los elementos de Datalog son átomos de la forma p(X1, X2,...,Xn). Aquí,1. p es un predicado: un símbolo que representa a un tipo de instrucción como “una definición llega al inicio de un bloque”.2. X1, X2,..., Xn son términos como variables o constantes. También debemos permitir expresiones simples como argumentos de un predicado.2Un átomo base (ground atom) es un predicado que sólo tiene constantes como argumentos. Todo átomo base afirma un hecho específico, y su valor es verdadero o falso. A menudo12.3 Una representación lógica del fl ujo de datos 9212De manera formal, dichos términos se crean a partir de símbolos de funciones y complican la implementaciónde Datalog de manera considerable. Sin embargo, sólo vamos a usar algunos operadores, como la suma o resta deconstantes, en contextos que no compliquen las cosas.Maq. Cap_12_AHO.indd 921 11/10/07 1:07:40 AM922 Capítulo 12. Análisis interprocedurales conveniente representar un predicado mediante una relación, o tabla de sus átomos baseverdaderos. Cada átomo base se representa mediante una sola fila, o tupla, de la relación. Lascolumnas de la relación se nombran mediante atributos, y cada tupla tiene un componente paracada atributo. Los atributos corresponden a los componentes de los átomos base representadospor la relación. Cualquier átomo base en la relación es verdadero, y los átomos base que noestán en la relación son falsos.Ejemplo 12.11: Vamos a suponer que el predicado ent(B, D) significa “la definición D llegaal inicio del bloque B”. Entonces podríamos suponer que, para un grafo de flujo específico,ent(b 1, d 1) es verdadero, al igual que ent(b 2, d 1) y ent(b 2, d 2). También podríamos suponerque para este grafo de flujo, todos los demás hechos ent son falsos. Entonces, la relación en lafigura 12.12 representa el valor de este predicado para este grafo de flujo.Los atributos de la relación son B y D. Las tres tuplas de la relación son (b 1, d 1), (b 2, d 1)y (b 2, d 2). ✷También veremos a veces un átomo, que en realidad es una comparación entre variables yconstantes. Un ejemplo sería X ± Y o X = 10. En estos ejemplos, el predicado es en realidadel operador de comparación. Es decir, podemos considerar que X = 10 está escrita en formade predicado: igual(X, 10). Sin embargo, hay una diferencia importante entre los predicados decomparación y los demás. Un predicado de comparación tiene su interpretación estándar,mientras que un predicado ordinario como ent sólo significa lo que está definido para significarmediante un programa Datalog (que describiremos a continuación).Una literal es un átomo o un átomo negado. Indicamos la negación con la palabra NOT enfrente del átomo. Por ende, NOT ent(B, D) es una afirmación de que la definición D no llega alinicio del bloque B.12.3.2 Reglas de DatalogLas reglas son una forma de expresar las inferencias locales. En Datalog, las reglas tambiénsirven para sugerir cómo debe llevarse a cabo un cálculo de los hechos verdaderos. La formade una regla es:H :− B1 & B2 & ... & BnLos componentes son los siguientes:• H y B 1, B 2,..., B n son literales; ya sea átomos o átomos negados.Figura 12.12: Representación del valor de un predicado por una relaciónMaq. Cap_12_AHO.indd 922 11/10/07 1:07:40 AM• H es el encabezado y B 1, B 2,..., B n forman el cuerpo de la regla.• A cada una de las Bi’s se le conoce algunas veces como una submeta de la regla.Debemos leer el símbolo :− como “si”. El significado de una regla es “el encabezado esverdadero si el cuerpo es verdadero”. Dicho en forma más precisa, aplicamos una regla a unconjunto dado de átomos base como se muestra a continuación. Considere todas las posiblessustituciones de constantes por las variables de la regla. Si esta sustitución hace verdaderacada una de las submetas del cuerpo (suponiendo que todos y sólo los átomos base dados seanverdaderos), entonces podemos inferir que el encabezado con esta sustitución de constantes porvariables es un hecho verdadero. Las sustituciones que no hacen verdaderas a todas las submetas no nos proporcionan información; el encabezado puede o no ser verdadero.Un programa en Datalog es una colección de reglas. Este programa se aplica a los “datos”;es decir, a un conjunto de átomos base para algunos de los predicados. El resultado del programa es el conjunto de átomos base inferidos mediante la aplicación de las reglas, hasta que nopuedan hacerse más inferencias.Ejemplo 12.12: Un ejemplo simple de un programa en Datalog es el cálculo de las caminosen un grafo, dadas sus aristas (dirigidas). Es decir, hay un predicado arista(X, Y ) que significa“hay una arista que va del nodo X al nodo Y”. Otro predicado camino(X, Y ) significa que hayun camino que va de X a Y. Las reglas que definen los caminos son: 1) camino(X, Y ) :− arista(X, Y ) 2) camino(X, Y ) :− camino(X, Z) & camino(Z, Y )La primera regla dice que una arista individual es un camino. Es decir, cada vez que sustituimos la variable X por una constante a y la variable Y por una constante b, y arista(a, b) esverdadero (es decir, hay una arista que va del nodo a al nodo b), entonces camino(a, b) tambiénes verdadero (es decir, hay un camino que va de a a b). La segunda regla dice que si hay uncamino que va de cierto nodo X a cierto nodo Z, y también hay un camino que va de Z al nodo Y,entonces hay un camino que va de X a Y. Esta regla expresa el “cierre transitivo”. Observe quepuede formarse cualquier camino si tomamos las aristas a lo largo del camino y aplicamos laregla del cierre transitivo en forma repetida.12.3 Una representación lógica del fl ujo de datos 923Convenciones de DatalogVamos a utilizar las siguientes convenciones para los programas en Datalog:1. Las variables empiezan con letra mayúscula.2. Todos los demás elementos empiezan con letras minúsculas u otros símbolos comolos dígitos. Estos elementos incluyen predicados y constantes que son argumentosde predicados.Maq. Cap_12_AHO.indd 923 11/14/07 3:58:02 PM924 Capítulo 12. Análisis interproceduralPor ejemplo, suponga que los siguientes hechos (átomos base) son verdaderos: arsita(1, 2),arista(2, 3) y arista(3, 4). Entonces, podemos usar la primera regla con tres distintas sustituciones para inferir camino (1, 2), camino (2, 3) y camino (3, 4). Como ejemplo, al sustituirX = 1 y Y = 2 se instancia la primera regla para que sea camino (1, 2) : – arista(1, 2). Comoarista(1, 2) es verdadero, podemos inferir camino (1, 2).Con estos tres hechos camino, podemos usar la segunda regla varias veces. Si sustituimosX = 1, Z = 2 y Y = 3, instanciamos la regla para que sea camino (1, 3) : – camino (1, 2) &camino (2, 3). Como ambas submetas del cuerpo se han inferido, se sabe que son verdaderas,por lo que podemos inferir el encabezado: camino (1, 3). Entonces, la sustitución X = 1, Z = 3y Y = 4 nos permite inferir el encabezado camino (1, 4); es decir, hay un camino que va delnodo 1 al nodo 4. ✷12.3.3 Predicados intensionales y extensionalesEs convencional en los programas en Datalog distinguir unos predicados de otros de la siguientemanera:1. Los predicados EDB (extensional database), o base de datos extensional, son aquellosque se definen a priori. Es decir, sus hechos verdaderos se proporcionan en una relacióno tabla, o se proporcionan mediante el significado del predicado (como sería el caso paraun predicado de comparación, por ejemplo).2. Los predicados IDB (intensional database), o base de datos intensional, se definen sólomediante las reglas.Un predicado debe ser IDB o EDB, y sólo puede tener una de éstas. Como resultado, cualquierpredicado que aparece en el encabezado de una o más reglas debe ser un predicado IDB. Los predicados que aparecen en el cuerpo pueden ser IDB o EDB. Por ejemplo, en el ejemplo 12.12,arista es un predicado EDB y camino es un predicado IDB. Recuerde que se nos proporcionaron ciertos hechos arista, como arista(1, 2), pero los hechos camino fueron inferidos por lasreglas.Cuando se utilizan programas en Datalog para expresar los algoritmos de flujo de datos, lospredicados EDB se calculan a partir del mismo grafo de flujo. Entonces, los predicados IDB seexpresan mediante reglas, y el problema de flujo de datos se resuelve al inferir todos los posibleshechos IDB a partir de las reglas y de los hechos EDB dados.Ejemplo 12.13: Vamos a considerar cómo deben expresarse las definiciones de alcance enDatalog. En primer lugar, tiene sentido pensar a nivel de instrucción, en vez de hacerlo a nivelde bloque; es decir, la construcción de conjuntos gen y eliminar de un bloque básico se integrarán con el cálculo de las mismas definiciones de alcance. Por ende, el bloque b 1 sugerido en lafigura 12.13 es común. Observe que identificamos los puntos dentro del bloque numerado como0, 1,..., n, si n es el número de instrucciones en el bloque. La i-ésima definición está “en” elpunto i, y no hay una definición establecida en el punto 0.Un punto en el programa debe representarse mediante un par (b, n), en donde b es el nombre de un bloque y n es un entero entre 0 y el número de instrucciones en el bloque b. Nuestraformulación requiere dos predicados EDB:Maq. Cap_12_AHO.indd 924 11/10/07 1:07:42 AM1. def(B, N, X ) es verdadera si, y sólo si la N-ésima instrucción en el bloque B puede definir la variable X. Por ejemplo, en la figura 12.13 def (b 1, 1, X ) es verdadera, def (b 1, 3,X ) es verdadera, y def (b 1, 2, Y) es verdadera para toda posible variable Y a la que ppueda apuntar en ese punto. Por el momento vamos a suponer que Y puede ser cualquiervariable del tipo al que apunta p.2. suc (B, N, C) es verdadero si, y sólo si el bloque C es sucesor del bloque B en el grafo deflujo, y B tiene N instrucciones. Es decir, el control puede fluir desde el punto N de B,hasta el punto 0 de C. Por ejemplo, suponga que b 2 es un predecesor del bloque b 1 en lafigura 12.13, y que b 2 tiene 5 instrucciones. Entonces, suc (b 2, 5, b 1) es verdadero.Hay un predicado IDB, da(B, N, C, M, X ). Se pretende que sea verdadera si, y sólo si la definición de la variable X en la M-ésima instrucción del bloque C llega al punto N en el bloque B.Las reglas que definen el predicado da están en la figura 12.14.12.3 Una representación lógica del fl ujo de datos 925Figura 12.13: Un bloque básico con puntos entre las instruccionesFigura 12.14: Reglas para el predicado daLa regla (1) establece que si la N-ésima instrucción del bloque B define a X, entonces esadefinición de X llega al N-ésimo punto de B (es decir, el punto que está justo después de lainstrucción). Esta regla corresponde al concepto de “gen” en nuestra formulación teórica deconjunto anterior sobre las definiciones de alcance.La regla (2) representa la idea de que una definición pasa a través de una instrucción amenos que se “elimine”, y la única manera de eliminar una definición es redefinir su variablecon un 100% de certeza. En detalle, la regla (2) dice que la definición de la variable X de laM-ésima instrucción del bloque C llega al punto N del bloque B si:a) llega al punto anterior N − 1 de B, ydadada dadaMaq. Cap_12_AHO.indd 925 11/10/07 1:07:42 AM926 Capítulo 12. Análisis interproceduralb) hay por lo menos una variable Y, aparte de X, que puede estar definida en la N-ésimainstrucción de B.Por último, la regla (3) expresa el flujo de control en el grafo. Dice que la definición deX en la M-ésima instrucción del bloque C llega al punto 0 de B si hay algún bloque D conN instrucciones, de tal forma que la definición de X llegue al final de D, y B es un sucesorde D. ✷El predicado EDB suc del ejemplo 12.13 puede leerse claramente del grafo de flujo. Podemos obtener def del grafo de flujo también, si somos conservadores y suponemos que unapuntador puede apuntar a cualquier parte. Si deseamos limitar el rango de un apuntador alas variables del tipo apropiado, entonces podemos obtener la información del tipo de la tablade símbolos, y utilizar una relación def más pequeña. Una opción es hacer a def un predicadoIDB y definirlo mediante las reglas. Estas reglas utilizarán predicados EDB más primitivos, loscuales se pueden determinar a partir del grafo de flujo y la tabla de símbolos.Ejemplo 12.14: Suponga que introducimos dos nuevos predicados EDB:1. asignar (B, N, X ) es verdadero siempre que la N-ésima instrucción del bloque B tiene aX a la izquierda. Observe que X puede ser una variable o una expresión simple con unvalor l, como *p.2. tipo(X, T ) es verdadero si el tipo de X es T. De nuevo, X puede ser cualquier expresióncon un valor l, y T puede ser cualquier expresión para un tipo válido.Entonces, podemos escribir reglas para def, convirtiéndolo en un predicado IDB. La figura12.15 es una expansión de la figura 12.14, con dos de las posibles reglas para def. La regla (4)establece que la N-ésima instrucción del bloque B define a X, si X se asigna mediante la N-ésima instrucción. La regla (5) dice que X también puede definirse mediante la N-ésima instrucción del bloque B, si esa instrucción asigna a *P, y X es cualquiera de las variables del tipo alque apunta P. Otros tipos de asignaciones necesitarían otras reglas para def.Como ejemplo de la forma en que haríamos inferencias mediante el uso de las reglas de lafigura 12.15, volvamos a examinar el bloque b 1 de la figura 12.13. La primera instrucción asigna un valor a la variable x, por lo que el hecho asignar (b 1, 1, x ) estaría en el EDB. La tercerainstrucción también asigna a x, por lo que asignar (b 1, 3, x) es otro hecho EDB. La segunda instrucción asigna de manera indirecta a través de p, por lo que un tercer hecho EDB seríaasignar (b 1, 2, *p). Entonces, la regla (4) nos permite inferir def (b 1, 1, x ) y def (b 1, 3, x ).Suponga que p es del tipo apuntador a entero (*int ), y que x y y son enteros. Entoncespodemos usar la regla (5), con B = b 1, N = 2, P = p, T = int y X igual a x o y, para inferirdef (b 1, 2, x ) y def (b 1, 2, y ). De manera similar, podemos inferir lo mismo acerca de cualquierotra variable cuyo tipo sea entero o que obligue a un entero. ✷Maq. Cap_12_AHO.indd 926 11/10/07 1:07:43 AM12.3.4 Ejecución de programas en DatalogCada conjunto de reglas de Datalog define las relaciones para sus predicados IDB, como unafunción de las relaciones que se proporcionan por sus predicados EDB. Empiece con la suposición de que las relaciones IDB están vacías (es decir, los predicados IDB son falsos paratodos los posibles argumentos). Entonces, aplique las reglas en forma repetida, infiriendo nuevos hechos cada vez que las reglas requieran que lo hagamos. Cuando el proceso converja,hemos terminado y las relaciones IDB resultantes forman la salida del programa. Este procesose formaliza en el siguiente algoritmo, que es similar a los algoritmos iterativos que vimos enel capítulo 9.Algoritmo 12.15: Evaluación simple de programas en Datalog.ENTRADA: Un programa en Datalog y conjuntos de hechos para cada predicado EDB.SALIDA: Conjuntos de hechos para cada predicado IDB.MÉTODO: Para cada predicado p en el programa, haga que Rp sea la relación de hechos quesean verdaderos para ese predicado. Si p es un predicado EDB, entonces Rp es el conjunto dehechos dados para ese predicado. Si p es un predicado IDB, debemos calcular Rp. Ejecute elalgoritmo de la figura 12.16. ✷Ejemplo 12.16: El programa en el ejemplo 12.12 calcula los caminos en un grafo. Para aplicarel Algoritmo 12.15, empezamos con el predicado EDB arista que contiene todas las aristas delgrafo, y con la relación para camino vacío. En la primera ronda, la regla (2) no produce nada,ya que no hay hechos camino. Pero la regla (1) hace que todos los hechos arista se conviertanen hechos camino también. Es decir, después de la primera ronda conocemos camino(a, b) si, ysólo si hay una arista que va de a a b.12.3 Una representación lógica del fl ujo de datos 927Figura 12.15: Reglas para los predicados da y defdadadadadaasignarasignartipotipoMaq. Cap_12_AHO.indd 927 11/10/07 1:07:44 AM928 Capítulo 12. Análisis interproceduralEn la segunda ronda, la regla (1) no produce nuevos hechos de caminos, ya que la relaciónEDB arista nunca cambia. Sin embargo, ahora la regla (2) nos permite reunir dos caminos delongitud 1 para crear caminos de longitud 2. Es decir, después de la segunda ronda, camino(a, b)es verdadera si, y sólo si hay un camino de longitud 1 o 2 de a a b. De manera similar, en latercera ronda podemos combinar caminos de longitud 2 o menos para descubrir todos los caminos de longitud 4 o menos. En la cuarta ronda, descubrimos caminos de longitud hasta 8, yen general, después de la i-ésima ronda, camino(a, b) es verdadera si, y sólo si hay un caminode a a b de longitud 2i− 1 o menor. ✷12.3.5 Evaluación incremental de programas en DatalogHay una posible mejora de eficiencia en el Algoritmo 12.15. Observe que un nuevo hecho IDBsólo puede descubrirse en la ronda i si es el resultado de la sustitución de constantes en unaregla, de tal forma que por lo menos una de las submetas se convierta en un hecho que acabade descubrirse en la ronda i − 1. La prueba de esa afirmación es que si todos los hechos entrelas submetas se conocieran en la ronda i − 2, entonces el “nuevo” hecho se habría descubiertocuando hiciéramos la misma sustitución de constantes en la ronda i − 1.Para sacar provecho de esta observación, introduzca para cada predicado IDB p un predicado nuevoP que sólo contenga los hechos p recién descubiertos de la ronda anterior. Cadaregla que tenga uno o más predicados IDB entre las submetas se sustituye por una colecciónde reglas. Cada regla en la colección se forma sustituyendo exactamente una ocurrencia decierto predicado IDB q en el cuerpo por nuevoQ. Por último, para todas las reglas sustituimosel predicado de encabezado h por nuevoH. Se dice que las reglas resultantes están en formaincremental.Las relaciones para cada predicado IDB p acumula a todos los hechos p, como en el Algoritmo 12.15. En una ronda:1. Aplicamos las reglas para evaluar los predicados nuevoP.Figura 12.16: Evaluación de programas en Datalogfor (cada predicado IDB p)Rp = ∅;while (ocurran cambios a cualquier Rp) { considerar todas las posibles sustituciones de constantes para las variables en todas las reglas; determinar, para cada sustitución, si todas las submetas del cuerpo son verdaderas, usando las Rps actuales para determinar la verdad de los predicados EDB e IDB;if (una sustitución hace que el cuerpo de una regla sea verdadero) agregar el encabezado a Rq, si q es el predicado de encabezado;}Maq. Cap_12_AHO.indd 928 11/10/07 1:07:45 AM2. Entonces, se resta p de nuevoP, para asegurar que los hechos en nuevoP sean realmentenuevos.3. Se agregan los hechos en nuevoP a p.4. Se establecen todas las relaciones nuevasX a ∅ para la siguiente ronda.Formalizaremos estas ideas en el Algoritmo 12.18. Sin embargo, primero veremos un ejemplo.Ejemplo 12.17: Considere el programa en Datalog del ejemplo 12.12 otra vez. La formaincremental de las reglas se proporciona en la figura 12.17. La regla (1) no cambia excepto enel encabezado, ya que no tiene submetas IDB en el cuerpo. Sin embargo, la regla (2), con dossubmetas IDB, se convierte en dos reglas distintas. En cada regla, una de las ocurrencias decamino en el cuerpo se sustituye por nuevoCamino. En conjunto, estas reglas hacen valer laidea de que por lo menos uno de los dos caminos concatenados por la regla debe haberse descubierto en la ronda anterior. ✷12.3 Una representación lógica del fl ujo de datos 929Evaluación incremental de conjuntosTambién es posible resolver los problemas de flujo de datos teóricos de conjuntos enforma incremental. Por ejemplo, en las definiciones de alcance, una definición sólo puedeser recién descubierta en ENT[B] en la i-ésima ronda si acaba de descubrirse que está enSAL[P] para cierto predecesor P de B. La razón por la que en general no tratamos deresolver dichos problemas de flujo de datos de manera incremental es que la implementación en vectores de bits de los conjuntos es muy eficiente. Por lo general, es más fácilpasar a través de los vectores completos que decidir si un hecho es nuevo o no.Algoritmo 12.18: Evaluación incremental de programas en Datalog.ENTRADA: Un programa en Datalog y conjuntos de hechos para cada predicado EDB.SALIDA: Conjuntos de hechos para cada predicado IDB.Figura 12.17: Reglas incrementales para el programa de caminos en Datalog 1) nuevoCamino(X, Y) :− arista(X, Y) 2a) nuevoCamino(X, Y) :− camino(X, Z) & nuevoCamino(Z, Y ) 2b) nuevoCamino(X, Y) :− nuevoCamino(X, Z) & camino(Z, Y)Maq. Cap_12_AHO.indd 929 11/10/07 1:07:46 AM930 Capítulo 12. Análisis interproceduralMÉTODO: Para cada predicado p en el programa, haga que Rp sea la relación de hechosque sean verdaderos para ese predicado. Si p es un predicado EDB, entonces Rp es el conjuntode hechos dados para ese predicado. Si p es un predicado IDB, debemos calcular Rp. Además,para cada predicado IDB p, haga que RnuevoP sea una relación de “nuevos” hechos para el predicado p.1. Modifique las reglas en la forma incremental antes descrita.2. Ejecute el algoritmo de la figura 12.18. ✷12.3.6 Reglas problemáticas en DatalogExisten ciertas reglas o programas en Datalog que técnicamente no tienen significado, por loque no deben usarse. Los dos riesgos más importantes son:1. Reglas inseguras: aquellas que tienen una variable en el encabezado que no aparezca enel cuerpo, de una manera que restrinja a esa variable para que sólo reciba valores queaparezcan en el EDB.2. Programas no estratificados: conjuntos de reglas que tienen una recursividad que involucraa una negación.Ahora vamos a explicar con más detalle cada uno de estos riesgos.Figura 12.18: Evaluación de programas en Datalogfor (cada predicado IDB p) {Rp = ∅;RnuevoP = ∅;}repeat { considerar todas las posibles sustituciones de constantes para las variables en todas las reglas; determinar, para cada sustitución, si todas las submetas del cuerpo son verdaderas, usando las Rps y RnuevoPs actuales para determinar la verdad de los predicados EDB e IDB;if (una sustitución hace que el cuerpo de una regla sea verdadero) agregar el encabezado a RnuevoH, en donde h es el predicado de encabezado;for (cada predicado p) { RnuevoP = RnuevoP − Rp; Rp = Rp  RnuevoP; }} until (todos los RnuevoPs estén vacíos);Maq. Cap_12_AHO.indd 930 11/10/07 1:07:46 AMSeguridad en las reglasCualquier variable que aparezca en el encabezado de una regla debe también aparecer en elcuerpo. Además, la apariencia debe estar en una submeta que sea un átomo IDB o EDB ordinario. No es aceptable si la variable aparece sólo en un átomo negado, o sólo en un operador decomparación. La razón de esta política es evitar las reglas que nos permiten inferir un númeroinfinito de hechos.Ejemplo 12.19: La siguiente regla:p(X, Y ) :− q(Z) & NOT r(X) & X ≠ Yes insegura por dos razones. La variable X aparece sólo en la submeta negada r (X ) y la comparación X ≠ Y. Y sólo aparece en la comparación. La consecuencia es que p es verdadera paraun número infinito de pares (X, Y ), siempre y cuando r(X ) sea falsa y Y sea cualquier cosadistinta de X. ✷Datalog estratificadoPara que un programa pueda tener sentido, la recursividad y la negación deben separarse.El requerimiento formal es el siguiente. Debemos ser capaces de dividir los predicados IDBen estratos, por lo que si hay una regla con el predicado de encabezado p y una submeta dela forma NOT q (...), entonces q es un predicado EDB o IDB en un estrato más inferior que p.Mientras se cumpla esta regla, podemos evaluar los estratos, de menor a mayor, mediante elAlgoritmo 12.15 o 12.18, y después tratar las relaciones de los predicados IDB de los estratoscomo si fueran EDB para los cálculos de los estratos superiores. No obstante, si violamos estaregla, entonces el algoritmo iterativo no puede llegar a la convergencia, como se muestra en elsiguiente ejemplo.Ejemplo 12.20: Considere el programa en Datalog que consiste en una única regla:p(X ) :− e(X ) & NOT p(X )Suponga que e es un predicado EDB, y que sólo e(1) es verdadero. ¿p(1) es verdadero?Este programa no está estratificado. Sea cual fuere el estrato en el que coloquemos a p, suregla tiene una submeta que se niega y tiene un predicado IDB (es decir, el mismo p) que sinduda no se encuentra en un estrato menor a p.Si aplicamos el algoritmo iterativo, empezamos con Rp = ∅, por lo que al principio la respuesta es “no; p(1) no es verdadero”. Sin embargo, la primera iteración nos permite inferir ap(1), ya que tanto e(1) como NOT p(1) son verdaderos. Pero entonces la segunda iteración nosindica que p(1) es falso. Es decir, sustituir 1 por X en la regla no nos permite inferir p(1), yaque la segunda submeta NOT p(1) es falsa. De manera similar, la tercera iteración indica quep(1) es verdadera, la cuarta dice que es falsa, y así sucesivamente. Concluimos que este programa no estratificado carece de significado, y no lo consideramos un programa válido. ✷12.3 Una representación lógica del fl ujo de datos 931Maq. Cap_12_AHO.indd 931 11/10/07 1:07:47 AM932 Capítulo 12. Análisis interprocedural12.3.7 Ejercicios para la sección 12.3Ejercicio 12.3.1: En este problema, vamos a considerar un análisis de flujo de datos de definiciones de alcance que es más simple que el del ejemplo 12.13. Suponga que cada instrucciónes por sí sola un bloque, y asuma al principio que cada instrucción define sólo a una variable.El predicado EDB pred(I, J) indica que la instrucción I es un predecesor de la instrucción J. Elpredicado EDB define(I, X ) indica que la variable definida por la instrucción I es X. Vamos ausar los predicados IDB ent(I, D) y sal(I, D) para indicar que la definición D llega al inicio oal final de la instrucción I, respectivamente. Tenga en cuenta que una definición es en realidadun número de instrucción. La figura 12.19 es un programa en Datalog que expresa el algoritmousual para calcular definiciones de alcance.Observe que la regla (1) establece que una instrucción se elimina a sí misma, pero la regla(2) asegura que una instrucción está en su propio “conjunto salida” de todas formas. La regla (3) es la función de transferencia normal, y la regla (4) permite la confluencia, ya que Ipuede tener varios predecesores.Su problema es modificar las reglas para manejar el caso común en el que una definición esambigua; por ejemplo, una asignación a través de un apuntador. En esta situación, define(I,X ) puede ser verdadera para varias Xs distintas y una I. Una definición se representa mejormediante un par (D, X ), en donde D es una instrucción y X es una de las variables que puedendefinirse en D. Como resultado, ent y sal se convierten en predicados de tres argumentos; porejemplo, ent(I, D, X ) indica que la (posible) definición de X en la instrucción D llega al iniciode la instrucción I.Ejercicio 12.3.2: Escriba un programa en Datalog que sea similar a la figura 12.19, paracalcular las expresiones disponibles. Además del predicado define, use un predicado eval(I, X,O, Y ) que diga que la instrucción I hace que se evalúe la expresión XOY. Aquí, O es el operador en la expresión; por ejemplo, +.Ejercicio 12.3.3: Escriba un programa en Datalog que sea similar a la figura 12.19, paracalcular las variables vivas. Además del predicado define, suponga que hay un predicadousa(I, X ) que indica que la instrucción I usa a la variable X.Ejercicio 12.3.4: En la sección 9.5 definimos un cálculo de flujo de datos que involucrabaa seis conceptos: anticipado, disponible, primero, postergable, último y usado. Suponga queescribimos un programa en Datalog para definir cada uno de éstos en términos de conceptosFigura 12.19: Programa en Datalog para un análisis simple de definiciones de alcance1) eliminar(I, D) :− define(I, X ) & define(D, X )2) sal(I, I) :− define(I, X )3) sal(I, D) :− ent(I, D) & NOT eliminar (I, D)4) ent(I, D) :− sal(J, D) & pred(J, I)!Maq. Cap_12_AHO.indd 932 11/10/07 1:07:47 AMEDB que pueden derivarse del programa (por ejemplo, información de gen y eliminar) y otrosde estos seis conceptos. ¿Cuáles de los seis dependen de cuáles otros? ¿Cuáles de estas dependencias están negadas? ¿Estaría estratificado el programa en Datalog resultante?Ejercicio 12.3.5: Suponga que el predicado EDB arista(X, Y ) consiste en los siguientes hechos:arista(1, 2) arista(2, 3) arista(3, 4)arista(4, 1) arista(4, 5) arista(5, 6)a) Simule el programa en Datalog del ejemplo 12.12 con estos datos, usando la estrategiasimple de evaluación del Algoritmo 12.15. Muestre los hechos camino que se descubrenen cada ronda.b) Simule el programa en Datalog de la figura 12.17 con estos datos, como parte de la estrategia de evaluación incremental del Algoritmo 12.18. Muestre los hechos camino quese descubren en cada ronda.Ejercicio 12.3.6: La siguiente regla:p(X, Y) :− q(X, Z) & r(Z, W) & NOT p(W, Y)es parte de un programa P en Datalog más grande. a) Identifique el encabezado, el cuerpo y las submetas de esta regla. b) ¿Qué predicados son sin duda predicados IDB del programa P? ! c) ¿Qué predicados son sin duda predicados EDB de P? d) ¿La regla es segura? e) ¿P está estratificado?Ejercicio 12.3.7: Convierta las reglas de la figura 12.14 a la forma incremental.12.4 Un algoritmo simple de análisis de apuntadoresEn esta sección, comenzaremos la discusión de un análisis muy simple de alias de apuntadoresinsensible al flujo, suponiendo que no hay llamadas a procedimientos. En las secciones siguientes mostraremos cómo manejar los procedimientos, primero de manera insensible al contexto,y después de manera sensible al contexto. La sensibilidad al flujo agrega mucha complejidad, yes menos importante en cuanto a la sensibilidad al contexto para los lenguajes como Java, endonde los métodos tienden a ser pequeños.La pregunta fundamental que debemos hacer en el análisis de alias de apuntadores es si unpar dado de apuntadores puede aliarse. Una manera de responder es calcular para cada apuntador la respuesta a la pregunta: “¿a qué objetos puede apuntar este apuntador?” Si dos apuntadores pueden apuntar al mismo objeto, entonces pueden aliarse.12.4 Un algoritmo simple de análisis de apuntadores 933Maq. Cap_12_AHO.indd 933 11/10/07 1:07:48 AM934 Capítulo 12. Análisis interprocedural12.4.1 Por qué es difícil el análisis de apuntadoresEl análisis de alias de apuntadores para los programas en C es bastante difícil, ya que los programas en C pueden realizar cálculos arbitrarios con apuntadores. De hecho, podemos leer unentero y asignarlo a un apuntador, lo cual convertiría a este apuntador en un alias potencialde todas las demás variables apuntadores en el programa. Los apuntadores en Java, conocidoscomo referencias, son mucho más simples. No se permiten operaciones aritméticas, y los apuntadores sólo pueden apuntar al inicio de un objeto.El análisis de alias de apuntadores debe ser interprocedural. Sin este análisis, debemos asumir que cualquier método al que se llame puede modificar el contenido de todas las variablesapuntador accesibles, con lo que cualquier análisis de alias de apuntadores intraprocedural seríainefectivo.Los lenguajes que permiten llamadas indirectas a funciones presentan un reto adicionalpara el análisis de alias de apuntadores. En C, podemos llamar a una función de manera indirecta, para lo cual llamamos a un apuntador a función desreferenciado. Debemos saber a quédatos puede apuntar el apuntador a función para poder analizar la función a la que se llamó.Y sin duda, después de analizar esta función, es posible descubrir más funciones a las que puedeapuntar el apuntador a función y, por lo tanto, el proceso debe iterarse.Aunque la mayoría de las funciones se llaman de manera indirecta en C, los métodos virtuales en Java hacen que muchas invocaciones sean indirectas. Dada una invocación x.m() enun programa en Java, puede haber muchas clases a las que podría pertenecer el objeto x y quetienen un método llamado m. Entre más preciso sea nuestro conocimiento del tipo actual de x,más preciso será nuestro grafo de llamadas. De manera ideal, podemos determinar en tiempode compilación la clase exacta de x y así saber con precisión a qué método se refiere m.Ejemplo 12.21: Considere la siguiente secuencia de instrucciones en Java:Object = o; o = new String(); n = o.length();Aquí, o se declara como un Object. Sin analizar a dónde hace referencia o, hay que considerarcomo posibles destinos a todos los posibles métodos llamados “length”, declarados para todaslas clases. Saber que o apunta a un objeto String reduce el análisis interprocedural a sólo elmétodo declarado para String. ✷Es posible aplicar aproximaciones para reducir el número de destinos. Por ejemplo, podemos determinar de manera estática cuáles son todos los tipos de objetos creados, y podemoslimitar el análisis a ellos. Pero podemos ser más precisos si descubrimos el grafo de llamadasal instante, con base en el análisis tipo “apunta a” que se obtiene al mismo tiempo. Los grafosde llamadas más precisos no sólo conllevan a resultados más precisos, sino que también puedenreducir en gran parte el tiempo de análisis, que de otra forma se requeriría.El análisis tipo “apunta a” es complicado. No es uno de esos problemas “fáciles” de flujo dedatos, en donde sólo debemos simular el efecto de recorrer un ciclo de instrucciones una vez.En vez de ello, a medida que descubrimos nuevos destinos para un apuntador, hay que volver aanalizar todas las instrucciones que asignan el contenido de ese apuntador a otro apuntador.Maq. Cap_12_AHO.indd 934 11/10/07 1:07:49 AMPor cuestión de simplicidad, nos enfocaremos principalmente en Java. Empezaremos con elanálisis insensible al flujo y el análisis insensible al contexto, suponiendo por ahora que no sehacen llamadas a métodos en el programa. Después, describiremos cómo podemos descubrir elgrafo de llamadas al instante, a medida que se calculan los resultados del análisis tipo “apunta a”.Por último, describimos la manera de manejar la sensibilidad al contexto.12.4.2 Un modelo para apuntadores y referenciasVamos a suponer que nuestro lenguaje tiene las siguientes maneras de representar y manipularreferencias:1. Ciertas variables del programa son de tipo “apuntador a T” o “referencia a T ”, en dondeT es un tipo. Estas variables son estáticas o están vivas en la pila en tiempo de ejecución.Las llamamos simplemente variables.2. Hay un montículo de objetos. Todas las variables apuntan a objetos del montículo, no aotras variables. Estos objetos se conocen como objetos montículo.3. Un objeto del montículo puede tener campos, y el valor de un campo puede ser unareferencia a un objeto montículo (pero no a una variable).Java está bien modelado mediante esta estructura, por lo que utilizaremos la sintaxis de Javaen los ejemplos. Tenga en cuenta que C se modela con menos precisión, ya que las variablesapuntador pueden apuntar a otras variables apuntador en C, y en principio, cualquier valor enC puede verse obligado a convertirse en un apuntador.Como vamos a realizar un análisis insensible, sólo debemos afirmar que una variable v dadapuede apuntar a un objeto h del montículo dado; no tenemos que lidiar con la cuestión de enqué parte del programa puede v apuntar a h, o en qué contexto puede v apuntar a h. Sin embargo, tenga en cuenta que las variables pueden nombrarse mediante su nombre completo. EnJava, este nombre puede incorporar el módulo, clase, método y bloque dentro de un método,así como el mismo nombre de la variable. Así, podemos distinguir muchas variables que tienenel mismo identificador.Los objetos del montículo no tienen nombres. A menudo se utiliza la aproximación paranombrar a los objetos, ya que puede crearse un número ilimitado de objetos en forma dinámica.Una convención es hacer referencia a los objetos mediante la instrucción en la cual se crearon.Como una instrucción puede ejecutarse muchas veces y crear un nuevo objeto cada vez, unaafirmación como “v puede apuntar a h” en realidad significa “v puede apuntar a uno o más delos objetos creados en la instrucción etiquetada como h”.El objetivo del análisis es determinar a dónde pueden apuntar cada variable y cada campode cada objeto del montículo. A esto se le conoce como análisis tipo “apunta a”; dos apuntadores se alían si sus conjuntos “apunta a” se intersectan. Aquí describimos un análisis basadoen la inclusión; es decir, una instrucción como v = w hace que la variable v apunte a todos losobjetos a los que apunta w, pero no viceversa. Aunque este método puede parecer obvio, existen otras alternativas para la forma en que definimos el análisis tipo “apunta a”. Por ejemplo,podemos definir un análisis basado en equivalencia de forma que una instrucción como v = wconvertiría a las variables v y w en una clase de equivalencia, apuntando a todas las variables alas que cada una de ellas puede apuntar. Aunque esta formulación no aproxima bien los alias,12.4 Un algoritmo simple de análisis de apuntadores 935Maq. Cap_12_AHO.indd 935 11/10/07 1:07:49 AM936 Capítulo 12. Análisis interproceduralproporciona una respuesta rápida, y a menudo apropiada, para la pregunta sobre qué variablesapuntan al mismo tipo de objetos.12.4.3 Insensibilidad al flujoEmpezaremos por mostrar un ejemplo muy simple para ilustrar el efecto de ignorar el flujo decontrol en el análisis tipo “apunta a”.Ejemplo 12.22: En la figura 12.20 se crean tres objetos, h, i y j, y se asignan a las variablesa, b y c, respectivamente. Por ende, sin duda a apunta a h, b apunta a i, y c apunta a j al finalde la línea (3).Si seguimos las instrucciones (4) a (6), descubriremos que después de la línea (4), a apunta sólo a i. Después de la línea (5), b apunta sólo a j y después de la línea (6), c apunta sóloa i. ✷El análisis anterior es sensible al flujo, ya que seguimos el flujo de control y calculamos alo que puede apuntar cada variable después de cada instrucción. En otras palabras, además deconsiderar qué información tipo “apunta a” es la que “genera” cada instrucción, también consideramos qué información tipo “apunta a” es la que “elimina” cada instrucción. Por ejemplo,la instrucción b = c; elimina el hecho anterior “b apunta a j ” y genera la nueva relación “bapunta a lo que c apunta”.Un análisis insensible al flujo ignora el flujo de control, que en esencia asume que cada instrucción en el programa puede ejecutarse en cualquier orden. Calcula sólo un mapa global tipo“apunta a”, indicando qué es a lo que posiblemente puede apuntar cada variable en cualquierpunto de la ejecución del programa. Si una variable puede apuntar a dos objetos distintosdespués de dos instrucciones distintas en un programa, sólo registramos que puede apuntar aambos objetos. En otras palabras, en el análisis insensible al flujo, una asignación no “elimina”relaciones tipo “apunta a”, sino que sólo puede “generar” más relaciones tipo “apunta a”. Paracalcular los resultados insensibles al flujo, agregamos en forma repetida los efectos tipo “apunta a”de cada instrucción en las relaciones tipo “apunta a”, hasta que no se encuentran nuevas relaciones. Es evidente que la falta de sensibilidad al flujo debilita los resultados del análisis enforma considerable, pero tiende a reducir el tamaño de la representación de los resultados yhace que el algoritmo converja con más rapidez.Figura 12.20: Código en Java para el ejemplo 12.22Maq. Cap_12_AHO.indd 936 11/10/07 1:07:50 AMEjemplo 12.23: Regresando al ejemplo 12.22, las líneas (1) a (3) nos indican de nuevo que apuede apuntar a h, b puede apuntar a i, y c puede apuntar a j. Con las líneas (4) y (5), a puedeapuntar tanto a h como a i, y b puede apuntar tanto a i como a j. Con la línea (6), c puede apuntar a h, i y j. Esta información afecta a la línea (5), que a su vez afecta a la línea (4).Al final, nos quedamos con la inútil conclusión de que cualquier cosa puede apuntar a cualquierotra cosa. ✷12.4.4 La formulación en DatalogAhora vamos a formalizar un análisis de alias de apuntadores insensible al flujo, con base en laexplicación anterior. Por ahora ignoraremos las llamadas a procedimientos y nos concentraremos en los cuatro tipos de instrucciones que pueden afectar a los apuntadores:1. Creación de objetos. h: T v = new T(); Esta instrucción crea un nuevo objeto del montículo, y la variable v puede apuntar a éste.2. Instrucción de copia. v = w; Aquí, v y w son variables. La instrucción hace que v apuntea cualquier objeto del montículo al que w apunte en ese momento; es decir, w se copiaen v.3. Almacenamiento de campo. v.f = w; El tipo del objeto al que apunta v debe tener uncampo f, y este campo debe ser de cierto tipo de referencia. Haga que v apunte al objetodel montículo h, y que w apunte a g. Esta instrucción hace que el campo f en h apunteahora a g. Observe que la variable v no se modifica.4. Carga de campo. v = w.f; Aquí, w es una variable que apunta a cierto objeto del montículo que tiene un campo f, y f apunta a cierto objeto del montículo h. La instrucciónhace que la variable v apunte a h.Observe que los accesos a los campos compuestos en el código fuente, como v = w.f.g, sedescompondrán en dos instrucciones primitivas de carga de campos:v1 = w.f; v = v1.g;Ahora vamos a expresar el análisis de manera formal en las reglas de Datalog. En primerlugar, sólo hay dos predicados IDB que debemos calcular:1. apnt(V, H) indica que la variable V puede apuntar al objeto del montículo H.2. hapnt(H, F, G) indica que el campo F del objeto del montículo H puede apuntar al objeto del montículo G.Las relaciones EDB se construyen a partir del mismo programa. Como la ubicación de instrucciones en un programa es irrelevante cuando el análisis es insensible al flujo, sólo tenemosque afirmar en la EDB la existencia de instrucciones que tienen ciertas formas. A continuación,haremos una simplificación conveniente. En vez de definir las relaciones EDB para que guardenla información obtenida del programa, vamos a usar una instrucción entre comillas para sugerir12.4 Un algoritmo simple de análisis de apuntadores 937Maq. Cap_12_AHO.indd 937 11/10/07 1:07:50 AM938 Capítulo 12. Análisis interproceduralla relación o relaciones EDB que representan la existencia de dicha instrucción. Por ejemplo,“H : T V = new T()” es un hecho EDB que afirma que en la instrucción H hay una asignaciónque hace que la variable V apunte a un nuevo objeto de tipo T. Suponemos que en la práctica,habría una relación EDB correspondiente que se llenaría con átomos base, uno para cada instrucción de esta forma en el programa.Con esta convención, todo lo que tenemos que escribir en el programa en Datalog es unaregla para cada uno de los cuatro tipos de instrucciones. El programa se muestra en la figura12.21. La regla (1) dice que la variable V puede apuntar al objeto del montículo H si la instrucción H es una asignación de un nuevo objeto a V. La regla (2) dice que si hay una instrucciónde copia V = W, y W puede apuntar a H, entonces V puede apuntar a H.La regla (3) dice que si hay una instrucción de la forma V.F = W, W puede apuntar a G, yV puede apuntar a H, entonces el campo F de H puede apuntar a G. Por último, la regla (4)dice que si hay una instrucción de la forma V = W.F, W puede apuntar a G, y el campo F de Gpuede apuntar a H, entonces V puede apuntar a H. Observe que apnt y hapnt son mutuamenterecursivos, pero este programa en Datalog puede evaluarse mediante cualquiera de los algoritmos iterativos que vimos en la sección 12.3.4.12.4.5 Uso de la información de los tiposComo Java tiene seguridad en los tipos, las variables sólo pueden apuntar a los tipos que soncompatibles con los tipos declarados. Por ejemplo, al asignar un objeto que pertenezca a unasuperclase del tipo declarado de una variable se produciría una excepción en tiempo de ejecución. Considere el ejemplo simple de la figura 12.22, en donde S es una subclase de T. Esteprograma generará una excepción en tiempo de ejecución si p es verdadera, ya que a a no se lepuede asignar un objeto de la clase T. Por ende, podemos concluir en forma estática que debidoa la restricción de los tipos, a sólo puede apuntar a h y no a g.Figura 12.21: Programa en Datalog para el análisis de apuntadores insensible al flujoapntapntapntapntapntapntapnthapnthapntMaq. Cap_12_AHO.indd 938 11/10/07 1:07:51 AMAsí, introducimos en nuestro análisis tres predicados EDB que reflejan una informaciónimportante sobre los tipos en el código que se está analizando. Vamos a utilizar lo siguiente:1. vTipo(V, T) indica que la variable V se declara para tener el tipo T.2. hTipo(H, T) indica que el objeto del montículo H se asigna con el tipo T. El tipo de unobjeto creado no puede conocerse con precisión si, por ejemplo, el objeto se devuelve mediante un método simple. Dichos tipos se modelan de manera conservadora como todoslos tipos posibles.3. asignable(T, S) indica que un objeto de tipo S puede asignarse a una variable con eltipo T. Por lo general, esta información se recopila de la declaración de los subtipos enel programa, pero también incorpora información acerca de las clases predefinidas dellenguaje. asignable(T, T ) siempre es verdadera.Podemos modificar las reglas de la figura 12.21 para permitir inferencias, sólo si la variableasignada obtiene un objeto del montículo de un tipo que pueda asignarse. Las reglas se muestran en la figura 12.23.La primera modificación es a la regla (2). Las últimas tres submetas dicen que podemos sóloconcluir que V puede apuntar a H si hay tipos T y S que la variable V y el objeto del montículo H puedan tener respectivamente, de tal forma que puedan asignarse objetos de tipo S alas variables que sean referencias al tipo T. A la regla (4) se le agregó una restricción adicionalsimilar. Observe que no hay restricción adicional en la regla (3), ya que todas las operacionesde almacenamiento deben pasar a través de las variables. Cualquier restricción de tipos sóloatraparía un caso extra, cuando el objeto base es una constante nula.12.4.6 Ejercicios para la sección 12.4Ejercicio 12.4.1: En la figura 12.24, h y g son etiquetas que se utilizan para representarobjetos recién creados, y no forman parte del código. Podemos asumir que los objetos de tipoT tienen un campo f. Use las reglas de Datalog de esta sección para inferir todos los posibleshechos apnt y hapnt.Figura 12.22: Programa en Java con un error de tipo12.4 Un algoritmo simple de análisis de apuntadores 939Maq. Cap_12_AHO.indd 939 11/10/07 1:07:52 AM940 Capítulo 12. Análisis interproceduralEjercicio 12.4.2: Si aplicamos el algoritmo de esta sección al siguiente código:inferiríamos que tanto a como b pueden apuntar a h y a g. Si el código se hubiera escrito así:inferiríamos correctamente que a puede apuntar a h, y que b y c pueden apuntar a g. Sugieraun análisis de flujo de datos intraprocedural que pueda evitar este tipo de imprecisión.Figura 12.23: Agregar restricciones de tipo al análisis de apuntadores insensible al flujoFigura 12.24: Código para el ejercicio 12.4.11) apnt(V, H) :− “H: T V = new T()”2) apnt(V, H) :− “V = W” & apnt(W, H) & vTipo(V, T ) & hTipo(H, S) & asignable(T, S)3) hapnt(H, F, G) :− “V.F = W” & apnt(W, G) & apnt(V, H)4) apnt(V, H) :− “V = W.F” & apnt(W, G) & hapnt(G, F, H) & vTipo(V, T ) & hTipo(H, S) & asignable(T, S)!Maq. Cap_12_AHO.indd 940 11/10/07 1:07:53 AMEjercicio 12.4.3: Podemos extender el análisis de esta sección para que sea interprocedural sisimulamos la llamada y el retorno como si fueran operaciones de copia, como en la regla (2) dela figura 12.21. Es decir, una llamada copia los valores actuales a sus correspondientes valoresformales, y el retorno copia la variable que contiene el valor de retorno a la variable a la que sele asigna el resultado de la llamada. Considere el programa de la figura 12.25.a) Realice un análisis insensible sobre este código.b) Algunas de las inferencias hechas en (a) son en realidad “falsas”, ya que no representanningún evento que pueda ocurrir en tiempo de ejecución. El problema puede rastrearsehasta las múltiples asignaciones a la variable b. Rescriba el código de la figura 12.25 demanera que ninguna variable se asigne más de una vez. Vuelva a ejecutar el análisis ymuestre que cada hecho apnt y hapnt inferido puede ocurrir en tiempo de ejecución.12.5 Análisis interprocedural insensible al contextoAhora consideraremos las invocaciones a los métodos. Primero explicaremos cómo puede usarse el análisis tipo “apunta a” para calcular un grafo de llamadas preciso, el cual es útil paracalcular los resultados precisos del tipo “apunta a”. Después formalizaremos el descubrimientode grafos de llamadas al instante y mostraremos cómo se puede usar Datalog para describir elanálisis en forma concisa.12.5.1 Efectos de la invocación a un métodoLos efectos de la llamada a un método como x = y.n(z) en Java, en las relaciones del tipo“apunta a”, puede calcularse de la siguiente manera:1. Determine el tipo del objeto receptor, que es el objeto al que apunta y. Suponga que sutipo es t. Haga que m sea el método de nombre n en la superclase más estrecha de t queFigura 12.25: Código de ejemplo para el análisis de apuntadores12.5 Análisis interprocedural insensible al contexto 941!Maq. Cap_12_AHO.indd 941 11/10/07 1:07:54 AM942 Capítulo 12. Análisis interproceduraltenga un método llamado n. Observe que, en general, el método que se invoque sólo sepuede determinar en forma dinámica.2. A los parámetros formales de m se les asignan los objetos a los que apuntan los parámetros actuales. Estos parámetros actuales incluyen no sólo los parámetros que se pasandirectamente, sino también el mismo objeto receptor. Cada invocación a un métodoasigna el objeto receptor a la variable this.3 Nos referimos a las variables this comolos 0-ésimos parámetros formales de los métodos. En x = y.n(z), hay dos parámetrosformales: el objeto al que apunta y se asigna a la variable this, y el objeto al que apuntaz se asigna al primer parámetro formal declarado de m.3. El objeto devuelto de m se asigna a la variable del lado izquierdo de la instrucción deasignación.En el análisis sensible al contexto, los parámetros y los valores devueltos se modelan mediante instrucciones de copia. La pregunta interesante que queda por contestar es cómo determinar el tipo del objeto receptor. Podemos determinar de manera conservadora el tipo, deacuerdo con la declaración de la variable; por ejemplo, si la variable declarada tiene el tipo t,entonces sólo se pueden invocar los métodos llamados n en los subtipos de t. Por desgracia,si la variable declarada tiene el tipo Object, entonces todos los métodos con el nombre n sondestinos potenciales. En los programas reales que utilizan mucho las jerarquías de objetos eincluyen muchas bibliotecas grandes, dicho método puede producir muchos destinos de llamadaespurios, con lo cual el análisis se hace lento e impreciso.Debemos saber a dónde apuntan las variables para poder calcular los destinos de llamada;pero a menos que conozcamos los destinos de llamada, no podemos averiguar a dónde puedenapuntar todas las variables. Esta relación recursiva requiere que descubramos los destinosde llamada al instante, a medida que calculamos el conjunto “apunta a”. El análisis continúahasta que no haya nuevos destinos de llamada y que no se encuentren nuevas relaciones “apunta a”.Ejemplo 12.24: En el código de la figura 12.26, r es un subtipo de s, que a su vez es unsubtipo de t. Con sólo utilizar la información del tipo declarado, a.n() podemos invocar cualquiera de los tres métodos declarados con el nombre n, ya que s y r son ambos subtipos deltipo declarado de a, t. Además, parece que a puede apuntar a los objetos g, h e i después dela línea (5).Al analizar las relaciones tipo “apunta a”, primero determinamos que a puede apuntara j, un objeto de tipo t. Por ende, el método declarado en la línea (1) es un destino de llamada.Al analizar la línea (1), determinamos que a también puede apuntar a g, un objeto de tipo r.Por ende, el método declarado en la línea (3) también puede ser un destino de llamada, y apuede ahora apuntar a i, otro objeto de tipo r. Como no hay más nuevos destinos de llamada,el análisis termina sin analizar el método declarado en la línea (2) y sin concluir que a puedeapuntar a h. ✷3Recuerde que las variables se diferencian mediante el método al que pertenecen, por lo que no hay sólo una variable llamada this, sino una de estas variables para cada método en el programa.Maq. Cap_12_AHO.indd 942 11/10/07 1:07:55 AM12.5.2 Descubrimiento del grafo de llamadas en DatalogPara formular las reglas de Datalog para el análisis interprocedural insensible al contexto,introducimos tres predicados EDB, cada uno de los cuales puede obtenerse con facilidad delcódigo fuente:1. actual(S, I, V) dice que V es el I-ésimo parámetro actual que se utiliza en el sitio dellamada S.2. formal(M, I, V) dice que V es el I-ésimo parámetro formal declarado en el método M.3. ajk (T, N, M) dice que M es el método que se llama cuando N se invoca en un objetoreceptor de tipo T. (ajk significa “análisis de jerarquía de clases”.)Cada arista del grafo de llamadas se representa mediante un predicado IDB llamado invoca.Al descubrir más aristas en el grafo de llamadas, se crean más relaciones tipo “apunta a” a medida que se pasan como entrada los parámetros y se pasan como salida los valores de retorno.Este efecto se sintetiza mediante las reglas que se muestran en la figura 12.27.La primera regla calcula el destino de llamada del sitio de llamada. Es decir, “S : V.N(…)”dice que hay un sitio de llamada etiquetado como S, que invoca al método llamado N en elobjeto receptor al que apunta V. Las submetas dicen que si V puede apuntar al objeto delmontículo H, que se asigna como el tipo T, y M es el método que se utiliza cuando N se invocaen objetos de tipo T, entonces el sitio de llamada S puede invocar al método M.La segunda regla dice que si el sitio S puede llamar al método M, entonces cada parámetroformal de M puede apuntar a cualquier parte a la que pueda apuntar el parámetro actual de lallamada. La regla para manejar los valores devueltos se deja como ejercicio.Al combinar estas dos reglas con las que se explican en la sección 12.4 se crea un análisistipo “apunta a” insensible al contexto, el cual utiliza un grafo de llamadas que se calcula al instante. Este análisis tiene el efecto adicional de crear un grafo de llamadas utilizando un análisisFigura 12.26: Una invocación a un método virtual12.5 Análisis interprocedural insensible al contexto 943Maq. Cap_12_AHO.indd 943 11/10/07 1:07:55 AM944 Capítulo 12. Análisis interproceduraltipo “apunta a” insensible al contexto y al flujo. Este grafo de llamadas es mucho más precisoque uno que se calcula sólo en base a las declaraciones de tipos y el análisis sintáctico.12.5.3 Carga dinámica y reflexiónLos lenguajes como Java permiten la carga dinámica de clases. Es imposible analizar todo elcódigo posible que ejecuta un programa, y por ende imposible proporcionar cualquier aproximación conservadora de los grafos de llamadas o de los alias de apuntadores en forma estática.El análisis estático sólo puede proporcionar una aproximación con base en el código analizado.Recuerde que todos los análisis aquí descritos se pueden aplicar en el nivel de bytecodes deJava y, por lo tanto, no es necesario examinar el código fuente. Esta opción es muy importante,ya que los programas en Java tienden a usar muchas bibliotecas.Aun si suponemos que se analiza todo el código que se va a ejecutar, hay una complicaciónmás que hace imposible el análisis conservador: la reflexión. La reflexión permite a un programa determinar en forma dinámica los tipos de los objetos a crear, los nombres de los métodosinvocados, así como los nombres de los campos a los que se accede. El tipo, el método y losnombres de los campos pueden calcularse o derivarse de la entrada del usuario, por lo que engeneral, la única aproximación posible es suponer el universo.Ejemplo 12.25: El siguiente código muestra un uso común de la reflexión:1) String nombreClase = ...;2) Class c = Class.forName(nombreClase);3) Object o = c.newInstance();4) T t = (T) o;El método forName en la biblioteca Class recibe una cadena que contiene el nombre de laclase y devuelve la clase. El método newInstance devuelve una instancia de esa clase. En vezde dejar el objeto o con el tipo Object, este objeto se convierte en una superclase T de todaslas clases esperadas. ✷Figura 12.27: Datal program for call-graph discoveryMaq. Cap_12_AHO.indd 944 11/10/07 1:07:56 AMAunque muchas aplicaciones extensas de Java utilizan la reflexión, tienden a usar especificaciones comunes, como el que se muestra en el ejemplo 12.25. Siempre y cuando la aplicaciónno redefina el cargador de clases, podemos conocer la clase del objeto si conocemos el valorde nombreClase. Si el valor de nombreClase se define en el programa, como las cadenas soninmutables en Java, saber a qué apunta nombreClase nos proporciona el nombre de la clase.Esta técnica es otro uso del análisis tipo “apunta a”. Si el valor de nombreClase se basa en laentrada del usuario, entonces el análisis tipo “apunta a” puede ayudar a localizar en dónde seintroduce el valor, y el desarrollador puede limitar el alcance de su valor.De manera similar, podemos explotar la instrucción de conversión de tipos, la línea (4) enel ejemplo 12.25, para aproximar el tipo de los objetos creados en forma dinámica. Suponiendoque no se ha redefinido el manejador de excepciones de conversión de tipo, el objeto debe pertenecer a una subclase de la clase T.12.5.4 Ejercicios para la sección 12.5Ejercicio 12.5.1: Para el código de la figura 12.26:a) Construya las relaciones EDB actual, formal y ajk.b) Haga todas las posibles inferencias de los hechos apnt y hapnt.Ejercicio 12.5.2: ¿Cómo agregaría a los predicados EDB y a las reglas de la sección 12.5.2predicados y reglas adicionales, para tomar en cuenta el hecho de que si la llamada a un método devuelve un objeto, entonces la variable a la cual se asigna el resultado de la llamadapuede apuntar a cualquier lugar a donde pueda apuntar la variable que contiene el valor deretorno?12.6 Análisis de apuntadores sensible al contextoComo vimos en la sección 12.1.2, la sensibilidad al contexto puede mejorar en gran parte laprecisión del análisis interprocedural. Hablamos acerca de dos métodos para este análisis, unobasado en la clonación (sección 12.1.4) y otro basado en los resúmenes (sección 12.1.5). ¿Cuáldebemos usar?Hay varias dificultades para calcular los resúmenes de la información tipo “apunta a”. Enprimer lugar, los resúmenes son extensos. El resumen de cada método debe incluir el efecto detodas las actualizaciones que pueden hacer la función y todos los métodos llamados, en términos de los parámetros de entrada. Es decir, un método puede cambiar los conjuntos “apunta a”de todos los datos que se pueden alcanzar a través de las variables estáticas, los parámetrosentrantes y todos los objetos creados por el método y todos los métodos a los que llama. Aunque se han propuesto esquemas complicados, no se conoce una solución que pueda escalar aprogramas extensos. Incluso si los resúmenes pueden calcularse en una pasada de abajo haciaarriba, el cálculo de los conjuntos “apunta a” para todos los exponencialmente diversos contextos en una típica pasada descendente presenta un problema aún mayor. Dicha informaciónes necesaria para las consultas globales, como buscar todos los puntos en el código que entrenen contacto con cierto objeto.12.6 Análisis de apuntadores sensible al contexto 945!Maq. Cap_12_AHO.indd 945 11/10/07 1:07:57 AM946 Capítulo 12. Análisis interproceduralEn esta sección veremos un análisis sensible al contexto, basado en la clonación. Lo únicoque hace el análisis basado en la clonación es clonar los métodos, uno para cada contexto deinterés. Después aplicamos el análisis insensible al contexto al grafo de llamadas clonado. Aunque este método parece simple, el problema está en los detalles de manejar el extenso númerode clones. ¿Cuántos contextos hay? Aun si utilizamos la idea de colapsar todos los ciclos recursivos, como vimos en la sección 12.1.3, es común encontrar 1014 contextos en una aplicación enJava. El reto es representar los resultados de estos diversos contextos.Vamos a separar la explicación de la sensibilidad al contexto en dos partes:1. ¿Cómo podemos manejar la sensibilidad al contexto en forma lógica? Esta parte esfácil, ya que tan sólo aplicamos el algoritmo insensible al contexto al grafo de llamadasclonado.2. ¿Cómo representar los exponencialmente diversos contextos? Una manera es representarla información como diagramas de decisiones binarios (BDDs), una estructura de datosmuy optimizada que se ha utilizado para muchas otras aplicaciones.Este método para la sensibilidad al contexto es un excelente ejemplo de la importancia dela abstracción. Como veremos en breve, para eliminar la complejidad algorítmica aprovechamoslos años de trabajo invertidos en la abstracción del BDD. Podemos especificar un análisis tipo“apunta a” sensible al contexto en sólo unas cuantas líneas de Datalog, que a su vez aprovechalos varios miles de líneas de código existente para la manipulación de BDDs. Este método tienevarias ventajas importantes. En primer lugar, hace posible la expresión fácil de análisis posteriores que utilizan los resultados del análisis tipo “apunta a”. Después de todo, los resultadosdel análisis “apunta a” por sí solos no son interesantes. En segundo lugar, facilita en formaconsiderable la escritura correcta del análisis, ya que aprovecha muchas líneas de código biendepurado.12.6.1 Contextos y cadenas de llamadasEl análisis tipo “apunta a” sensible al contexto antes descrito asume que ya se ha calculado ungrafo de llamadas. Este paso ayuda a hacer posible una representación compacta de los diversoscontextos de llamada. Para obtener el grafo de llamadas, primero ejecutamos un análisis tipo“apunta a” insensible al contexto que calcula el grafo de llamadas al instante, como vimos enla sección 12.5. Ahora describiremos cómo crear un grafo de llamadas clonado.Un contexto es una representación de la cadena de llamadas que forma el historial de lasllamadas a las funciones activas. Otra forma de ver el contexto es como un resumen de la secuencia de llamadas, cuyos registros de activación se encuentran en ese momento en la pila entiempo de ejecución. Si no hay funciones recursivas en la pila, entonces la cadena de llamadas(la secuencia de ubicaciones desde las cuales se hicieron las llamadas en la pila) es una representación completa. También es una representación aceptable, en el sentido de que sólo hay unnúmero finito de contextos distintos, aunque ese número puede ser exponencial en el númerode funciones en el programa.No obstante, si hay funciones recursivas en el programa, entonces el número de posiblescadenas de llamadas es infinito, y no podemos permitir que todas las posibles cadenas de llamadas representen distintos contextos. Hay varias formas en las que podemos limitar el númeroMaq. Cap_12_AHO.indd 946 11/10/07 1:07:58 AMde distintos contextos. Por ejemplo, podemos escribir una expresión regular que describa todaslas posibles cadenas de llamadas y que convierta esa expresión regular en un autómata finitodeterminista, usando los métodos de la sección 3.7. Así, los contextos pueden identificarse conlos estados de este autómata.Aquí vamos a adoptar un esquema más simple, que captura el historial de las llamadas norecursivas pero considera que las llamadas recursivas son “demasiado difíciles de desenmarañar”. Empezaremos por buscar todos los conjuntos mutuamente recursivos de funciones en elprograma. El proceso es simple y no lo elaboraremos con detalle aquí. Piense en un grafo cuyosnodos son las funciones, con una arista que va de p a q si la función p llama a la función q.Los componentes fuertemente conectados (SCCs) de este grafo son los conjuntos de funcionesmutuamente recursivas. Como un caso especial común, una función p que se llama a sí misma,pero que no está en un SCC con cualquier otra función es un SCC por sí sola. Las funciones norecursivas son también SCCs por sí solas. A un SCC le llamamos no trivial si tiene más de unmiembro (el caso mutuamente recursivo), o si tiene un solo miembro recursivo. Los SCCs queson funciones no recursivas individuales son SCCs triviales.La siguiente es nuestra modificación de la regla que establece que cualquier cadena de llamadas es un contexto. Dada una cadena de llamadas, se elimina la ocurrencia de un sitio dellamada s si:1. s está en una función p.2. La función q se llama en el sitio s (es posible que q = p).3. p y q se encuentran en el mismo componente fuerte (es decir, p y q son mutuamenterecursivas, o p = q y p es recursiva).El resultado es que cuando se hace una llamada a un miembro de un SCC S no trivial, el sitiopara esa llamada se vuelve parte del contexto, pero las llamadas dentro de S a otras funcionesen el mismo SCC no forman parte del contexto. Por último, cuando se hace una llamada fuerade S, registramos ese sitio de llamada como parte del contexto.Ejemplo 12.26: En la figura 12.28 hay un bosquejo de cinco funciones con algunos sitios dellamada y ciertas llamadas entre ellos. Un análisis de las llamadas muestra que q y r son mutuamente recursivas. Sin embargo, p, s y t no son recursivas en definitiva. Por ende, nuestroscontextos serán listas de todos los sitios de llamada excepto s3 y s5, en donde se realizan lasllamadas entre q y r.Vamos a considerar todas las formas en las que podríamos llegar de p a t; es decir, todos loscontextos en los que ocurren llamadas a t:1. p podría llamar a s en s2, y después s podría llamar a t en s7 o s8. Por ende, dos posiblescadenas de llamadas son (s2, s7) y (s2, s8).2. p podría llamar a q en s1. Entonces, q y r podrían llamarse entre sí en forma recursiva,un cierto número de veces. Podríamos descomponer el ciclo: (a) En s4, en donde q llama directamente a t. Esta opción nos conduce a sólo un contexto, (s1, s4).12.6 Análisis de apuntadores sensible al contexto 947Maq. Cap_12_AHO.indd 947 11/10/07 1:07:58 AM948 Capítulo 12. Análisis interprocedural (b) En s6, en donde r llama a s. Aquí podríamos llegar a t mediante la llamada a s7,o mediante la llamada a s8. Eso nos proporciona dos contextos más, (s1, s6, s7) y(s1, s6, s8).Hay, por lo tanto, cinco contextos distintos en los que se puede llamar a t. Observe que todosestos contextos omiten los sitios de llamada recursivos, s3 y s5. Por ejemplo, el contexto (s1,s4) en realidad representa el conjunto infinito de cadenas de llamadas (s1, s3, (s5, s3)n, s4)para todas las n ¦ 0. ✷Ahora describiremos cómo derivamos el grafo de llamadas clonado. Cada método clonado seidentifica mediante el método en el programa M y un contexto C. Las aristas pueden derivarseagregando los correspondientes contextos a cada uno de las aristas en el grafo de llamadasoriginal. Recuerde que hay una arista en el grafo de llamadas original que enlaza al sitio dellamada S con el método M, si el predicado invoca(S, M) es verdadero. Para agregar contextosFigura 12.28: Funciones y sitios de llamada para un ejemplo abiertoMaq. Cap_12_AHO.indd 948 11/10/07 1:07:59 AMcon el fin de identificar los métodos en el grafo de llamadas clonado, podemos definir un predicado CSinvoca correspondiente, de tal forma que CSinvoca(S, C, M, D) sea verdadero si elsitio de llamada S en el contexto C llama al contexto D del método M.12.6.2 Agregar contexto a las reglas de DatalogPara buscar relaciones “apunta a” sensibles al contexto, sólo debemos aplicar el mismo análisistipo “apunta a” insensible al contexto al grafo de llamadas clonado. Como un método en elgrafo de llamadas clonado se representa mediante el método original y su contexto, revisamostodas las reglas de Datalog de manera acorde. Por cuestión de simplicidad, las reglas que semuestran a continuación no incluyen la restricción de tipos, y los símbolos − son todas lasvariables nuevas.Figura 12.29: Programa en Datalog para el análisis tipo “apunta a” sensible al contextoUn argumento adicional, que representa al contexto, se debe proporcionar al predicadoIDB apnt. apnt(V, C, H) dice que la variable V en el contexto C puede apuntar al objeto delmontículo H. Todas las reglas se explican por sí solas, tal vez con la excepción de la regla 5.Esta regla dice que si el sitio de llamada S en el contexto C llama al método M del contexto D,entonces los parámetros formales en el método M del contexto D pueden apuntar a los objetosa los que apuntan los parámetros actuales correspondientes en el contexto C.12.6.3 Observaciones adicionales acerca de la sensibilidadLo que hemos descrito es una formulación de sensibilidad al contexto, la cual ha demostradoser lo suficientemente práctica como para manejar muchos programas extensos reales en Java,apnthapntCSinvocaapntapntapntapntapntapntapntapntCSinvocahapnt12.6 Análisis de apuntadores sensible al contexto 949Maq. Cap_12_AHO.indd 949 11/10/07 1:08:00 AM950 Capítulo 12. Análisis interproceduralusando los trucos que se describen brevemente en la siguiente sección. Sin embargo, el algoritmo no puede aún manejar las aplicaciones en Java más extensas.Los objetos montículo en esta formulación se nombran en base a su sitio de llamada, perosin sensibilidad al contexto. Esa simplificación puede provocar problemas. Considere el modismo de fábrica de objetos (object-factory) en el que todos los objetos del mismo tipo se asignanpor la misma rutina. El esquema actual haría que todos los objetos de esa clase compartan elmismo nombre. En esencia, es muy simple manejar tales casos si ponemos en línea el códigode asignación. En general, lo ideal sería aumentar la sensibilidad al contexto al nombrar losobjetos. Aunque es fácil agregar la sensibilidad al contexto de los objetos a la formulación enDatalog, obtener el análisis para escalar a programas más extensos es otro asunto.Otra forma importante de sensibilidad es la sensibilidad a los objetos. Una técnica sensiblea los objetos puede diferenciar entre los métodos invocados sobre distintos objetos receptores.Considere el caso de un sitio de llamada en un contexto de llamada, en el que se descubre queuna variable apunta a dos objetos receptores distintos de la misma clase. Sus campos puedenapuntar a distintos objetos. Sin hacer diferencias entre los objetos, una copia entre los camposde la referencia al objeto this creará relaciones espurias, a menos que separemos el análisis deacuerdo a los objetos receptores. La sensibilidad a los objetos es más útil que la sensibilidad alcontexto para ciertos análisis.12.6.4 Ejercicios para la sección 12.6Figura 12.30: Código para los ejercicios 12.6.1 y 12.6.2Ejercicio 12.6.1: ¿Cuáles son todos los contextos que se diferenciarían si aplicamos los métodos de esta sección al código de la figura 12.30?Maq. Cap_12_AHO.indd 950 11/10/07 1:08:01 AMEjercicio 12.6.2: Realice un análisis sensible al contexto del código de la figura 12.30.Ejercicio 12.6.3: Extienda las reglas de Datalog de esta sección para incorporar la información de tipos y subtipos, siguiendo el método de la sección 12.5.12.7 Implementación en Datalog mediante BDDsLos diagramas de decisiones binarios (Binary Decision Diagrams, BDDs) son un método pararepresentar las funciones booleanas mediante grafos. Como hay 22n funciones booleanas de nvariables, ningún método de representación va a ser muy conciso en todas las funciones booleanas. Sin embargo, las funciones booleanas que aparecen en la práctica tienen en general mucharegularidad. Por ende, es común que podamos encontrar un BDD conciso para las funcionesque en realidad deseamos representar.Resulta que las funciones booleanas descritas por los programas en Datalog que hemosdesarrollado para analizar programas no son la excepción. Aunque los BDDs concisos querepresentan información sobre un programa se deben encontrar a menudo mediante el uso deheurística y las técnicas usadas en los paquetes de manipulación de BDDs comerciales, el método del BDD ha sido bastante exitoso en la práctica. En especial, supera a los métodos basadosen sistemas de administración de bases de datos convencionales, ya que éstos están diseñados para los patrones de datos más irregulares que aparecen en los datos comerciales típicos.La discusión acerca de la tecnología BDD que se ha desarrollado a través de los años es algoque va más allá de este libro. Aquí le presentaremos la notación BDD. Después vamos a sugerircómo podemos representar los datos relacionales como BDDs y cómo podríamos manipular losBDDs para reflejar las operaciones que se llevan a cabo para ejecutar programas en Datalog,mediante algoritmos como el Algoritmo 12.18. Por último, describiremos cómo representar losexponencialmente diversos contextos en los BDDs, la clave para el éxito del uso de BDDs enel análisis sensible al contexto.12.7.1 Diagramas de decisiones binariosUn BDD representa a una función booleana mediante un GDA con raíz. Cada uno de los nodosinteriores del GDA está etiquetado por una de las variables de la función representada. Al finalhay dos hojas, una etiquetada como 0 y la otra como 1. Cada nodo interior tiene dos aristashacia los hijos; a estas aristas se les conoce como “inferior” y “superior”. La arista inferior seasocia con el caso en donde la variable en el nodo tiene el valor 0, y el arista superior se asociacon el caso en donde la variable tiene el valor 1.Dada una asignación de verdad para las variables, podemos empezar en la raíz y en cadanodo, por decir un nodo etiquetado como x, seguir la arista inferior o superior, dependiendo desi el valor de verdad para x es 0 o 1, respectivamente. Si llegamos a la hoja etiquetada como 1,entonces la función representada es verdadera para esta asignación de verdad; en caso contrarioes falsa.Ejemplo 12.27: En la figura 12.31 podemos ver un BDD. En breve veremos la función querepresenta. Observe que hemos etiquetado todas las aristas “inferiores con 0 y todas las aristas“superiores” con 1. Considere la asignación de verdad para las variables wxyz que establece12.7 Implementación en Datalog mediante BDDs 951!!Maq. Cap_12_AHO.indd 951 11/10/07 1:08:02 AM952 Capítulo 12. Análisis interproceduralw = x = y = 0 y z = 1. Empezando en la raíz, como w = 0 tomamos la arista inferior, el cualnos lleva al extremo izquierdo de los nodos etiquetados como x. Ya que x = 0, seguimos de nuevola arista inferior desde este nodo, el cual nos lleva al extremo izquierdo de los nodos etiquetadoscomo y. Ya que y = 0, a continuación nos movemos al extremo izquierdo de los nodos etiquetados como z. Ahora, como z = 1, tomamos la arista superior y terminamos en la hoja etiquetadacomo 1. Nuestra conclusión es que la función es verdadera para esta asignación de verdad.Ahora, considere la asignación de verdad wxyz = 0101; es decir, w = y = 0 y x = z = 1. Denuevo empezamos en la raíz. Como w = 0 otra vez nos movemos al extremo izquierdo de losnodos etiquetados como x. Pero ahora, como x = 1, seguimos la arista superior que salta hastala hoja 0. Es decir, no sólo sabemos que la asignación de verdad 0101 hace a la función falsa,sino que como ni siquiera vimos y o z, cualquier asignación de verdad de la forma 01yz tambiénhará que la función tenga el valor de 0. Esta habilidad de “corto circuito” es una de las razonespor las que los BDDs tienden a ser representaciones concisas de las funciones booleanas. ✷En la figura 12.31 los nodos interiores se encuentran en rangos; cada rango tiene nodos conuna variable específica como etiqueta. Aunque no es un requerimiento absoluto, es convenientelimitarnos a los BDDs ordenados. En un BDD ordenado, hay un orden x1, x 2,..., x n para lasvariables, y cada vez que hay una arista de un nodo padre etiquetado como xi a un nodo hijoetiquetado como xj, entonces i < j. Más adelante veremos que es más fácil operar con los BDDsordenados, y de aquí en adelante supondremos que todos los BDDs están ordenados.Figura 12.31: Un diagrama de decisiones binariowx xy yz z0 1000000 011111 11Maq. Cap_12_AHO.indd 952 11/10/07 1:08:03 AMObserve también que los BDDs son, no árboles. Por lo general, las hojas 0 y 1 no sólotendrán muchos padres, sino que los nodos interiores también pueden tener varios padres. Porejemplo, el extremo derecho de los nodos etiquetados como z en la figura 12.31 tiene dos padres.Esta combinación de nodos que resultaría en la misma decisión es otra razón por las que losBDDs tienden a ser concisos.12.7.2 Transformaciones en BDDsEn la explicación anterior pasamos por alto dos simplificaciones en los BDDs que ayudan a quesean más concisos:1. Corto circuito: Si las aristas superior e inferior de un nodo N van al mismo nodo M,entonces podemos eliminar a N. Las aristas que entran a N van ahora a M.2. Mezcla de nodos: Si dos nodos N y M tienen aristas bajos que van al mismo nodo, ytambién tienen aristas superiores que van al mismo nodo, entonces podemos mezclar aN con M. Las aristas que entran a N o a M van al nodo mezclado.También es posible ejecutar estas transformaciones en dirección opuesta. En especial, podemosintroducir un nodo a lo largo de una arista de N a M. Ambas aristas que provienen del nodo introducido van a M, y la arista que proviene de N ahora va al nodo introducido. Sin embargo,observe que la variable asignada al nuevo nodo debe ser una de las que se encuentran entrelas variables de N y M en el orden. La figura 12.32 muestra las dos transformaciones en formaesquemática.(a) Corto circuito (b) Mezcla de nodosFigura 12.32: Transformaciones en BDDsxyzxzxx xyz yz12.7 Implementación en Datalog mediante BDDs 953Maq. Cap_12_AHO.indd 953 11/10/07 1:08:04 AM954 Capítulo 12. Análisis interprocedural12.7.3 Representación de las relaciones mediante BDDsLas relaciones con las que hemos estado tratando tienen componentes que se toman de los“dominios”. Un dominio para un componente de una relación es el conjunto de posibles valoresque pueden tener las tuplas en ese componente. Por ejemplo, la relación apnt(V, H) tiene eldominio de todas las variables del programa para su primer componente, y el dominio de todaslas instrucciones de creación de objetos para el segundo componente. Si un dominio tiene másde 2n− 1 posibles valores pero no más de 2n valores, entonces requiere n bits o variables booleanas para representar los valores en ese dominio.Una tupla en una relación puede entonces considerarse como una asignación de verdad alas variables que representan valores en los dominios para cada uno de los componentes de latupla. Podemos ver una relación como una función booleana que devuelve el valor verdaderopara todas y sólo esas asignaciones de verdad que representan a las tuplas en la relación. Unejemplo aclarará estas ideas.Ejemplo 12.28: Considere una relación r (A, B) tal que el domino de A y B está compuestopor {a, b, c, d}. Debemos codificar a mediante los bits 00, b mediante 01, c mediante 10, y dmediante 11. Hagamos que las tuplas de la relación r sean:Usemos las variables booleanas wx para codificar el primer componente (A) y las variables yzpara codificar el segundo componente (B). Entonces, la relación r se convierte en:Es decir, la relación r se ha convertido en la función booleana que es verdadera para las tresasignaciones de verdad wxyz = 0001, 0010 y 1110. Observe que estas tres secuencias de bits sonexactamente las que etiquetan los caminos de la raíz a la hoja 1 en la figura 12.31. Es decir, elBDD en esa figura representa a la relación r, si se utiliza la codificación antes descrita. ✷12.7.4 Operaciones relacionales como operaciones BDDAhora podemos ver cómo representar las relaciones como BDDs. Pero para implementar unalgoritmo como el 12.18 (evaluación incremental de programas en Datalog), tenemos que manipular los BDDs de una forma que refleje cómo se manipulan las mismas relaciones. He aquílas operaciones principales sobre las relaciones que debemos realizar:Maq. Cap_12_AHO.indd 954 11/10/07 1:08:05 AM1. Inicialización: Debemos crear un BDD que represente una sola tupla de una relación.Ensamblaremos éstos en BDDs que representen relaciones extensas, mediante el uso dela unión.2. Unión: Para usar la unión de las relaciones, tomamos el OR lógico de las funcionesbooleanas que representan a las relaciones. Esta operación se necesita no sólo para construir las relaciones iniciales, sino también para combinar los resultados de varias reglaspara el mismo predicado de encabezado, y para acumular nuevos hechos en el conjuntode hechos anteriores, como en el Algoritmo 12.18 incremental.3. Proyección: Al evaluar el cuerpo de una regla, debemos construir la relación de encabezado que está implícita mediante las tuplas verdaderas del cuerpo. En términos del BDDque representa a la relación, tenemos que eliminar los nodos que se etiquetan medianteesas variables booleanas que no representan componentes del encabezado. Tal vez también tengamos que renombrar las variables en el BDD para que correspondan con lasvariables booleanas para los componentes de la relación de encabezado.4. Combinación: Para encontrar las asignaciones de los valores a las variables que hacenque el cuerpo de una regla sea verdadero, debemos “combinar” las relaciones correspondientes a cada una de las submetas. Por ejemplo, suponga que tenemos dos submetasr (A, B) & s(B, C). La combinación de las relaciones para estas submetas es el conjuntode tripletas (a, b, c), de tal forma que (a, b) es una tupla en la relación para r, y (b, c)es una tupla en la relación para s. Más adelante veremos que, después de renombrar lasvariables booleanas en los BDDs de manera que los componentes para las dos Bs coincidan en los nombres de las variables, la operación sobre los BDDs es similar al ANDlógico, que a su vez es similar a la operación OR sobre los BDDs que implementa a lacombinación.BDDs para tuplas individualesSi deseamos inicializar una relación, debemos tener una forma de construir un BDD parala función que sea verdadera para una sola asignación de verdad. Suponga que las variablesbooleanas son x 1, x 2,..., x n, y la asignación de verdad es a1, a2, ... a n, en donde cada ai puedeser 0 o 1. El BDD tendrá un nodo Ni para cada x i. Si a i = 0, entonces la arista superior queproviene de Ni conduce hasta la hoja 0, y la arista inferior conduce hasta Ni+1, o hasta lahoja 1 si i = n. Si a i = 1, entonces hacemos lo mismo, pero se invierten las aristas superiore inferior.Esta estrategia proporciona un BDD que comprueba si cada xi tiene el valor correcto, parai = 1, 2,..., n. Tan pronto como encontramos un valor incorrecto, saltamos directamente a lahoja 0. Sólo terminamos en la hoja 1 si todas las variables tienen su valor correcto.Como ejemplo, vea la figura 12.33(b). Este BDD representa la función que es verdadera si,y sólo si x = y = 0; es decir, la asignación verdadera 00.12.7 Implementación en Datalog mediante BDDs 955Maq. Cap_12_AHO.indd 955 11/10/07 1:08:06 AM956 Capítulo 12. Análisis interproceduralUniónVamos a proporcionar con detalle un algoritmo para obtener el OR lógico de los BDDs; esdecir, la unión de las relaciones representadas por los BDDs.Algoritmo 12.29: Unión de BDDs.ENTRADA: Dos BDDs ordenados con el mismo conjunto de variables, en el mismo orden.SALIDA: Un BDD que representa la función que es el OR lógico de las dos funciones booleanasrepresentadas por los BDDs de entrada.MÉTODO: Vamos a describir un procedimiento recursivo para combinar dos BDDs. La inducción está en el tamaño del conjunto de variables que aparecen en los BDDs.BASE: Cero variables. Ambos BDDs deben ser hojas, etiquetados como 0 o 1. La salida es lahoja etiquetada como 1 si cualquiera de las entradas es 1, o la hoja etiquetada como 0 si ambasson 0.INDUCCIÓN: Suponga que hay k variables y1, y2,..., y k que se encuentran entre los dos BDDs.Haga lo siguiente:1. Si es necesario, use el corto circuito inverso para agregar una nueva raíz, de manera queambos BDDs tengan una raíz etiquetada como y1.2. Haga que las dos raíces sean N y M; haga que sus hijos inferiores sean N0 y M0, y quesus hijos superiores sean N1 y M1. Aplique este algoritmo en forma recursiva a los BDDscon raíz en N0 y M0. Además, aplique este algoritmo en forma recursiva a los BDDs conraíz en N1 y M1. El primero de estos BDDs representa a la función que es verdadera paratodas las asignaciones de verdad que tienen y1 = 0 y que hacen verdadero a uno o ambosde los BDDs dados. El segundo representa lo mismo para las asignaciones de verdad cony1 = 1.3. Cree un nodo etiquetado como y1. Su hijo inferior es la raíz del primer BDD construidoen forma recursiva, y su hijo superior es la raíz del segundo BDD.4. Mezcle las dos hojas etiquetadas como 0 y las dos hojas etiquetadas como 1 en el BDDcombinado que acaba de construir.5. Aplique la mezcla y el corto circuito en donde sea posible, para simplificar el BDD. ✷Ejemplo 12.30: En la figura 12.33(a) y (b) hay dos BDDs simples. El primero representa ala función x OR y, y el segundo representa a la función:NOT x AND NOT yMaq. Cap_12_AHO.indd 956 11/10/07 1:08:06 AMObserve que su OR lógico es la función 1 que siempre es verdadera. Para aplicar el Algoritmo12.29 a estos dos BDDs, consideramos los hijos inferiores de las dos raíces y los hijos superioresde las dos raíces; vamos a tomar estos últimos primero.El hijo superior de la raíz en la figura 12.33(a) es 1, y en la figura 12.33(b) es 0. Como estoshijos se encuentran al nivel de hoja, no tenemos que insertar nodos etiquetados como y a lolargo de cada arista, aunque el resultado sería el mismo si hubiéramos optado por hacerlo. Elcaso base para la unión de 0 y 1 es producir una hoja etiquetada como 1 que se convertirá enel hijo superior de la nueva raíz.Los hijos inferiores de las raíces en las figuras 12.33(a) y (b) están etiquetados como y, porlo que podemos calcular su BDD de unión en forma recursiva. Estos dos nodos tienen hijosinferiores etiquetados como 0 y 1, por lo que la combinación de sus hijos inferiores es la hojaetiquetada como 1. De igual forma, sus hijos superiores son 1 y 0, por lo que la combinaciónes de nuevo la hoja 1. Cuando agregamos una nueva raíz etiquetada como x, tenemos el BDDque se ve en la figura 12.33(c).No hemos terminado, ya que la figura 12.33(c) se puede simplificar. El nodo etiquetadocomo y tiene en ambos hijos el nodo 1, por lo que podemos eliminar el nodo y y hacer quela hoja 1 sea el hijo inferior de la raíz. Ahora, ambos hijos de la raíz son la hoja 1, por loque podemos eliminar la raíz. Es decir, el BDD más simple para la unión es la hoja 1, por sísola. ✷12.7.5 Uso de BDDs para el análisis tipo “apunta a”El proceso de lograr que funcione el análisis tipo “apunta a” insensible al contexto es ya depor sí poco trivial. El orden de las variables del BDD puede modificar en forma considerable eltamaño de la representación. Se requieren muchas consideraciones, así como la prueba y error,para obtener un orden que permita al análisis completarse con rapidez.Es aún más difícil lograr que se ejecute el análisis tipo “apunta a” sensible al contexto,debido a los exponencialmente diversos contextos en el programa. En especial, si asignamosFigura 12.33: Construcción del BDD para un OR lógicoxy0 1xy0 1xy0 1(a) (b) (c)12.7 Implementación en Datalog mediante BDDs 957Maq. Cap_12_AHO.indd 957 11/10/07 1:08:07 AM958 Capítulo 12. Análisis interproceduralnúmeros en forma arbitraria para representar contextos en un grafo de llamadas, no podemos manejar ni siquiera los programas pequeños en Java. Es importante que los contextos seenumeren de tal forma que la codificación binaria del análisis tipo “apunta a” pueda hacersemuy compacta. Dos contextos del mismo método con caminos de llamada similares compartenmuchas funcionalidades, por lo que es conveniente enumerar los n contextos de un método enforma consecutiva. De manera similar, como los pares de emisor-receptor para el mismo sitiode llamada comparten muchas similitudes, es conveniente enumerar los contextos de tal formaque la diferencia numérica entre cada par de emisor-receptor de un sitio de llamada sea siempreuna constante.Aun con un esquema de numeración inteligente para los contextos de llamada, sigue siendodifícil analizar con eficiencia los programas extensos en Java. El aprendizaje activo de las máquinas ha demostrado ser útil para derivar un orden de las variables lo bastante eficiente comopara manejar las aplicaciones extensas.12.7.6 Ejercicios para la sección 12.7Ejercicio 12.7.1: Usando la codificación de símbolos en el ejemplo 12.28, desarrolle un BDDque represente la relación consistente en los tuples (b, b), (c, a) y (b, a). Puede ordenar lasvariables booleanas en cualquier forma que le proporcione el BDD más conciso.Ejercicio 12.7.2: Como función de n, ¿cuántos nodos hay en el BDD más conciso que representa a la función or exclusivo en n variables? Es decir, la función es verdadera si un númeroimpar de las n variables son verdaderas y falsa si un número par de ellas son verdaderas.Ejercicio 12.7.3: Modifique el Algoritmo 12.29 de manera que produzca la intersección (ANDlógico) de dos BDDs.Ejercicio 12.7.4: Busque algoritmos para realizar las siguientes operaciones relacionales enlos BDDs ordenados que las representan:a) Proyecte algunas de las variables booleanas. Es decir, la función representada debe serverdadera para una asignación de verdad α si hay alguna asignación de verdad para lasvariables faltantes que, en conjunto con α, hacen que la función original sea verdadera.b) Combine dos relaciones r y s, combinando una tupla de r con uno de s cada vez que estastuplas concuerden con los atributos que r y s tienen en común. En realidad es suficienteconsiderar el caso en el que las relaciones sólo tienen dos componentes, y uno de cadarelación coincide; es decir, las relaciones son r (A, B) y s (B, C).12.8 Resumen del capítulo 12♦Análisis interprocedural: Un análisis de flujo de datos que rastrea la información a travésde los límites de los procedimientos se considera como interprocedural. Muchos análisis,como los análisis tipo “apunta a”, sólo pueden realizarse en un forma significativa si soninterprocedural.!!!Maq. Cap_12_AHO.indd 958 11/10/07 1:08:08 AM♦Sitios de llamada: Los programas llaman a los procedimientos en ciertos puntos conocidoscomo sitios de llamada. El procedimiento llamado en un sitio puede ser evidente, o puedeser ambiguo, en caso de que la llamada sea indirecta a través de un apuntador o unallamada de un método virtual que tiene varias implementaciones.♦Grafos de llamadas: Un grafo de llamadas para un programa es un grafo bipartido connodos para los sitios de llamada y nodos para los procedimientos. Una arista va de unnodo de sitio de llamada hacia un nodo de procedimiento, si es que se puede hacer unallamada a ese procedimiento en el sitio.♦Poner en línea: Siempre y cuando no haya recursividad en un programa, podemos enprincipio sustituir todas las llamadas a procedimientos por copias de su código, y usar elanálisis intraprocedural en el programa resultante. Este análisis es, en efecto, interprocedural.♦Sensibilidad al flujo y sensibilidad al contexto: Un análisis de flujo de datos que producehechos que dependen en la ubicación en el programa se considera como sensible al flujo.Si el análisis produce hechos que dependen del historial de las llamadas a procedimientos,se considera como sensible al contexto. Un análisis de flujo de datos puede ser sensible alflujo o al contexto, a ambos o a ninguno.♦Análisis sensible al contexto basado en la clonación: En principio, una vez que establecemos los distintos contextos en los que se puede llamar un procedimiento, podemos imaginar que hay un clon de cada procedimiento para cada contexto. De esa forma, un análisisinsensible al contexto sirve como un análisis sensible al contexto.♦Análisis sensible al contexto basado en el resumen: Otro método para el análisis interprocedural extiende la técnica de análisis basado en regiones que describimos para el análisisintraprocedural. Cada procedimiento tiene una función de transferencia y se trata comouna región, en cada lugar en donde se llama a ese procedimiento.♦Aplicaciones del análisis interprocedural: Una aplicación importante que requiere del análisis interprocedural es la detección de las vulnerabilidades del software. A menudo, éstasse caracterizan por hacer que un procedimiento lea datos de una fuente de entrada noconfiable, y que otro procedimiento utilice esos datos de forma que puedan explotarse.♦Datalog: El lenguaje Datalog es una notación simple para las reglas if-then que puedenusarse para describir los análisis de flujo de datos de alto nivel. Las colecciones de reglasde Datalog, o de programas en Datalog, pueden evaluarse mediante el uso de uno de varios algoritmos estándar.♦Reglas de Datalog: Una regla de Datalog consiste en un cuerpo (antecedente) y un encabezado (consecuente). El cuerpo es uno o más átomos, y el encabezado es un átomo. Losátomos son predicados que se aplican a los argumentos que son variables o constantes.Los átomos del cuerpo se conectan mediante un AND lógico, y un átomo en el cuerpopuede negarse.12.8 Resumen del capítulo 12 959Maq. Cap_12_AHO.indd 959 11/10/07 1:08:08 AM960 Capítulo 12. Análisis interprocedural♦Predicados IDB y EDB: A los predicados EDB en un programa en Datalog se lesproporcionan sus hechos verdaderos a-priori. En un análisis de flujo de datos, estospredicados corresponden a los hechos que pueden obtenerse del código que se está analizando. Los predicados IDB se definen mediante las mismas reglas y corresponden, enun análisis de flujo de datos, a la información que tratamos de extraer del código quese está analizando.♦Evaluación de los programas en Datalog: Para aplicar las reglas, sustituimos las constantes por variables que hacen que el cuerpo sea verdadero. Cada vez que hacemos esto,inferimos que el encabezado, con la misma sustitución para las variables, también esverdadero. Esta operación se repite hasta que no puedan inferirse más hechos.♦Evaluación incremental de los programas en Datalog: Puede obtenerse una mejora en laeficiencia al realizar una evaluación incremental. Realizamos una serie de rondas. En unaronda, consideramos sólo sustituciones de constantes por variables que hacen que, por lomenos, un átomo del cuerpo sea un hecho que acaba de descubrirse en la ronda anterior.♦Análisis de apuntadores en Java: Podemos modelar el análisis de apuntadores en Javamediante un framework en el cual hay variables de referencia que apuntan a objetos delmontículo, los cuales pueden tener campos que apuntan a otros objetos del montículo.Un análisis de apuntadores insensible puede escribirse como un programa en Datalogque infiere dos tipos de hechos: una variable puede apuntar a un objeto montículo, o uncampo de un objeto montículo puede apuntar a otro objeto del montículo.♦Información de tipos para mejorar el análisis de apuntadores: Podemos obtener un análisis de apuntadores más preciso si aprovechamos el hecho de que las variables de referenciasólo pueden apuntar a objetos del montículo que sean del mismo tipo que la variable o unsubtipo.♦Análisis de apuntadores interprocedural: Para realizar el análisis interprocedural, debemosagregar reglas que reflejen la forma en que los parámetros pasan y devuelven los valoresasignados a las variables. En esencia, estas reglas son las mismas que para copiar unavariable de referencia a otra.♦Descubrimiento del grafo de llamadas: Como Java tiene métodos virtuales, el análisisinterprocedural requiere que primero limitemos qué procedimientos pueden llamarse enun sitio de llamada dado. La principal forma de descubrir los límites sobre qué puedellamarse y en qué parte, es analizar los tipos de los objetos y aprovechar el hecho de queel método actual al que hace referencia una llamada a un método virtual debe pertenecera una clase apropiada.♦Análisis sensible al contexto: Cuando los procedimientos son recursivos, debemos condensar la información contenida en las cadenas de llamadas en un número finito de contextos.Una manera efectiva de hacer esto es retirar de la cadena de llamadas cualquier sitio dellamada en donde un procedimiento llame a otro procedimiento (tal vez a sí mismo) con elque sea mutuamente recursivo. Usando esta representación, podemos modificar las reglasMaq. Cap_12_AHO.indd 960 11/10/07 1:08:09 AMpara el análisis de apuntadores interprocedural, de manera que el contexto se acarree enpredicados; este método simula el análisis basado en la clonación.♦Diagramas de decisiones binarios: Los BDDs son una representación concisa de las funciones booleanas mediante GDAs con raíz. Los nodos interiores corresponden a las variables booleanas y tienen dos hijos, inferior (que representa el valor de verdad 0) y superior(que representa 1). Hay dos hojas etiquetadas como 0 y 1. Una asignación de verdadhace que la función representada sea verdadera si, y sólo si el camino de la raíz en la cualvamos al hijo inferior si la variable en un nodo es 0 y al hijo superior en caso contrario,conduce a la hoja 1.♦BDDs y relaciones: Un BDD puede servir como una representación concisa de uno de lospredicados en un programa en Datalog. Las constantes se codifican como asignaciones deverdad para una colección de variables booleanas, y la función representada por el BDDes verdadera si, y sólo si las variables booleanas representan un hecho verdadero para esepredicado.♦Implementación del análisis de flujo de datos mediante BDDs: Cualquier análisis de flujode datos que pueda expresarse como reglas de Datalog puede implementarse mediantemanipulaciones en los BDDs que representan a los predicados involucrados en esas reglas.A menudo, esta representación conduce a una implementación más eficiente del análisisde flujo de datos que cualquier otro método conocido.12.9 Referencias para el capítulo 12Podemos encontrar algunos de los conceptos básicos en un análisis interprocedural en [1, 6, 7y 21]. Callahan y sus colegas [11] describen un algoritmo de propagación de constantes interprocedural.Steensgaard [22] publicó el primer análisis de alias de apuntadores escalable. Es insensibleal contexto, insensible al flujo y está basado en equivalencias. Andersen [2] derivó una versióninsensible al contexto del análisis tipo “apunta a” basado en la inclusión. Después, Heintzey Tardieu [15] describieron un algoritmo eficiente para este análisis. Fähndrich, Rehof y Das[14] presentaron un análisis insensible al contexto, insensible al flujo y basado en equivalencias que escala a programas extensos como gcc. Ghiya y Hendren [13] diseñaron un algoritmotipo “apunta a” basado en la clonación, insensible al contexto, insensible al flujo y basado enla inclusión, que es notable entre los intentos anteriores de crear un análisis tipo “apunta a”insensible al contexto, basado en la inclusión.Los diagramas de decisiones binarios (BDDs) aparecieron por primera vez en Bryant [9].Berndl y sus colegas [4] lo usaron por primera vez para el análisis de flujo de datos. La aplicación de BDDs al análisis de apuntadores insensible se reporta por Zhu [25] y Berndl, junto consus colegas [8]. Whaley y Lam [24] describen el primer algoritmo sensible al contexto, insensible al flujo y basado en inclusiones, que ha demostrado su utilidad en aplicaciones reales. Eldocumento describe una herramienta llamada bddbddb que traduce en código BDD de formaautomática el análisis descrito en Datalog. Milanova, Rountev y Ryder [18] introdujeron lasensibilidad a los objetos.12.9 Referencias para el capítulo 12 961Maq. Cap_12_AHO.indd 961 11/10/07 1:08:09 AM962 Capítulo 12. Análisis interproceduralPara una explicación sobre Datalog, consulte a Ullman y Widom [23]. Consulte tambiéna Lam y sus colaboradores [16] para una explicación sobre la conexión del análisis de flujo dedatos a Datalog.El comprobador de código Metal se describe por Engler y sus colaboradores [12]; Bush, Pincus y Sielaff [10] crearon el comprobador PREfix. Ball y Rajamani [4] desarrollaron un motorde análisis de programas llamado SLAM, usando la comprobación de modelos y la ejecuciónsimbólica para simular todos los posibles comportamientos de un sistema. Ball y sus colaboradores [5] han creado una herramienta de análisis estático llamada SDV, basada en SLAM, parabuscar los errores de uso de la API en programas controladores de dispositivos en C, mediantela aplicación de BDDs a la comprobación de modelos.Livshits y Lam [17] describen cómo puede usarse el análisis tipo “apunta a” sensible al contexto para buscar vulnerabilidades de SQL en las aplicaciones Web en Java. Ruwase y Lam [20]describen cómo llevar el registro de las extensiones de arreglos e insertar comprobaciones dinámicas de límites en forma automática. Rinard y sus colaboradores [19] describen cómo extenderarreglos en forma dinámica, para dar cabida al contenido desbordado. Avots y sus colaboradores [3] extienden a C el análisis en Java tipo “apunta a” sensible al contexto, y muestran cómopuede usarse para reducir el costo de la detección dinámica de desbordamientos de búfer. 1. Allen, F. E., “Interprocedural data flow analysis”, Proc. IFIP Congress 1974, pp. 398-402, Holanda del Norte, Ámsterdam, 1974. 2. Andersen, L., Program Analysis and Specialization for the C Programming Language,tesis Ph. D., DIKU, Universidad de Copenhague, Dinamarca, 1994. 3. Avots, D., M. Dalton, V. B. Livshits y M. S. Lam, “Improving software security witha C pointer analysis”, ICSE 2005: Proc. 27th International Conference on SoftwareEngineering, pp. 332-341. 4. Ball, T. y S. K. Rajamani, “A symbolic model checker for boolean programs”, Proc.SPIN 2000 Workshop on Model Checking of Software, pp. 113-130. 5. Ball, T., E. Bounimova, B. Cook, V. Levin, J. Lichtenber, C. McGarvey, B. Ondrusek, S. Rajamani y A. Ustuner, “Thorough static analysis of device drivers”, EuroSys(2006), pp. 73-85. 6. Banning, J. P., “An efficient way to find the side effects of procedural calls and thealiases of variables”, Proc. Sixth Annual Symposium on Principles of ProgrammingLanguages (1979), pp. 29-41. 7. Barth, J. M., “A practical interprocedural data flow analysis algorithm”, Comm. ACM21:9 (1978), pp. 724-736. 8. Berndl, M., O. Lohtak, F. Qian, L. Hendren y N. Umanee, “Points-to analysis usingBDD’s”, Proc. ACM SIGPLAN 2003 Conference on Programming Language Design andImplementation, pp. 103-114. 9. Bryant, R. E., “Graph-based algorithms for Boolean function manipulation”, IEEETrans. on Computers C-35:8 (1986), pp. 677-691.Maq. Cap_12_AHO.indd 962 11/10/07 1:08:10 AM10. Bush, W. R., J. D. Pincus y D. J. Sielaff, “A static analyzer for finding dynamic programming errors”, Software-Practice and Experience, 30:7 (2000), pp. 775-802.11. Callahan, D., K. D. Cooper, K. Kennedy y L. Torczon, “Interprocedural constantpropagation”, Proc. SIGPLAN 1986 Symposium on Compiler Construction, SIGPLANNotices, 21:7 (1986), pp. 152-161.12. Engler, D., B. Chelf, A. Chou y S. Hallem, “Checking system rules using system-specific, programmer-written compiler extensions”, Proc. Sixth USENIX Conference onOperating Systems Design and Implementation (2000), pp. 1-16.13. Emami, M., R Ghiya y L. J. Hendren, “Context-sensitive interprocedural points-toanalysis in the presence of function pointers”, Proc. SIG-PLAN Conference on Programming Language Design and Implementation (1994), pp. 224-256.14. Fähndrich, M., J. Rehof y M. Das, “Scalable context-sensitive flow analysis using instantiation constraints”, Proc. SIGPLAN Conference on Programming Language Designand Implementation (2000), pp. 253-263.15. Heintze, N. y O. Tardieu, “Ultra-fast aliasing analysis using CLA: a million lines of Ccode in a second”, Proc of the SIGPLAN Conference on Programming Language Designand Implementation (2001), pp. 254-263.16. Lam, M. S., J. Whaley, V. B. Livshits, M. C. Martin, D. Avots, M. Carbin y C. Unkel,“Context-sensitive program analysis as database queries”, Proc 2005 ACM Symposiumon Principles of Database Systems, pp. 1-12.17. Livshits, V. B. y M. S. Lam, “Finding security vulnerabilities in Java applications usingstatic analysis”, Proc. 14th USENIX Security Symposium (2005), pp. 271-286.18. Milanova, A., A. Rountev y B. G. Ryder, “Parametrized object sensitivity for points-toand side-effect analyses for Java”, Proc. 2002 ACM SIGSOFT International Symposium on Software Testing and Analysis, pp. 1-11.19. Rinard, M., C. Cadar, D. Dumitran, D. Roy y T. Leu, “A dynamic technique for eliminating buffer overflow vulnerabilities (and other memory errors)”, Proc. 2004 AnnualComputer Security Applications Conference, pp. 82-90.20. Ruwase, O. y M. S. Lam, “A practical dynamic buffer overflow detector”, Proc. 11thAnnual Network and Distributed System Security Symposium (2004), pp. 159-169.21. Sharir, M. y A. Pnueli, “Two approaches to interprocedural data flow analysis”, enProgram Flow Analysis: Theory and Applications, de S. Muchnick y N. Jones (eds.),capítulo 7, pp. 189-234. Prentice-Hall, Upper Saddle River NJ, 1981.22. Steensgaard, B., “Points-to analysis in linear time”, Twenty-Third ACM Symposium onPrinciples of Programming Languages (1996).12.9 Referencias para el capítulo 12 963Maq. Cap_12_AHO.indd 963 11/10/07 1:08:10 AM964 Capítulo 12. Análisis interprocedural23. Ullman, J. D. y J. Widom, A First Course in Database Systems, Prentice-Hall, UpperSaddle River NJ, 2002.24. Whaley, J. y M. S. Lam, “Cloning-based context sensitive pointer alias analysis usingbinary decision diagrams”, Proc. ACM SIGPLAN 2004 Conference on ProgrammingLanguage Design and Implementation, pp. 131-144.25. Zhu, J., “Symbolic Pointer Analysis”, Proc. International Conference in Computer-Aided Design (2002), pp. 150-157.Maq. Cap_12_AHO.indd 964 11/10/07 1:08:11 AMEl front-end completo para el compilador de este apéndice se basa en el compilador simple delas secciones 2.5 a 2.8, descrito de manera informal. La diferencia principal del capítulo 2 esque el front-end genera código de salto para las expresiones booleanas, como en la sección 6.6.Empezamos con la sintaxis del lenguaje de código fuente, descrito por una gramática que debeadaptarse para el análisis sintáctico descendenteEl código en Java para el traductor consiste en cinco paquetes: main, analizadorLexico,simbolos, analizador e inter. El paquete inter contiene las clases para las construccionesdel lenguaje en la sintaxis abstracta. Como el código para el analizador sintáctico interactúacon el resto de los paquetes, se describirá al final. Cada paquete se almacena como un directorioseparado, con un archivo por clase.Al pasar por el analizador sintáctico, el programa fuente consiste en un flujo de tokens, porlo que la orientación a objetos tiene poco que ver con el código para el analizador sintáctico.Al salir de éste, el programa fuente consiste en un árbol sintáctico, en el cual las construccioneso nodos se implementan como objetos. Estos objetos se encargan de lo siguiente: construir unnodo del árbol sintáctico, comprobar tipos y generar código intermedio de tres direcciones (veael paquete inter).A.1 El lenguaje de código fuenteUn programa en el lenguaje consiste en un bloque con declaraciones e instrucciones opcionales.El token basico representa a los tipos básicos. programa → bloque bloque → { decls instrs } decls → decls decl | decl → tipo id ; tipo → tipo [ num ] | basico instrs → instrs stmt |Al tratar a las asignaciones como instrucciones, en vez de operadores dentro de expresiones,se simplifica la traducción.Apéndice AUn front-end completo965Maq. Apendices_AHO.indd 965 11/10/07 1:09:11 AM966 Apéndice A. Un front-end completo instr → loc = bool ; | if ( bool ) instr | if ( bool ) instr else instr | while ( bool ) instr | do instr while ( bool ) ; | break ; | bloque loc → loc [ bool ] | idLas producciones para las expresiones manejan la asociatividad y la precedencia de losoperadores. Utilizan un símbolo no terminal para cada nivel de precedencia, y un símbolo noterminal (factor) para las expresiones entre paréntesis, identificadores, referencias a arreglos yconstantes. bool → bool || comb | comb comb → comb && igualdad | igualdad igualdad → igualdad == rel | igualdad != rel | rel rel → expr < expr | expr <= expr | expr >= expr | expr > expr | expr expr → expr + term | expr − term | term term → term * unario | term / unario | unario unario → ! unario | - unario | factor factor → ( bool ) | loc | num | real | true | falseA.2 MainLa ejecución empieza en el método main en la clase Main. El método main crea un analizadorléxico (analizadorLexico) y un analizador sintáctico (analizador), y después llama al métodoprograma en el analizador sintáctico:Comparación entre orientación a objetos y orientación a fasesCon un método orientado a objetos, todo el código para una construcción se recolecta enla clase del constructor. De manera alternativa, con un método orientado a fases, el código se agrupa por fase, de manera que un procedimiento de comprobación de tipos tendríauna instrucción case para cada construcción, y un procedimiento de generación de códigotendría una instrucción case para cada construcción, y así sucesivamente.La concesión es que un método orientado a objetos facilita el proceso de modificaro agregar una construcción, como las instrucciones “for”, y el método orientado a fasesfacilita el proceso de modificar o agregar una fase, como una comprobación de tipos. Conlos objetos, para agregar una nueva construcción se escribe una clase autocontenida, perouna modificación a una fase, como el insertar código para las coerciones, requiere modificaciones a través de todas las clases afectadas. Con las fases, una nueva construcción puede provocar como resultado modificaciones a través de los procedimientos para las fases.Maq. Apendices_AHO.indd 966 11/10/07 1:09:14 AM 1) package main; // Archivo Main.java 2) import java.io.*; import analizadorLexico.*; import analizador.*; 3) public class Main { 4) public static void main(String[] args) throws IOException { 5) AnalizadorLexico lex = new AnalizadorLexico(); 6) Analizador analizar = new Analizador(lex); 7) analizar.programa(); 8) System.out.write(’\n’); 9) }10) }A.3 Analizador léxicoEl paquete analizadorLexico es una extensión del código para el analizador léxico de la sección 2.6.5. La clase Etiqueta define las constantes para los tokens: 1) package analizadorLexico; // Archivo Etiqueta.java 2) public class Etiqueta { 3) public final static int 4) AND = 256, BASIC = 257, BREAK = 258, DO = 259, ELSE = 260, 5) EQ = 261, FALSE = 262, GE = 263, ID = 264, IF = 265, 6) INDEX = 266, LE = 267, MINUS = 268, NE = 269, NUM = 270, 7) OR = 271, REAL = 272, TEMP = 273, TRUE = 274, WHILE = 275; 8) }Tres de las constantes, INDEX, MINUS y TEMP, no son tokens léxicos; se utilizarán en los árbolessintácticos.Las clases Token y Num son como en la sección 2.6.5, sólo que se agregó el método to−String: 1) package analizadorLexico; // Archivo Token.java 2) public class Token { 3) public final int etiqueta; 4) public Token(int t) {etiqueta = t;} 5) public String toString() {return "" + (char)etiqueta;} 6) } 1) package analizadorLexico; // Archivo Num.java 2) public class Num extends Token { 3) public final int valor; 4) public Num(int v) {super(Etiqueta.NUM); valor = v; } 5) public String toString() { return "" + valor; } 6) }La clase Palabra administra los lexemas para las palabras reservadas, identificadores ytokens compuestos como &&. También es útil para administrar la forma escrita de los operadores en el código intermedio, como el menos unario; por ejemplo, el texto fuente −2 tiene laforma intermedia minus 2. 1) package analizadorLexico; // Archivo Palabra.java 2) public class Palabra extends Token { 3) public String lexema = ""; 4) public Palabra(String s, int etiqueta) { super(etiqueta); lexema = s; } 5) public String toString() { return lexema;} 6) public static final Palabra 7) and = new Palabra( "&&", Etiqueta.AND ), or = new Palabra( "||", Etiqueta.OR ), 8) eq = new Palabra( "==", Etiqueta,EQ ), ne = new Palabra( "!=", Etiqueta.NE ),A.3 Analizador léxico 967Maq. Apendices_AHO.indd 967 11/10/07 1:09:15 AM968 Apéndice A. Un front-end completo 9) le = new Palabra( "<=", Etiqueta.LE ), ge = new Palabra( ">=", Etiqueta.GE ), 10) minus = new Palabra( "minus", Etiqueta.MINUS ), 11) True = new Palabra( "true", Etiqueta.TRUE ), 12) False = new Palabra( "false", Etiqueta.FALSE ), 13) temp = new Palabra( "t", Etiqueta.TEMP ); 14) }La clase Real es para los números de punto flotante: 1) package analizadorLexico; // Archivo Real.java 2) public class Real extends Token { 3) public final float valor; 4) public Real(float v) {super(Etiqueta.REAL); valor = v; } 5) public String toString() {return "" + valor; } 6) }El método principal en la clase AnalizadorLexico, la función explorar, reconoce números,identificadores y palabras reservadas, como vimos en la sección 2.6.5.Las líneas 9-13 en la clase AnalizadorLexico reservan las palabras clave seleccionadas.Las líneas 14-16 reservan los lexemas para los objetos definidos en cualquier otra parte. Losobjetos Palabra.True y Palabra.False se definen en la clase Palabra. Los objetos paralos tipos básicos int, char, bool y float se definen en la clase Tipo, una subclase de Palabra.La clase Tipo es del paquete simbolos. 1) package analizadorLexico; // Archivo AnalizadorLexico.java 2) import java.io.*; import java.util.*; import simbolos.*; 3) public class AnalizadorLexico { 4) public static int linea = 1; 5) char preanalisis = ’ ’; 6) Hashtable palabras = new Hashtable(); 7) void reservar(Palabra w) { palabras.put(w.lexema, w); } 8) public AnalizadorLexico() { 9) reservar( new Palabra("if", Etiqueta.IF) ); 10) reservar( new Palabra("else", Etiqueta.ELSE) ); 11) reservar( new Palabra("while", Etiqueta.WHILE) ); 12) reservar( new Palabra("do", Etiqueta.DO) ); 13) reservar( new Palabra("break", Etiqueta.BREAK) ); 14) reservar( Palabra.True ); reservar( Palabra.False ); 15) reservar( Tipo.Int ); reservar( Tipo.Char ); 16) reservar( Tipo.Bool ); reservar( Tipo.Float ); 17) }La función readch() (línea 18) se utiliza para leer el siguiente carácter de entrada y colocarlo en la variable preanalisis. El nombre readch se reutiliza o se sobrecarga (líneas 19-24)para ayudar a reconocer los tokens compuestos. Por ejemplo, una vez que se ve la entrada <,la llamada readch(’=’) lee el siguiente carácter y lo coloca en preanalisis, y comprueba sies =. 18) void readch() throws IOException { preanalisis = (char) System.in.read(); } 19) boolean readch(char c) throws IOException { 20) readch(); 21) if( preanalisis != c ) return false; 22) preanalisis = ’ ’; 23) return true; 24) }Maq. Apendices_AHO.indd 968 11/10/07 1:09:15 AMLa función explorar empieza ignorando el espacio en blanco (líneas 26-30). Reconoce lostokens compuestos como <= (líneas 31-44) y los números como 365 y 3.14 (líneas 45-58), antesde recolectar palabras (líneas 59-70). 25) public Token explorar() throws IOException { 26) for( ; ; readch() ) { 27) if( preanalisis == ’ ’ || preanalisis == ’\t’ ) continue; 28) else if( preanalisis == ’\n’ ) linea = linea + 1; 29) else break; 30) } 31) switch( preanalisis ) { 32) case ’&’: 33) if( readch(’&’) ) return Palabra.and; else return new Token(’&’); 34) case ’|’: 35) if( readch(’|’) ) return Palabra.or; else return new Token(’|’); 36) case ’=’: 37) if( readch(’=’) ) return Palabra.eq; else return new Token(’=’); 38) case ’!’: 39) if( readch(’=’) ) return Palabra.ne; else return new Token(’!’); 40) case ’<’: 41) if( readch(’=’) ) return Palabra.le; else return new Token(’<’); 42) case ’>’: 43) if( readch(’=’) ) return Palabra.ge; else return new Token(’>’); 44) } 45) if( Character.isDigit(preanalisis) ) { 46) int v = 0; 47) do { 48) v 0 10*v + Character.digit(preanalisis, 10); readch(); 49) } while ( Character.isDigit(preanalisis) ); 50) if( preanalisis != ’.’) return new Num(v); 51) float x = v; float d = 10; 52) for(;;) { 53) readch(); 54) if( ! Character.isDigit(preanalisis) ) break; 55) x = x + Character.digit(preanalisis, 10) / d; d = d*10; 56) } 57) return new Real(x); 58) } 59) if( Character.isLetter(preanalisis) ) { 60) StringBuffer b = new StringBuffer(); 61) do { 62) b.append(preanalisis); readch(); 63) } while( Character.isLetterOrDigit(preanalisis) ); 64) String s = b.toString(); 65) Palabra w = (Palabra)palabras.get(s); 66) if( w != null ) return w; 67) w = new Palabra(s, Etiqueta.ID); 68) palabras.put(s, w); 69) return w; 70) }Por último, los caracteres restantes se devuelven como tokens (líneas 71-72). 71) Token tok = new Token(preanalisis); preanalisis = ’ ’; 72) return tok; 73) } 74) }A.3 Analizador léxico 969Maq. Apendices_AHO.indd 969 11/10/07 1:09:16 AM970 Apéndice A. Un front-end completoA.4 Tablas de símbolos y tiposEl paquete simbolos implementa a las tablas de símbolos y los tipos.En esencia, la clase Ent permanece sin cambios, como en la figura 2.37. Mientras que laclase AnalizadorLexico asigna cadenas a palabras, la clase Ent asigna tokens de palabras aobjetos de la clase Id, la cual se define en el paquete inter, junto con las clases para expresiones e instrucciones. 1) package simbolos; // Archivo Ent.java 2) import java.util.*; import analizadorLexico.*; import inter.*; 3) public class Ent { 4) private Hashtable tabla; 5) protected Ent ant; 6) public Ent(Ent n) { tabla = new Hashtable(); ant = n; } 7) public void put(Token w, Id i) { tabla.put(w, i); } 8) public Id get(Token w) { 9) for( Env e = this; e != null; e = e.ant ) { 10) Id encontro = (Id)(e.tabla.get(w)); 11) if( encontro != null ) return encontro; 12) } 13) return null; 14) } 15) }Definimos la clase Tipo como una subclase de Palabra, ya que los nombres de los tiposbásicos como int son sólo palabras reservadas, que el analizador léxico asigna de los lexemasa los objetos apropiados. Los objetos para los tipos básicos son Tipo.Int, Tipo.Float, Tipo.Char y Tipo.Bool (líneas 7-10). En todos ellos el campo heredado etiqueta se establece enEtiqueta.BASIC, por lo que el analizador sintáctico las trata a todas por igual. 1) package simbolos; // Archivo Tipo.java 2) import analizadorLexico.*; 3) public class Tipo extends Palabra { 4) public int anchura = 0; // anchura se usa para asignacion de almacenamiento 5) public Tipo(String s, int etiqueta, int w) { super(s, etiqueta); anchura = w; } 6) public static final Tipo 7) Int = new Tipo( "int", Etiqueta.BASIC, 4 ), 8) Float = new Tipo( "float", Etiqueta.BASIC, 8 ), 9) Char = new Tipo( "char", Etiqueta.BASIC, 1 ), 10) Bool = new Tipo( "bool", Etiqueta.BASIC, 1 );Las funciones numerico (líneas 11-14) y max (líneas 15-20) son útiles para las conversionesde tipos. 11) public static bolean numerico(Tipo p) { 12) if (p == Tipo.Char || p == Tipo.Int || p == Tipo.Float) return true; 13) else return false; 14) } 15) public static Tipo max(Tipo p1, Tipo p2) { 16) if ( ! numerico(p1) || ! numerico(p2) ) return null; 17) else if (p1 == Tipo.Float || p2 == Tipo.Float ) return Tipo.Float; 18) else if (p1 == Tipo.Int || p2 == Tipo.Int ) return Tipo.Int; 19) else return Tipo.Char; 20) } 21) }Maq. Apendices_AHO.indd 970 11/10/07 1:09:17 AMA.5 Código intermedio para las expresiones 971Se permiten las conversiones entre los tipos “numéricos” Tipo.Char, Tipo.Int y Tipo.Float.Cuando se aplica un operador aritmético a dos tipos numéricos, el resultado es el “máximo”(máx) de los dos tipos.Los arreglos son el único tipo construido en el lenguaje fuente. La llamada a super en lalínea 7 establece el campo anchura, que es esencial para los cálculos de direcciones. Tambiénestablece lexema y tok a valores predeterminados que no se utilizan. 1) package symbols; // Archivo Arreglo.java 2) import analizadorLexico.*; 3) public class Arreglo extends Tipo { 4) public Tipo de; // arreglo *de* tipo 5) public int tamanio = 1; // numero de elementos 6) public Arreglo(int tm, Tipo p) { 7) super("[]", Etiqueta.INDEX, tm*p.anchura); tamanio = tm; de = p; 8) } 9) public String toString() { return "[" + tamanio + "] " + de.toString(); } 10) }A.5 Código intermedio para las expresionesEl paquete inter contiene la jerarquía de clases Nodo, la cual tiene dos subclases: Expr para losnodos de expresiones e Instr para los nodos de instrucciones. Esta sección presenta a Expr ysus subclases. Algunos de los métodos en Expr tratan con valores booleanos y código de salto;los veremos en la sección A.6, junto con el resto de las subclases de Expr.Los nodos en el árbol sintáctico se implementan como objetos de la clase Nodo. Para reportar errores, el campo linealex (línea 4, archivo Nodo.java) guarda el número de línea decódigo fuente de la construcción en este nodo. Las líneas 7-10 se utilizan para emitir el códigode tres direcciones. 1) package inter; // Archivo Nodo.java 2) import analizadorLexico.*; 3) public class Nodo { 4) int linealex = 0; 5) Nodo() { linealex = AnalizadorLexico.linea; } 6) void error(String s) {throw new Error("cerca de la linea "+lineales+": "+s); } 7) static int etiquetas = 0; 8) public int nuevaEtiqueta() { return ++etiquetas; } 9) public void emitirEtiqueta(int i) { System.out.print("L" + i + ":"); } 10) public void emitir(String s) { System.out.println("\t" + s); } 11) }Las construcciones de expresiones se implementan mediante subclases de Expr. La claseExpr tiene los campos op y tipo (líneas 4-5, archivo Expr.java), que representan al operador yel tipo, respectivamente, en un nodo. 1) package inter; // Archivo Expr.java 2) import analizadorLexico.*; import simbolos.*; 3) public class Expr extends Nodo { 4) public Token op; 5) public Tipo tipo; 6) Expr(Token tok, Tipo p) { op = tok; tipo = p; }Maq. Apendices_AHO.indd 971 11/10/07 1:09:17 AM972 Apéndice A. Un front-end completoEl método gen (línea 7) devuelve un “término” que puede caber del lado derecho de unainstrucción de tres direcciones. Dada la expresión E = E1 + E2, el método gen devuelve un término x 1+ x 2, en donde x 1 y x 2 son direcciones para los valores de E1 y E2, respectivamente. Elvalor de retorno this es apropiado, si este objeto es una dirección; por lo general, las subclasesde Expr vuelven a implementar a gen.El método reducir (línea 8) calcula o “reduce” una expresión hasta una sola dirección; esdecir, devuelve una constante, un identificador o un nombre temporal. Dada la expresión E, elmétodo reduce devuelve un nombre temporal t que contiene el valor de E. De nuevo, this esun valor de retorno apropiado, si este objeto es una dirección.Aplazaremos la explicación sobre los métodos salto y emitirsaltos (líneas 9-18) hasta lasección A.6; éstos generan el código de salto para las expresiones booleanas. 7) public Expr gen() { return this; } 8) public Expr reducir() { return this; } 9) public void salto(int t, int f) { emitirsaltos(toString(), t, f); } 10) public void emitirsaltos(String prueba, int t, int f) { 11) if( t != 0 && f != 0 ) { 12) emitir("if " + prueba + " goto L" + t); 13) emitir("goto L" + f); 14) } 15) else if( t != 0 ) emitir("if " + prueba + " goto L" + t); 16) else if( f != 0 ) emitir("iffalse " + prueba + " goto L" + f); 17) else ; // nada, ya que tanto t como f pasan directo 18) } 19) public String toString() { return op.toString(); } 20) }La clase Id hereda las implementaciones predeterminadas de gen y reducir en la claseExpr, ya que un identificador es una dirección. 1) package inter; // Archivo Id.java 2) import analizadorLexico.*; import simbolos.*; 3) public class Id extends Expr { 4) public int desplazamiento; // direccion relativa 5) public Id(Palabra id, Tipo p, int b) { super(id, p); desplazamiento = b; } 6) }El nodo para un identificador de la clase Id es una hoja. La llamada super(id, p) (línea 5,archivo Id.java) almacena a id y p en los campos heredados op y tipo, respectivamente. Elcampo desplazamiento (línea 4) contiene la dirección relativa de este identificador.La clase Op proporciona una implementación de reducir (líneas 5-10, archivo Op.java) queheredan las subclases Arit para los operadores aritméticos, Unario para los operadores unarios, y Acceso para los accesos a arreglos. En cada caso, reducir llama a gen para generar untérmino, emite una instrucción para asignar el término a un nuevo nombre temporal y devuelveese temporal. 1) package inter; // Archivo Op.java 2) import analizadorLexico; import simbolos.*; 3) public class Op extends Expr { 4) public Op(Token tok, Tipo p) { super(tok, p); } 5) public Expr reducir() { 6) Expr x = gen();Maq. Apendices_AHO.indd 972 11/10/07 1:09:18 AM 7) Temp t = new Temp(tipo); 8) emitir( t.toString() + " = " + x.toString() ); 9) return t; 10) } 11) }La clase Arit implementa los operadores binarios como + y *. El constructor Arit empiezapor llamar a super(tok,null) (línea 6), en donde tok es un token que representa al operadory null es un receptáculo para el tipo. El tipo se determina en la línea 7 mediante el uso deTipo.max, que comprueba si los dos operandos pueden forzarse a un tipo numérico común;el código para Tipo.max está en la sección A.4. Si pueden forzar, tipo se establece al tipo delresultado; en caso contrario se reporta un error (línea 8). Este compilador simple compruebalos tipos, pero no inserta las conversiones de tipos. 1) package inter; // Archivo Arit.java 2) import analizadorLexico.*; import simbolos.*; 3) public class Arit extends Op { 4) public Expr expr1, expr2; 5) public Arit(Token tok, Expr x1, Expr x2) { 6) super(tok, null); expr1 = x1; expr2 = x2; 7) tipo = Tipo.max(expr1.tipo, expr2.tipo); 8) if (tipo == null ) error("error de tipo"); 9) } 10) public Expr gen() { 11) return new Arit(op, expr1.reducir(), expr2.reducir()); 12) } 13) public String toString() { 14) return expr1.toString()+" "+op.toString()+" "+expr2.toString(); 15) } 16) }El método gen construye el lado derecho de una instrucción de tres direcciones, al reducir lasubexpresión a direcciones y aplicar el operador a las direcciones (línea 11, archivo Arit.java).Por ejemplo, suponga que gen se llama en la raíz para a+b*c. Las llamadas a reducir devuelven a como la dirección para la subexpresión a y un nombre temporal t como la dirección parab*c. Mientras tanto, reducir emite la instrucción t=b*c. El método gen devuelve un nuevonodo Arit, con el operador * y las direcciones a y t como operandos.1Vale la pena recalcar que los nombres temporales tienen tipo, junto con todas las demásexpresiones. Por lo tanto, el constructor Temp se llama con el tipo como un parámetro (línea 6,archivo Temp.java).2 1) package inter; // Archivo Temp.java 2) import analizadorLexico.*; import simbolos.*; 3) public class Temp extends Expr {A.5 Código intermedio para las expresiones 9731 Para reportar errores, el campo linealex en la clase Nodo registra el número de línea en el que se encontródurante el análisis léxico actual cuando se construye un nodo. Dejamos al lector el trabajo de rastrear los números delínea cuando se construyen nuevos nodos durante la generación de código intermedio. 2 Un método alternativo podría ser que el constructor recibiera un nodo de expresión como un parámetro, paraque pudiera copiar el tipo y la posición léxica del nodo de expresión.Maq. Apendices_AHO.indd 973 11/10/07 1:09:18 AM974 Apéndice A. Un front-end completo 4) static int conteo = 0; 5) int numero = 0; 6) public Temp(Tipo p) { super(Palabra.temp, p); numero = ++conteo; } 7) public String toString() { return "t" + numero; } 8) }La clase Unario es la contraparte de la clase Arit, con un operando: 1) package inter; // Archivo Unario.java 2) import analizadorLexico.*; import simbolos.*; 3) public class Unario extends Op { 4) public Expr expr; 5) public Unario(Token tok, Expr x) { // maneja el menos, para ! vea Not 6) super(tok, null); expr = x; 7) tipo = Tipo.max(Tipo.Int, expr.tipo); 8) if (tipo == null ) error("error de tipo"); 9) } 10) public Expr gen() { return new Unario(op, expr.reducir()); } 11) public String toString() { return op.toString()+" "+expr.toString(); } 12) }A.6 Código de salto para las expresiones booleanasEl código de salto para una expresión booleana B se genera mediante el método salto, el cualrecibe dos etiquetas t y f como parámetros, a los cuales se les conoce como las salidas verdadera y falsa de B, respectivamente. El código contiene un salto a t si B se evalúa como verdadero,y un salto a f si B se evalúa como falso. Por convención, la etiqueta especial 0 significa que elcontrol pasa a través de B hacia la siguiente instrucción después del código para B.Empezamos con la clase Constante. El constructor Constante en la línea 4 recibe un tokentok y un tipo p como parámetros. Construye una hoja en el árbol sintáctico con la etiqueta toky el tipo p. Por conveniencia, el constructor Constante está sobrecargado (línea 5) para crearun objeto constante a partir de un entero. 1) package inter; // Archivo Constante.java 2) import analizadorLexico.*; import simbolos.*; 3) public class Constante extends Expr { 4) public Constante(Token tok, Tipo p) { super(tok, p); } 5) public Constante(int i) { super(new Num(i), Tipo.Int); } 6) public static final Constante 7) True = new Constante(Palabra.True, Tipo.Bool), 8) False = new Constante(Palabra.False, Tipo.Bool); 9) public void salto(int t, int f) { 10) if ( this == True && t != 0 ) emitir("goto L" + t); 11) else if ( this == False && f != 0) emitir("goto L" + f); 12) } 13) }El método salto (líneas 9-12, archivo Constante.java) recibe dos parámetros, las etiquetast y f. Si esta constante es el objeto estático True (definido en la línea 7) y t no es la etiquetaespecial 0, entonces se genera un salto a t. En caso contrario, si éste es el objeto False (definido en la línea 8) y f es distinta de cero, entonces se genera un salto a f.Maq. Apendices_AHO.indd 974 11/10/07 1:09:19 AMA.6 Código de salto para las expresiones booleanas 975La clase Logica proporciona cierta funcionalidad para las clases Or, And y Not. Los camposx y y (línea 4) corresponden a los operandos de un operador lógico. (Aunque la clase Not implementa a un operador unario, por conveniencia, es una subclase de Logica.) El constructorLogica(tok, a, b) (líneas 5-10) construye un nodo sintáctico con el operador tok y los operandos a y b. Al hacer esto, utiliza la función comprobar para asegurar que tanto a como bsean booleanas. Hablaremos sobre el método gen al final de esta sección. 1) package inter; // Archivo Logica.java 2) import analizadorLexico.*; import simbolos.*; 3) public class Logica extends Expr { 4) public Expr expr1, expr2; 5) Logica(Token tok, Expr x1, Expr x2) { 6) super(tok, null); 7) expr1 = x1; expr2 = x2; 8) tipo = comprobar(expr1.tipo, expr2.tipo); 9) if (tipo == null ) error("error de tipo"); 10) } 11) public Tipo comprobar(Tipo p1, Tipo p2) { 12) if ( p1 == Tipo.Bool && p2 == Tipo.Bool ) return Tipo.Bool; 13) else return null; 14) } 15) public Expr gen() { 16) int f = nuevaEtiqueta(); int a = nuevaEtiqueta(); 17) Temp temp = new Temp(tipo); 18) this.salto(0,f); 19) emitir(temp.toString() + " = true"); 20) emitir("goto L" + a); 21) emitirEtiqueta(f); emitir(temp.toString() + " = false"); 22) emitirEtiqueta(a); 23) return temp; 24) } 25) public String toString() { 26) return expr1.toString()+" "+op.toString()+" "+expr2.toString(); 27) } 28) }En la clase Or, el método salto (líneas 5-10) genera el código de salto para una expresiónbooleana B = B1||B2. Por el momento, suponga que ni la salida verdadera t ni la salida falsa fde B son la etiqueta especial 0. Como B es verdadera si B1 es verdadera, la salida verdadera deB1 debe ser t y la salida falsa corresponde a la primera instrucción de B2. Las salidas verdaderay falsa de B2 son iguales que las de B. 1) package inter; // Archivo Or.java 2) import analizadorLexico.*; import simbolos.*; 3) public class Or extends Logica { 4) public Or(Token tok, Expr x1, Expr x2) { super(tok, x1, x2); } 5) public void salto(int t, int f) { 6) int etiqueta = t != 0 ? t : nuevaEtiqueta(); 7) expr1.salto(etiqueta, 0); 8) expr2.salto(t,f); 9) if( t == 0 ) emitirEtiqueta(etiqueta); 10) } 11) }Maq. Apendices_AHO.indd 975 11/10/07 1:09:19 AM976 Apéndice A. Un front-end completoEn el caso general, t, la salida verdadera de B, puede ser la etiqueta especial 0. La variableetiqueta (línea 6, archivo Or.java) asegura que la salida verdadera de B1 se establezca demanera apropiada al final del código para B. Si t es 0, entonces etiqueta se establece a unanueva etiqueta que se emite después de la generación de código para B1 y B2.El código para la clase And es similar al código para Or. 1) package inter; // Archivo And.java 2) import analizadorLexico.*; import simbolos.*; 3) public class And extends Logica { 4) public And(Token tok, Expr x1, Expr x2) { super(tok, x1, x2); } 5) public void salto(int t, int f) { 6) int etiqueta = f != 0 ? f : nuevaEtiqueta(); 7) expr1.salto(etiqueta, 0); 8) expr2.salto(t,f); 9) if( f == 0 ) emitirEtiqueta(etiqueta); 10) } 11) }La clase Not tiene tanto en común con los demás operadores booleanos, que la haremosuna subclase de Logica, aun cuando Not implementa a un operador unario. La superclaseespera dos operandos, por lo que b aparece dos veces en la llamada a super en la línea 4. Sólose utiliza y (declarada en la línea 4, archivo Logica.java) en los métodos en las líneas 5-6. Enla línea 5, el método salto simplemente llama a y.salto con las salidas verdadera y falsainvertidas. 1) package inter; // Archivo Not.java 2) import analizadorLexico.*; import simbolos.*; 3) public class Not extends Logica { 4) public Not(Token tok, Expr x2) { super(tok, x2, x2); } 5) public void salto(int t, int f) { expr2.salto(f, t); } 6) public String toString() { return op.toString()+" "+expr2.toString(); } 7) }La clase Rel implementa los operadores <, <=, ==, !=, >= y >. La función comprobar (líneas5-9) comprueba que los dos operandos tienen el mismo tipo y no son arreglos. Por cuestión desimplicidad, no se permiten las coerciones. 1) package inter; // Archivo Rel.java 2) import analizadorLexico.*; import simbolos.*; 3) public class Rel extends Logica { 4) public Rel(Token tok, Expr x1, Expr x2) { super(tok, x1, x2); } 5) public Tipo comprobar(Tipo p1, Tipo p2) { 6) if ( p1 instanceof Arreglo || p2 instanceof Arreglo ) return null; 7) else if( p1 == p2 ) return Tipo.Bool; 8) else return null; 9) } 10) public void salto(int t, int f) { 11) Expr a = expr1.reducir(); 12) Expr b = expr2.reducir(); 13) String prueba = a.toString() + " " + op.toString() + " " + b.toString(); 14) emitirsaltos(prueba, t, f); 15) } 16) }Maq. Apendices_AHO.indd 976 11/10/07 1:09:20 AMEl método salto (líneas 10-15, archivo Rel.java) empieza generando código para las subexpresiones x y y (líneas 11-12). Después llama al método emitirSaltos definido en las líneas10-18, archivo Expr.java, en la sección A.5. Si ni t ni f son la etiqueta especial 0, entoncesemitirSaltos ejecuta lo siguiente: 12) emitir("if " + prueba + " goto L" + t); // Archivo Expr.java 13) emitir("goto L" + f);A lo más se genera una instrucción si t o f son la etiqueta especial 0 (de nuevo, del archivoExpr.java): 15) else if( t != 0 ) emitir("if " + prueba + " goto L" + t); 16) else if( f != 0 ) emitir("iffalse " + prueba + " goto L" + f); 17) else ; // nada, ya que tanto t como f pasan directoPara otro uso de emitirSaltos, considere el código para la clase Acceso. El lenguaje fuentepermite asignar valores booleanos a identificadores y elementos de arreglos, por lo que unaexpresión booleana puede ser un acceso a un arreglo. La clase Acceso tiene el método gen paragenerar código “normal” y el método salto para el código de salto. El método salto (línea11) llama a emitirSaltos después de reducir este acceso a arreglo a un nombre temporal.El constructor (líneas 6-9) se llama con un arreglo a aplanado, un índice i, y el tipo p de unelemento en el arreglo aplanado. La comprobación de tipos se realiza durante el cálculo de lasdirecciones del arreglo. 1) package inter; // Archivo Acceso.java 2) import analizadorLexico.*; import simbolos.*; 3) public class Acceso extends Op { 4) public Id arreglo; 5) public Expr indice; 6) public Acceso(Id a, Expr i, Tipo p) { // p es el tipo de elemento despues de 7) super(new Palabra("[]", Etiqueta.INDEX), p);// aplanar el arreglo 8) arreglo = a; indice = i; 9) } 10) public Expr gen() { return new Acceso(arreglo, indice.reducir(), tipo); } 11) public void salto(int t,int f) { emitirSaltos(reducir().toString(),t,f); } 12) public String toString() { 13) return arreglo.toString() + " [ " + indice.toString() + " ]"; 14) } 15) }El código de salto también puede usarse para devolver un valor booleano. La clase Logica,que se muestra antes en esta sección, tiene un método gen (líneas 15-24) que devuelve un nombre temporal temp, cuyo valor se determina mediante el flujo de control a través del código desalto para esta expresión. En la salida verdadera de esta expresión booleana, a temp se le asignatrue; en la salida falsa, a temp se le asigna false. El nombre temporal se declara en la línea 17.El código de salto para esta expresión se genera en la línea 18, en donde la salida verdadera esla siguiente instrucción y la falsa es una nueva etiqueta f. La siguiente instrucción asigna truea temp (línea 19), seguida de un salto a una nueva etiqueta a (línea 20). El código en la línea 21emite la etiqueta f y una instrucción que asigna false a temp. El fragmento de código terminacon la etiqueta a, que se genera en la línea 22. Por último, gen devuelve temp (línea 23).A.6 Código de salto para las expresiones booleanas 977Maq. Apendices_AHO.indd 977 11/10/07 1:09:20 AM978 Apéndice A. Un front-end completoA.7 Código intermedio para las instruccionesCada construcción de instrucción se implementa mediante una subclase de Instr. Los campospara los componentes de una construcción están en la subclase relevante; por ejemplo, la claseWhile tiene campos para una expresión de prueba y una subinstrucción, como veremos másadelante.Las líneas 3-4 en el siguiente código para la clase Instr tratan con la construcción de árboles sintácticos. El constructor Instr() no hace nada, ya que el trabajo se realiza en las subclases. El objeto estático Instr.Null (línea 4) representa una secuencia vacía de instrucciones. 1) package inter; // Archivo Instr.java 2) public class Instr extends Nodo { 3) public Instr() { } 4) public static Instr Null = new Instr(); 5) public void gen(int b, int a) {} // se llama con etiquetas inicio y despues 6) int despues = 0; // almacena la etiqueta despues 7) public static Instr Circundante = Instr.Null; // se utiliza para instrs break 8) }Las líneas 5-7 tratan con la generación de código de tres direcciones. El método gen sellama con dos etiquetas b y a, en donde b marca el inicio del código para esta instrucción y amarca la primera instrucción después del código para esta instrucción. El método gen (línea 5)es un receptáculo para los métodos gen en las subclases. Las subclases While y Do guardansu etiqueta a en el campo despues (linea 6), por lo que cualquier instrucción break encerradapuede usarla para salir de su construcción circundante. El objeto Instr.Circundante se utiliza durante el análisis sintáctico para llevar el registro de la construcción circundante. Paraun lenguaje fuente con instrucciones “continue”, podemos usar el mismo método para llevar elregistro de la construcción circundante para una instrucción “continue”.El constructor para la clase If construye un nodo para una instrucción if (E) S. Los campos expr e instr contienen los nodos para E y S, respectivamente. Observe que expr en letrasminúsculas nombra a un campo de la clase Expr; de manera similar, instr nombra a un campode la clase Instr. 1) package inter; // Archivo If.java 2) import simbolos.*; 3) public class If extends Instr { 4) Expr expr; Instr instr; 5) public If(Expr x, Instr s) { 6) expr = x; instr = s; 7) if( expr.tipo != Tipo.Bool ) expr.error("se requiere booleano en if"); 8) } 9) public void gen(int b, int a) { 10) int etiqueta = nuevaEtiqueta(); // etiqueta para el codigo de instr 11) expr.salto(0, a); // pasa por alto en true, va hacia a en false 12) emitirEtiqueta(etiqueta); instr.gen(etiqueta, a); 13) } 14) }El código para un objeto If consiste en el código de salto para expr, seguido del códigopara instr. Como vimos en la sección A.6, la llamada expr.salto(0,f) en la línea 11 especiMaq. Apendices_AHO.indd 978 11/10/07 1:09:21 AMfica que el control debe pasar por alto el código para expr si ésta se evalúa como verdadera, ydebe fluir hacia la etiqueta a en caso contrario.La implementación de la clase Else, que maneja instrucciones condicionales con partes delelse, es similar a la de la clase If: 1) package inter; // Archivo Else.java 2) import simbolos.*; 3) public class Else extends Instr { 4) Expr, expr; Instr instr1, instr2; 5) public Else(Expr x, Instr s1, Instr s2) { 6) expr = x; instr1 = s1; instr2 = s2; 7) if( expr.tipo != Tipo.Bool ) expr.error("se requiere booleano en if"); 8) } 9) public void gen(int b, int a) { 10) int etiqueta1 = nuevaEtiqueta(); // etiqueta1 para instr1 11) int etiqueta2 = nuevaEtiqueta(); // etiqueta2 para instr2 12) expr.salto(0,etiqueta2); // pasa hacia instr1 en true 13) emitirEtiqueta(etiqueta1); instr1.gen(etiqueta1, a); emitir("goto L" + a); 14) emitirEtiqueta(etiqueta2); instr2.gen(etiqueta2, a); 15) } 16) }La construcción de un objeto While se divide entre el constructor While(), el cual crea unnodo con hijos nulos (línea 5) y una función de inicialización inic(x, s), la cual establece elhijo expr a x y el hijo instr a s (líneas 6-9). La función gen(b, a) para generar código de tresdirecciones (línea 10-16) está a la par con la correspondiente función gen() en la clase If. Ladiferencia es que la etiqueta a se guarda en el campo despues (línea 11) y que el código parainstr va seguido de un salto a b (línea 15) para la siguiente iteración del ciclo while. 1) package inter; // Archivo While.java 2) import simbolos.*; 3) public class While extends Instr { 4) Expr expr; Instr instr; 5) public While() { expr = null; instr = null; } 6) public void inic(Expr x, Instr s) { 7) expr = x; instr = s; 8) if( expr.tipo != Tipo.Bool ) expr.error("se requiere booleano en while"); 9) } 10) public void gen(int b, int a) { 11) despues = a; // guarda la etiqueta a 12) expr.salto(0, a); 13) int etiqueta = nuevaEtiqueta(); // etiqueta para instr 14) emitirEtiqueta(etiqueta); instr.gen(etiqueta, b); 15) emitir("goto L" + b); 16) } 17) }La clase Do es muy similar a la clase While. 1) package inter; // Archivo Do.java 2) import simbolos.*; 3) public class Do extends Instr { 4) Expr expr; Instr, instr;A.7 Código intermedio para las instrucciones 979Maq. Apendices_AHO.indd 979 11/10/07 1:09:22 AM980 Apéndice A. Un front-end completo 5) public Do() { expr = null; instr = null; } 6) public void inic(Instr s, Expr x) { 7) expr = x; instr = s; 8) if( expr.tipo != Tipo.Bool ) expr.error("se requiere booleano en do"); 9) } 10) public void gen(int b, int a) { 11) despues = a; 12) int etiqueta = nuevaEtiqueta(); // etiqueta para expr 13) instr.gen(b,etiqueta); 14) emitirEtiqueta(etiqueta); 15) expr.salto(b,0); 16) } 17) }La clase Est implementa asignaciones con un identificador del lado izquierdo y una expresión a la derecha. La mayor parte del código en la clase Est es para construir un nodo ycomprobar tipos (líneas 5-13). La función gen emite una instrucción de tres direcciones (líneas14-16). 1) package inter; // Archivo Est.java 2) import analizadorLexico.*; import simbolos.*; 3) public class Est extends Instr { 4) public Id id; public Expr expr; 5) public Est(Id i, Expr x) { 6) id = i; expr = x; 7) if ( comprobar(id.tipo, expr.tipo) == null ) error("error de tipo"); 8) } 9) public Tipo comprobar(Tipo p1, Tipo p2) { 10) if ( Tipo.numerico(p1) && Tipo.numerico(p2) ) return p2; 11) else if ( p1 == Tipo.Bool && p2 == Tipo.Bool ) return p2; 12) else return null; 13) } 14) public void gen(int b, int a) { 15) emitir( id.toString() + " = " + expr.gen().toString(); 16) } 17) }La clase EstElem implementa las asignaciones para un elemento de un arreglo: 1) package inter; // Archivo EstElem.java 2) import analizadorLexico.*; import simbolos.*; 3) public class EstElem extends Instr { 4) public Id arreglo; public Expr indice; public Expr expr; 5) public EstElem(Acceso x, Expr y) { 6) arreglo = x.arreglo; indice = x.indice; expr = y; 7) if ( comprobar(x.tipo, expr.tipo) == null ) error("error de tipo"); 8) } 9) public Tipo comprobar(Tipo p1, Tipo p2) { 10) if ( p1 instanceof Arreglo || p2 instanceof Arreglo ) return null; 11) else if ( p1 == p2 ) return p2; 12) else if ( Tipo.numerico(p1) && Tipo.numerico(p2) ) return p2; 13) else return null; 14) } 15) public void gen(int b, int a) { 16) String s1 = indice.reducir().toString(); 17) String s2 = expr.reducir().toString();Maq. Apendices_AHO.indd 980 11/10/07 1:09:22 AM 18) emitir(arreglo.toString() + " [ " + s1 + " ] = " + s2); 19) } 20) }La clase Sec implementa una secuencia de instrucciones. Las pruebas para las instruccionesnulas en las líneas 6-7 son para evitar las etiquetas. Observe que no se genera código para lainstrucción nula, Instr.Null, ya que el método gen en la clase Instr no hace nada. 1) package inter; // Archivo Sec.java 2) public class Sec extends Instr { 3) Instr instr1; Instr instr2; 4) public Sec(Instr s1, Instr s2) { instr1 = s1; instr2 = s2; } 5) public void gen(int b, int a) { 6) if ( instr1 == Instr.Null ) instr2.gen(b, a); 7) else if ( instr2 == Instr.Null ) instr1.gen(b, a); 8) else { 9) int etiqueta = nuevaEtiqueta(); 10) instr1.gen(b,etiqueta); 11) emitirEtiqueta(etiqueta); 12) instr2.gen(etiqueta, a); 13) } 14) } 15) }Una instrucción break envía el control hacia fuera de un ciclo o instrucción switch circundante. La clase Break utiliza el campo instr para guardar la construcción de la instruccióncircundante (el analizador asegura que Instr.Circundante denote el nodo del árbol sintácticopara la construcción circundante). El código para un objeto Break es un salto a la etiquetainstr.despues, que marca la instrucción justo después del código para instr. 1) package inter; // Archivo Break.java 2) public class Break extends Instr { 3) Instr instr; 4) public Break() { 5) if( Instr.Circundante == null ) error("break no encerrada"); 6) instr = Instr.Circundante; 7) } 8) public void gen(int b, int a) { 9) emitir( "goto L" + instr.despues); 10) } 11) }A.8 Analizador sintácticoEl analizador sintáctico lee un flujo de tokens y construye un árbol sintáctico llamando a lasfunciones constructor apropiadas de las secciones A.5-A.7. La tabla de símbolos actual se mantiene como en el esquema de traducción en la figura 2.38 de la sección 2.7.El paquete analizador contiene una clase, Analizador: 1) package analizador; // Archivo Analizador.java 2) import java.io.*; import analizadorLexico.*; import simbolos.*; import inter.*;A.8 Analizador sintáctico 981Maq. Apendices_AHO.indd 981 11/10/07 1:09:23 AM982 Apéndice A. Un front-end completo 3) public class Analizador { 4) private AnalizadorLexico lex; //analizador lexico para este analizador sintactico 5) private Token busca; // marca de busqueda por adelantado 6) Ent sup = null; // tabla de simbolos actual o superior 7) int usado = 0; // almacenamiento usado para las declaraciones 8) public Analizador(AnalizadorLexico 1) throws IOException { lex = 1; mover(); } 9) void mover() throws IOException { busca = lex.escanear(); } 10) void error(String s) { throw new Error("cerca de linea "+lex.linea+": "+s); } 11) void coincidir(int t) throws IOException { 12) if( busca.etiqueta == t ) mover(); 13) else error("error de sintaxis"); 14) }Al igual que el traductor de expresiones simple de la sección 2.5, la clase Analizador tiene unprocedimiento para cada no terminal. Los procedimientos se basan en una gramática que se forma al eliminar la recursividad izquierda de la gramática del lenguaje fuente en la sección A.1.El análisis sintáctico empieza con una llamada al procedimiento programa, el cual llama abloque() (línea 16) para analizar el flujo de entrada y construir el árbol sintáctico. Las líneas17-18 generan el código intermedio. 15) public void programa() throws IOException { // programa −> bloque 16) Instr s = bloque(); 17) int inicio = s.nuevaEtiqueta(); int despues = s.nuevaEtiqueta(); 18) s.emitirEtiqueta(inicio); s.gen(inicio, despues); s.emitirEtiqueta(despues); 19) }El manejo de la tabla de símbolos se muestra de manera explícita en el procedimientobloque.3 La variable sup (declarada en la línea 5) contiene la tabla de símbolos superior; lavariable entGuardado (línea 21) es un enlace a la tabla de símbolos anterior. 20) Instr bloque() throws IOException { // bloque −> { decls instrs } 21) coincidir(’{’); Ent entGuardado = sup; sup = new Ent(sup); 22) decls(); Instr s = instrs(); 23) coincidir(’}’); sup = entGuardado; 24) return s; 25) }Las declaraciones producen entradas en la tabla de símbolos para los identificadores (vea lalínea 36). Aunque no se muestra aquí, las declaraciones también pueden producir instruccionespara reservar almacenamiento para los identificadores en tiempo de ejecución. 26) void decls() throws IOException { 27) while( busca.etiqueta == Etiqueta.BASIC ) { // D −> tipo ID; 28) Tipo p = tipo(); Token tok = busca; coincidir(Etiqueta.ID); coincidir(’;’); 29) Id id = new Id((Palabra)tok, p, usado); 30) sup.put( tok, id ); 31) usado = usado + p.anchura; 32) } 33) } 34) Tipo tipo() throws IOException { 35) Tipo p = (Tipo)busca; // espera busca.etiqueta == Etiqueta.BASIC3 Una alternativa atractiva es agregar los métodos push y pop a la clase Ent, para que se pueda acceder a la tablaactual a través de una variable estática Ent.sup.Maq. Apendices_AHO.indd 982 11/10/07 1:09:23 AM 36) coincidir(Etiqueta.BASIC); 37) if( busca.etiqueta != ’[’ ) devuelve p; // T −> basico 38) else return dims(p); // devuelve el tipo del arreglo 39) } 40) Tipo dims(Tipo p) throws IOException { 41) coincidir(’[’); Token tok = busca; coincidir(Etiqueta.NUM); coincidir(’]’); 42) if( busca.etiqueta == ’[’ ) 43) p = dims(p); 44) return new Arreglo(((Num)tok).valor, p); 45) }El procedimiento instr tiene una instrucción switch con instrucciones case que corresponden a las producciones para la no terminal Instr. Cada case construye un nodo para unaconstrucción, usando las funciones constructor que vimos en la sección A.7. Los nodos para lasinstrucciones while y do se construyen cuando el analizador ve la palabra clave de apertura.Los nodos se construyen antes de que se analice la instrucción para permitir que cualquierinstrucción break encerrada apunte de vuelta a su ciclo circundante. Los ciclos anidados se manejan mediante el uso de la variable Instr.Circundante en la clase Instr y instrGuardada(declarada en la línea 52) para mantener el ciclo circundante actual. 46) Instr instrs() throws IOException { 47) if ( busca.etiqueta == ’}’ ) return Instr.Null; 48) else return new Sec(instr(), instrs()); 49) } 50) Instr instr() throws IOException { 51) Expr x; Instr s, s1, s2; 52) Instr instrGuardada; // guarda ciclo circundante p/instrucciones break 53) switch( busca.etiqueta ) { 54) case ’;’: 55) mover(); 56) return Instr.Null; 57) case Etiqueta.IF; 58) coincidir(Etiqueta.IF); coincidir(’(’); x = bool(); coincidir(’)’); 59) s1 = instr(); 60) if( busca.etiqueta != Etiqueta.ELSE ) return new If(x, s1); 61) coincidir(Etiqueta.ELSE); 62) s2 = instr(); 63) return new Else(x, s1, s2); 64) case Etiqueta.WHILE: 65) While nodowhile = new While(); 66) instrGuardada = Instr.Circundante; Instr.Circundante = nodowhile; 67) coincidir(Etiqueta.WHILE); coincidir(’(’); x = bool(); coincidir(’)’); 68) s1 = instr(); 69) nodowhile.inic(x, s1); 70) Instr.Circundante = instrGuardada; // restablece Instr.Circundante 71) return nodowhile; 72) case Etiqueta.DO; 73) Do nododo = new Do(); 74) instrGuardada = Instr.Circundante; Instr.Circundante = nododo; 75) coincidir(Etiqueta.DO); 76) s1 = instr(); 77) coincidir(Etiqueta.WHILE); coincidir(’(’); x = bool(); coincidir(’)’); coincidir(’;’); 78) nododo.inic(s1, x); 79) Instr.Circundante = instrGuardada; // restablece Instr.Circundante 80) return nododo; 81) case Etiqueta.BREAK;A.8 Analizador sintáctico 983Maq. Apendices_AHO.indd 983 11/10/07 1:09:24 AM984 Apéndice A. Un front-end completo 82) coincidir(Etiqueta.BREAK); coincidir(’;’); 83) return new Break(); 84) case ’{’: 85) return bloque(); 86) default: 87) return asignar(); 88) } 89) }Por conveniencia, el código para las asignaciones aparece en un procedimiento auxiliar,asignar. 90) Instr asignar() throws IOException { 91) Instr instr; Token t = busca; 92) coincidir(Etiqueta.ID); 93) Id id = sup.get(t); 94) if( id == null ) error(t.toString() + " no declarado"); 95) if( busca.etiqueta == ’=’ ) { // S −> id = E ; 96) mover(); instr = new Est(id, bool()); 97) } 98) else { // S −> L = E ; 99) Acceso x = desplazamiento(id);100) coincidir(’=’); instr = new EstElem(x, bool());101) }102) coincidir(’;’);103) return instr;104) }El análisis sintáctico de las expresiones aritméticas y booleanas es similar. En cada caso, secrea un nodo de árbol sintáctico apropiado. La generación de código para cualquiera de las dosexpresiones es distinta, como vimos en las secciones A.5 y A.6.105) Expr bool() throws IOException {106) Expr x = unir();107) while( busca.etiqueta == Etiqueta.OR ) {108) Token tok = busca; mover(); x = new Or(tok, x, unir());109) }110) return x;111) }112) Expr unir() throws IOException {113) Expr x = igualdad();114) while( busca.etiqueta == Etiqueta.AND ) {115) Token tok = busca; mover(); x = new And(tok, x, igualdad());116) }117) return x;118) }119) Expr igualdad() throws IOException {120) Expr x = rel();121) while( busca.etiqueta == Etiqueta.EQ || busca.etiqueta == Etiqueta.NE ) {122) Token tok = busca; mover(); x = new Rel(tok, x, rel());123) }124) return x;125) }126) Expr rel() throws IOException {127) Expr x = expr();128) switch( busca.etiqueta ) {Maq. Apendices_AHO.indd 984 11/10/07 1:09:24 AM129) case ’<’: case Etiqueta.LE: case Etiqueta.GE: case ’>’:130) Token tok = busca; mover(); return new Rel(tok, x, expr());131) default:132) return x;133) }134) }135) Expr expr() throws IOException {136) Expr x = term();137) while( busca.etiqueta == ’+’ || busca.etiqueta == ’−’ ) {138) Token tok = busca; mover(); x = new Arit(tok, x, term());139) }140) return x;141) }142) Expr term() throws IOException {143) Expr x = unario();144) while(busca.etiqueta == ’*’ || busca.etiqueta == ’/’) {145) Token tok = busca; mover(); x = new Arit(tok, x, unario());146) }147) return x;148) }149) Expr unario() throws IOException {150) if( busca.etiqueta == ’−’ ) {151) mover(); return new Unario(Palabra.minus, unario());152) }153) else if( busca.etiqueta == ’!’ ) {154) Token tok = busca; mover(); return new Not(tok, unario());155) }156) else return factor();157) }El resto del código en el analizador sintáctico trata con los “factores” en las expresiones. Elprocedimiento auxiliar desplazamiento genera código para los cálculos de las direcciones delos arreglos, como vimos en la sección 6.4.3.158) Expr factor() throws IOException {159) Expr x = null;160) switch( busca.etiqueta ) {161) case ’(’:162) mover(); x = bool(); coincidir(’)’);163) return x;164) case Etiqueta.NUM:165) x = new Constante(busca, Tipo.Int); mover(); return x;166) case Etiqueta.REAL:167) x = new Constante(busca, Tipo.Float); mover(); return x;168) case Etiqueta.TRUE:169) x = Constante.True; mover(); return x;170) case Etiqueta.FALSE:171) x = Constante.False mover(); return x;172) default:173) error("error de sintaxis");174) return x;175) case Etiqueta.ID:176) String s = busca.toString();177) Id id = sup.get(busca);178) if( id == null ) error(busca.toString() + " no declarado");179) mover();A.8 Analizador sintáctico 985Maq. Apendices_AHO.indd 985 11/10/07 1:09:25 AM986 Apéndice A. Un front-end completo180) if (busca.etiqueta != ’[’ ) return id;181) else return desplazamiento(id);182) }183) }184) Acceso desplazamiento(Id a) throws IOException { // I −> [E] | [E] I185) Expr i; Expr w Expr t1, t2; Expr ubic; // hereda id186) Tipo tipo = a.tipo;187) coincidir(’[’); i = bool(); coincidir(’]’); // primer indice, I −> [ E ]188) tipo = ((Arreglo)tipo).de;189) w = new Constante(tipo.anchura);190) t1 = new Arit(new Token(’*’), i, w);191) ubic = t1;192) while( busca.etiqueta == ’[’ ) { // multi−dimensional I −> [ E ] I193) coincidir(’[’); i = bool(); coincidir(’[’);194) tipo = ((Arreglo)tipo).de;195) w = new Constante(tipo.anchura);196) t1 = new Arit(new Token(’*’), i, w);197) t2 = new Arit(new Token(’+’), ubic, t1);198) ubic = t2;199) }200) return new Acceso(a, ubic, tipo);201) }202) }A.9 Creación del front-endEl código para los paquetes aparece en cinco directorios: main, analizadorLexico, simbolos,analizador e inter. Los comandos para crear el compilador varían de sistema en sistema. Lossiguientes comandos son para una implementación en UNIX:javac analizadorLexico/*.javajavac simbolos/*.javajavac inter/*.javajavac analizador/*.javajavac main/*.javaEl comando javac crea archivos .class para cada clase. Después podemos probar el traductor escribiendo java main.Main, seguido del programa fuente que se desea traducir; porejemplo, el contenido del archivo prueba: 1) { // Archivo prueba 2) int i; int j; float v; float x; float[100] a; 3) while( true ) { 4) do i = i+1; while( a[i] < v); 5) do j = j−1; while( a[j] > v); 6) if( i >= j ) break; 7) x = a[i]; a[i] = a[j]; a[j] = x; 8) } 9) }En esta entrada, el front-end produce lo siguiente:Maq. Apendices_AHO.indd 986 11/10/07 1:09:25 AMPruébelo.A.9 Creación del front-end 987Maq. Apendices_AHO.indd 987 11/10/07 1:09:26 AMMaq. Apendices_AHO.indd 988 11/10/07 1:09:27 AMApéndice BBúsqueda de solucioneslinealmente independientes989Algoritmo B.1: Busca un conjunto máximo de soluciones linealmente independientes para, y las expresa como filas de la matriz B.ENTRADA: Una matriz A de m 3 n.SALIDA: Una matriz B de soluciones linealmente independientes para .MÉTODO: El algoritmo se muestra en seudocódigo a continuación. Observe que X[y] denotala y-ésima fila de la matriz X, X[y : z] denota las filas y a z de la matriz X, y X[y : z][u : v] denota el rectángulo de la matriz X en las filas de la y a la z y las columnas de la u a la v. ✷Maq. Apendices_AHO.indd 989 11/10/07 1:09:27 AMM = AT;r 0 = 1;c 0 = 1;B = In×n; /* una matriz identidad de n por n */while ( true ) { /* 1. Convertir a M[r0 : r  – 1][c0 : c  – 1] en una matriz diagonal con entradas diagonales positivas y M[r : n][c0 : m] = 0.M[r : n] son soluciones. */r = r0; c  = c0;while ( exista M[r ][c ] =/ 0 de tal forma quer – r  y c – c sean ambas ¦ 0 ) { Mover pivote M[r][c] a M’[r][c] mediante intercambio de fila y columna; Intercambiar fila r con fila r en B;if (M[r][c] < 0 ) {M[r] = –1 * M[r]; B[r] = –1 * B[r];}for ( fila = r 0 a n ) {if ( fila   r y M[fila][c] =/ 0 { u = –(M[fila][c]/M[r][c]); M [fila] = M[fila] + u * M[r]; B[fila] = B[fila] + u * B[r];}}r = r  + 1; c = c + 1;}990 Apéndice B. Búsqueda de soluciones linealmente independientesMaq. Apendices_AHO.indd 990 11/10/07 1:09:28 AMApéndice B Búsqueda de soluciones linealmente independientes 991 /* 2. Buscar una solución aparte de M[r : n]. Debe ser una combinación no negativa de M[r 0 : r – 1][c 0 : m] */ Buscar kr 0, …, kr–1 ¦ 0 de tal forma quekr 0M[r0][c : m] + . . . + kr–1M[r – 1][c : m] ¦ 0; if ( existe una solución no trivial, por decir kr > 0 ) {M[r] = Kr 0M[r0]+ . . . + kr–1M[[r – 1]; NoMásSols = false; } else /* M[r : n] son las únicas soluciones */NoMasSols = true; /* 3. Hacer que M[r0 : rn – 1][c0 : m] ¦ 0 */ if ( NoMásSols ) { /* Mover soluciones M[r : n] a M[r 0 : rn – 1] */for ( r = r hasta n )Intercambiar filas r y r 0 + r – r en M y B;rn = r0 + n – r + 1; else { /* Usar suma de filas para encontrar más soluciones */rn = n + 1;for ( col = c hasta m )if (existe M[fila][col ] < 0 tal que fila ¦ r 0 ) if ( existe M[r ][col ] > 0 tal que r ¦ r 0 ) for (fila = r0 hasta rn – 1 ) if ( M [fila][col ] < 0 ) { u = ⎡(–M[fila][col ]/M[r ][col ])⎤; M[fila] = M[fila] + u * M[r ]; B[fila] = B[fila] + u * B[r ]; } else for ( fila = rn – 1 hasta r0 en pasos de –1 ) if ( M [fila][col ] < 0 { rn = rn – 1; Intercambiar M[fila] con M[rn]; Intercambiar B[fila] con B[rn]; }} /* 4. Hacer que M[r0 : rn – 1][1 : c0 – 1] ¦ 0 */ for ( fila = r0 hasta rn – 1 )for ( col = 1 hasta c0 – 1 )if ( M[fila][col ] < 0 { Elegir una r tal que M[r][col ] > 0 y r < r0 ; u = ⎡(–M[fila][col ]/M[r][col ])⎤; M[fila] = M[fila] + u * M[r ]; B[fila] = B[fila] + u * B[r ];}Maq. Apendices_AHO.indd 991 11/10/07 1:09:29 AM /* 5. Si es necesario, repetir con las filas M[rn : n] */ if ( (NoMásSols o rn > n o rn == r0) { Eliminar las filas desde rn hasta n de B;return B;} else {cn = m + 1;for ( col = m hasta 1 en pasos de –1 )if ( no hay M[r ][col ] > 0 tal que r < rn { cn = cn – 1; Intercambiar columna col con cn en M;}r0 = rn;c0 = cn;}}992 Apéndice B. Búsqueda de soluciones linealmente independientesMaq. Apendices_AHO.indd 992 11/10/07 1:09:29 AMÍndice993 AAbstracto, árbol sintácticoVea Sintáctico, árbolAbu-Sufah, W., 900Acción, 58-59, 249, 327Aceptación, 149Acíclica cadena de llamadas, 910 prueba, 821-822Acíclico, camino, 667Ada, 391AFDVea Determinista, autómata fi nitoAfín acceso a arreglo, 781, 801-804, 815-826 expresión, 687-770 partición de espacio, 830-838 particionamiento, 781 transformación, 778-782, 846-851AFNVea No determinista, autómata fi nitoAho, A. V., 189-190, 301, 579-580Aho-Corasick, algoritmo de, 138-140Al instante, generación, 340-343, 380-381Alcance, 86 defi niciones de, 601-608, 615Alfabeto, 117Algebraicas, identidades, 536-552Alias, 35, 713, 903, 917, 933Alineación, 374, 428Allen, F. E., 704, 899-900, 962Allen, R., 900Almacenamiento distribución del, 373 instrucción de, 512Vea también Dinámico,almacenamiento; Estático, almacenamiento; Antidependencia;Salida, dependencia deAlpha, 703Altura de un semienrejado, 623, 626, 628Ambigua, gramática, 47, 203-204, 210-212,255, 278-283, 291-294Amdahl, ley de, 774Ampliación, 388-389Análisis, 4 del fl ujo de datos, framework, 618 sintáctico, árbol de, 45-48, 201-204Vea también Anotado, árbol de análisis sintácticoAnalizador sintáctico, 8, 41, 45, 60-61,110-111, 191-302, 981-986 sintáctico, estado del, 241-242Vea también Conjunto de elementos;Ascendente, analizador sintáctico;Descendente, analizador sintácticodeAnalizadores sintácticos, generador deVea Antlr, CUP, YaccAncestro, 46Anchura de un tipo, 374Andersen, L., 962Anidadas, declaraciones de procedimientos,442-445Anotado, árbol de análisis sintáctico, 54Maq. Indice AHO.indd 993 11/10/07 1:10:20 AM994 ÍndiceAnticipada, expresión, 645-648, 653Antidependencia, 711, 816Antisimetría, 619Antlr, 300, 302Apuntador, 365, 373, 514, 539, 935Vea también Colgante, apuntador; Pila, apuntador deApuntadores, análisis de, 713, 903, 917,933-951Árbol, 46, 56 de activación, 430-433Árboles, reescritura de, 558-567AristaVea Avance, arista de; Posterior,arista, Crítica, arista; Cruzada,arista, Retirada, arista deAritmética, expresión, 49-50, 68-69, 378-381, 971-974Arquitectura, 19-22Arreglo, 373-375, 381-384, 537-539, 541,584, 712-713, 770, 920Vea también Afín, acceso a arregloArreglos, contracción de, 884-887Ascendente, analizador sintáctico, 233-240Vea también LR, analizador sintáctico; Desplazamiento-reducción, analizador sintácticoASCII, 117Asignación de memoria, 453Asociatividad, 48, 122, 279-281, 293, 619Átomo, 921Atribuida, gramática, 306Atributo, 54, 112 L, defi nición de, 313-314, 331-352 S, defi nición de, 306, 312-313, 324Vea también Heredado, atributo;Principal, atributo; Sintetizado,atributoAumentada, gramática, 243Auslander, M. A., 580Autoincremento, 739Autómata, 147Vea también Determinista, autómatafi nito; LR(0), autómata;No determinista, autómatafi nitoAvance, arista de, 661Avots, D., 962-963 BBackpatching, 410-417Backus, J. W., 300-301Backus-Naur, formaVea BNFBaker, algoritmo de, 475-476, 482Baker, H. G. Jr., 502Ball, T., 962Banerjee, U., 900Banning, J. P., 962Barth, J. M., 962Base átomo, 921 dirección, 381 registro, 762Básico bloque, 525-541, 597, 600-601, 721-726 tipo, 371Bauer, F. L., 354-355BDD, 951-958Bddbddb, 961Bergin, T. J., 38Berndl, M., 961-962Bernstein, D., 766-767BifurcaciónVea SaltoBifurcar y delimitar, 824-825Binaria, traducción, 22Binario, alfabeto, 117Birman, A., 301Bison, 300Bloque, 29, 86-87, 95Vea también Básico, bloqueBloques estructuraVea Estático, alcance uso de, 770-771, 785-787, 877-880, 888BNFVea Libre de contexto, gramáticaBooleana, expresión, 399-400, 403-409,411-413, 974-977Bounimova, E., 962Maq. Indice AHO.indd 994 11/10/07 1:10:23 AMÍndice 995Break, instrucción, 416-417Brooker, R. A., 354Bryant, R. E., 961-962Búfer, 115-117 desbordamiento de, 918, 920-921Burke, M., 900Bus, 772-773Bush, W. R., 962-963Búsqueda en profundidad, árbol de expansión con, 660Bytecode, 2 CC, 13, 18, 25, 28-29, 381, 498, 903, 934C++, 13, 18, 34, 498Caché, 20, 454-455, 457, 772, 783-785 interferencia, 788Cadar, C., 963Cadena, 118-119, 373Callahan, D., 961, 963CaminoVea Acíclico, camino; Crítico, camino;Ejecución, camino de; Reuniónsobre los caminos, solución; Pesode un caminoCampo, 377, 584, 935 almacenamiento de, 937 carga de, 937CanalizaciónVea Instrucciones, canalización de;Canalizaciones; Software, canalización porCanalizaciones, 861-864Canónica, derivaciónVea Por la derecha, derivación deCanónico analizador sintáctico LR, 259, 265-266, 283 LR(0), conjunto de elementos, 243, 247 LR(1), conjunto de elementos, 260-264Cantor, D. C., 300-301Caracter, clase, 123, 126Carbin, M., 963Carga, instrucción de, 512Cargador, 3Centinela, 116 derecha, forma, 201, 256 forma, 200 izquierda, forma, 201CFGVea GramáticaChaitin, G. J., 580Chandra, A. K., 580Charles, P., 900Chelf, B., 963Chen, S., 766-767, 899, 901Cheney, algoritmo de, 479-482Cheney, C. J., 502-503Cheong, G. I., 901Chomsky, Forma Normal de, 232, 300Chomsky, N., 300-301Chou, A., 963Chow, F., 579-580Church, A., 502-503Cíclica, basura, 488Ciclo, 531, 554, 556, 655-656, 775 desenrollamiento de, 735, 740-741, 743 expresión invariante de, 641-642 prueba de residuo de, 822-823Vea también Ejecución total, ciclode; Completamente permutables,ciclos; Natural, cicloCiclos anidamiento de, 780, 791, 792, 862fi sión de; Vea Fisión fusión de; Vea Fusión región de, 674Cierre, 119, 121-122, 243-245, 261-262 de funciones de transferencia, 679Vea también Positivo, cierreCircular, dependencia, 307CISC, 21, 507-508Clase, 33, 376 variable de, 25-26Clonación, 910-911Coalescencia de trozos, 459-460Cocke, J., 301, 579-580, 704, 900Cocke-Younger-Kasami, algoritmo de, 232,301Maq. Indice AHO.indd 995 11/10/07 1:10:23 AM996 ÍndiceCódigo generación de, 10-11, 505-581, 707-767Vea también Programación movimiento de, 592Vea también Hacia abajo, movimiento de código; optimización de, 5, 10, 15-19, 368, 583-705, 769-963 P, 386 programación deVea ProgramaciónCoerción, 99, 388-389Coffman, E. G., 900-901Coherente, protocolo de caché, 772-773Cola, recursividad, 73Colgante apuntador, 461 else, 210-212, 281-283Collins, G. E., 503Coloración, 556-557Columnas, orden por, 382, 785Combinación/unión, 621, 955Comentario, 77Compilación, tiempo de, 25Completamente permutables, ciclos, 861,864-867, 875-876Composición, 624, 678, 693Computadora arquitectura de Vea Arquitectura con conjunto complejo de instruccionesVea CISCComún, subexpresión, 533-535, 588-590,611, 639-641Comunicación, 828, 881-882, 894Concatenación, 119, 121-122Concurrente, recolección de basura,495-497Condición conmutativa, 122, 619 distributiva, 122Condicional, salto, 513, 527Confi guración, 249Confl icto, 144, 565Vea también Desplazamiento- reducción, confl icto;Conjunto de corte, 645 de elementos, 243-246, 257Vea también Canónico, LR(0), conjunto de elementos; Canónico,LR(1), conjunto de elementosde las partes de un conjunto (Power Set), 620Conjuntos, asociatividad de, 457Conservador, análisis de fl ujo de datos,603Constante, 78-79Constantes plegado de, 536, 632-637 propagación deVea Constantes, plegado deContenedores, uso de, trozos, 458Contexto, sensibilidad al, 906-908, 945,951Contigua, evaluación, 574Continue, instrucción, 416-417Control enlace de, 434 equivalencia de, 728Convexo, poliedro, 789-790, 795-796Cook, B., 962Cooper, K. D., 580, 963Copia instrucción de, 544, 937 propagación de, 590-591Copiado, recolector de basura de, 478-482,488, 497-498Corasick, M. J., 189-190Corto circuito, 953Cosecha, 46-47, 201Cousot, P., 704Cousot, R., 704Crítica, arista, 643Crítico camino, 725 ciclo, 758Cruzada, arista, 662Cuádruple, 366-368Cuarta generación, lenguaje de, 13Cuerpo, 43, 197, 923Cuerpo, región del, 673CUP, 300, 302Maq. Indice AHO.indd 996 11/10/07 1:10:24 AMÍndice 997CYK, algoritmoVea Cocke-Younger-Kasami, algoritmo deCytron, R., 704, 900 DDain, J., 300-301Dalton, M., 962Dantzig, G., 900Das, M., 961, 963Datalog, 921-933Datalog, programa en, 923Datos abstracción de, 18 dependencia de, 711-715, 732, 747-749, 771, 781-782, 804-805, 815-826Vea también Antidependencia; Salida, dependencia de; Verdadera, dependencia espacio de, 779-780 grafo de dependencia de, 722-723 localidad de, 891-892Vea también Localidad reutilización de, 804-815, 887-888Davidson, E. S., 767Declaración, 32, 373, 376Declarativo, lenguaje, 13Decodifi car, 708Def, 609Defi nición, 32 de una variable, 601Delimitadora condición, 615 etiqueta, 459Dependencia de control, restricción de, 710, 716-717 grafo de, 310-312Derecha, asociatividad por la, 48Derecho, ladoVea CuerpoDeRemer, F., 300-301Derivación, 44-46, 199-202Vea también Por la izquierda,derivación de; Por laderecha, derivación deDerramamiento de registros, 716Desasignación de memoria, 453, 460-463Descendente, analizador sintáctico de,61-68, 217-233Vea también Predictivo, analizadorsintáctico; Recursivo, analizadorsintáctico descendenteDescendiente, 46Desplazamiento (skewing), 377-378, 849-850 -reducción, analizador sintáctico, 236- 240 -reducción, confl icto, 238-240, 293Desreferencia, 461Destino códigoVea Objeto, código conjunto, 488 lenguaje, 1Determinista, autómata fi nito, 149-156,164-166, 170-186, 206Diagrama de decisiones binarioVea BDDDiferido, movimiento de códigoVea Parcial, eliminación de redundanciaDijkstra, E. W., 502-503Dinámica carga, 944 política, 25 programación, 573-577Vea también Cocke-Younger-Kasami, algoritmo de RAM, 456Dinámico acceso, 816 alcance, 31-33 almacenamiento, 429Vea también Montículo; Tiempo de ejecución, pila en programador, 719, 737Diofantina, ecuación, 818-820Dirección, 364, 374Direcciones descriptor de, 543, 545-547 espacio de, 427Maq. Indice AHO.indd 997 11/10/07 1:10:24 AM998 ÍndiceDirigido, grafo acíclicoVea GDADistributivo, framework, 625, 635-636Dominador, 656-659, 672, 728Dominadores, árbol de, 657Dominio de un análisis de fl ujo de datos, 599, 615 de una relación, 954Donnelly, C., 301Dumitran, D., 963 E producción, 63, 65-66Vea Vacía, cadenaEarley, J., 301Eaves, B. C., 900EDBVea también Extensional, predicado de base de datosEjecución camino de, 597, 628 cruzada, ciclos de, 743-745 total, ciclo de, 738Elemento, 242-243Vea también Núcleo, elemento;Conjunto de elementos; Válido,elementoEliminar, 601, 603, 611Emami, M., 961, 963Encabezado, 42, 197, 665, 672, 923Engler, D., 963Enlace de acceso, 434, 445-449Enrejados, diagrama de, 621-622Ensamblador, 3 lenguaje, 13, 508Entera, programación lineal, 817-825Entorno, 26-28Entrada, nodo de, 531, 605Epílogo, 742Eqn, 331Equivalencias, análisis basado en, 935Errores corrección de, 113-114, 192-196, 228-231 producción de, 196 recuperación de, 283-284, 295-297Ershov, A. P., 426, 579-580, 705Ershov, número de, 567-572Escalado, 848, 850Escaneado, estado, 474Escaneo, 110Vea también Léxico, analizadorEscritura, barrera de, 486-487Espacial localidad, 455-457, 465, 477, 884 reutilización, 806, 809-811Espacio en blanco, 41, 77-78 restricción de partición de, 831-838Vea Datos, espacio de; Iteraciones, espacio de; Nulo, espacio;Procesadores, espacio deEspeculativa, ejecución, 708, 717-719Estable conjunto, 488 estado, 742Estado, 147, 205 de aceptaciónVea Final, estado del almacenamiento del programa, 26-28 inicialVea Inicial, estado Vea también Muerto, estado;Minimización de estados; Analizador sintáctico, estado delEstática comprobación, 97-98, 357Vea también Tipos, comprobación de forma de asignación individual, 369-370 política, 25 RAM, 456 repartición, 518, 524Estático acceso, 816 alcance, 25, 28-31, 442Vea también Alcance almacenamiento, 429, 442Maq. Indice AHO.indd 998 11/10/07 1:10:25 AMÍndice 999Estratifi cado, programa en Datalog, 930-931EstructuraVea ClaseEtiqueta, 46, 364, 366Euclidiano, algoritmo, 820Expresión, 94, 96-97, 101-105, 359, 568-572 disponible, 610-615, 648-649, 653Vea también Aritmética, expresión; Booleana, expresión; Infi jo,expresión; Postfi jo, expresión;Prefi jo, expresión; Tipo, expresióndeExtensional, predicado de base de datos, 924Extremo posterior, 4, 357 FFahndrich, M., 961, 963Farkas, lema de, 872-875Fase, 11Feautrier, P., 900Feldman, S. I., 426Fenichel, R. R., 502-503Ferrante, J., 704, 900FilaVea TuplaFilas, orden por, 382, 785Final, estado, 130-131, 147, 205Finito, autómataVea AutómataFirma, 361Fischer, C. N., 580Fisher, J. A., 766-767Física dirección, 427 memoria, 454-455Fisión, 848, 850, 854Flex, 189-190Flotante, basura, 484Floyd R. W., 300-301Flujo de control, 399-409, 413-417, 525 de control, ecuación de, 600, 605 de datos, análisis de, 18, 23, 597-705, 921 grafo de, 529-531Vea también Reducible, grafo de fl ujo; Súper, grafo de fl ujode control sensibilidad al 933, 936-937Formal, parámetro, 33, 942Fortran, 113, 382, 779, 886Fortran, H., 703Fourier-Motzkin, algoritmo, 796-797Fragmentación, 457-460FrameworkVea Flujo de datos, framework deanálisis de; Distributivo, framework; Monótono, frameworkFraser, C. W., 580Frege, G., 502-503Frente de onda, 876-877Front end, 4, 40-41 357, 986FronteraVea CosechaFuente, lenguaje, 1Fuertemente, conectado, componente, 751, 859 tipifi cado, lenguaje, 387Fuerza, reducción deVea Reducción en fuerzaFunción, 29 tipo de, 371, 423Vea también ProcedimientoFuncional, lenguaje, 443Fusión, 848, 850 GGanapathi, M., 579-580Gao, G., 902GCD, 818-820GDA, 359-362, 533-541, 951Gear, C. W., 705Gen, 603, 611Gen-eliminar, forma, 603Generacional, recolección de basura, 483,488-489Geschke, C. M., 705Ghiya, R., 961, 963Gibson, R. G., 38Maq. Indice AHO.indd 999 11/10/07 1:10:25 AM1000 ÍndiceGiratorio, archivo de registro, 762Glaeser, C. D., 767Glanville, R. S., 579-580Global optimización de códigoVea Código, optimización de variable, 442GNU, 38, 426Gosling, J., 426GOTO, 246, 249, 261GrafoVea Llamadas, grafo de; GDA; Datos,grafo de dependencia de; Dependencia, grafo de; Flujo, grafo de;Dependencias del programa, grafode; ColoraciónGraham, S. L., 579-580Gramática, 42-50, 197-199, 204-205Vea también Ambigua, gramática; Aumentada, gramáticaGramatical, símbolo, 199Granularidad, 917 del paralelismo, 773-775Gross, T. R., 766-767Grune, D., 302Grupo, reutilización de, 806, 811-813Gupta, A., 900-901 HHacia abajo, movimiento de código, 731-732Hacia arriba, movimiento de código, 730-732Hacia atrás, fl ujo, 610, 615, 618, 627, 669Hacia delante, fl ujo, 615, 618, 627, 668Hallem, S., 963Halstead, M. H., 426Hanson, D. R., 580Hardware renombramiento de registros de, 714 síntesis de, 22Hecht, M. S., 705Heintze, N., 961, 963Hendren, L. J., 961-963Hennessy, J. L., 38, 579-580, 766-767, 899,901Heredado, atributo, 55, 304-305, 307Herencia, 18Hewitt, C., 502-503Hijo, 46Hoare, C. A. R., 302Hobbs, S. O., 705Hoja, 45-46 región, 673Hopcroft, J. E., 189-190, 302Hopkins, M. E., 580Hoyo, 457Hudson, R. L., 502-503Hudson, S. E., 302Huffman, D. A., 189-190Huskey, H. D., 426 IIDBVea Intensional, predicado de base de datosIdempotente, 122, 619Identidad, función, 624Identifi cador, 28, 79-80If, instrucción, 401Imperativo, lenguaje, 13Inalcanzable código; Vea Muerto, código estado, 473Inclusión, análisis basado en la, 935Incremental evaluación, 928-930 recolección de basura, 483-487 traducción; Vea Al instante, generaciónIncremento, instrucción de, 509Indexada, dirección, 513Índice, 365Indirecta, dirección, 513Indirectos, triples, 368-369Individual, producción, 232Inducción, variable de, 592-596, 687-688Inexplorado, estado, 474Inferior, elemento, 619, 622Infi jo, expresión, 40, 52-53Ingerman, P. Z., 302Iniciación, intervalo de, 745Maq. Indice AHO.indd 1000 11/10/07 1:10:25 AMÍndice 1001Inicial estado, 131, 147, 205 símbolo, 43, 45, 197Inicialización, 615Inmediato, dominador, 657-658Insegura, regla de Datalog, 930Inseguro, lenguaje, 498Instrucción, 93-94, 100-101, 978-981Vea también Break, instrucción;Continue, instrucción; If, instrucción; Switch, instrucción; While,instrucciónInstrucciones, canalización de, 708-709Vea también Software, canalización porIntencional, predicado de base de datos, 924Intercalación, 887-890Intermedio, código, 9, 91-105, 357-426, 507,971-981Intérprete, 2Interprocedural, análisis, 713, 903, 964Interrupción, 526Intersección, 612-613, 615, 620, 650Intraprocedural, análisis, 903Inversión, 849-850Irons, E. T., 354Iteraciones, espacio de, 779-780, 788-799Iterativo, algoritmo de fl ujo de datos,605-607, 610, 614, 626-628Izquierda asociatividad por la, 48 factorización por la, 214-215 recursividad, 67-68, 71, 212-214, 328- 331Izquierdo, ladoVea Encabezado JJacobs, C. J. H., 302Java, 2, 13, 18-19, 25, 34, 76, 381, 903, 934, 944 máquina virtual de, 507-508Jazayeri, M., 354Jerárquica, reducción, 761-762Jerárquico, tiempo, 857-859JFlex, 189-190Johnson, R. K., 705Johnson, S. C., 300-302, 355, 426, 502-503,579-580Justo a tiempo, compilación, 508JVMVea Java, máquina virtual de KKam, J. B., 705Kasami, T., 301-302, 705Kennedy, K., 899-900, 963Kernighan, B. W., 189-190Killdall, G., 704-705Kleene, S. C., 189-190 cierre de; Vea CierreKnoop, J., 705Knuth, D. E., 189-190, 300, 302, 354-355,502-503Knuth-Morris-Pratt, algoritmo de, 136-138Korenjak, A. J., 300, 302Kosaraju, S. R., 705Kuck, D. J., 766-767, 899-901Kung, H. T., 901 LLALR, analizador sintáctico, 259, 266-275,283, 287Lam, M. S., 767, 899-902, 961-964Lamport, L., 503, 766-767, 899-901Lattice, 621Vea también Semi-latticeLawrie, D. H., 900Lea, 458LeBlanc, R. J., 580Lectura, barrera de, 486Leiserson, C. E., 901Lenguaje, 44, 118Vea también Java; Fuente, lenguaje; Destino, lenguajeLesk, M. E., 189-190Leu, T., 963Levin, V., 962Lewis, P. M. II, 300, 302, 355Maq. Indice AHO.indd 1001 11/10/07 1:10:26 AM1002 ÍndiceLex, 126-127, 140-145, 166-167, 189-190,294-295Lexema, 111Léxico alcance; Vea Estático, alcance analizador, 5-7, 41, 76-84, 86, 109-190, 209-210, 294-295, 967-969 error, 194Lexicográfi co, orden, 791LeyVea Asociatividad, Condición conmu- tativa, Condición distributiva, IdempotenteLiao, S.-W., 901Libre de contexto, gramática; Vea Gramática estado, 473 lista, 459-460, 471 trozo, 457Lichtenber, J., 962Líder, 526-527Lieberman, H., 502-503Lim, A. W., 901Límites, comprobación de, 19, 24, 920-921Lineal, programaciónVea Entera, programación linealListas, programación de, 723-726Literal, 922Livshits, V. B., 962-963LL, analizador sintácticoVea Predictivo, analizador sintácticoLL, gramática, 223Llamada, 365, 423-424, 467, 518-522, 539, 541 por nombre, 35 por referencia, 34 por valor, 34 sitio de, 904, 950Llamadas cadena de, 908-910, 946-949 grafo de, 904-906, 943-944 secuencia de, 436-438LLgen, 300Local, optimización de códigoVea Básico, bloqueLocalidad, 455, 769Vea también Espacial, localidad; Temporal, localidadLógica, dirección, 427Lógico, error, 194Lohtak, O., 962Longitud variable, datos de, 438-440Loveman, D. B., 901Lowry, E. S., 579-580, 705LR de lectura anticipada, analizadorsintácticoVea LALR, analizador sintácticoLR(0), autómata, 243, 247-248, 252LR, analizador sintáctico, 53-252, 275-277,325, 348-352Vea también Canónico, analizadorsintáctico LR; LALR,analizador sintáctico;SLR, analizador sintáctico MMacro, 13Manejador, 235-236Mapeo directo, caché de, 457, 788Máquina, lenguaje, 508Marcar y compactar, 476-482 y limpiar, 471-476, 482Markstein, P. W., 580Martin, A. J., 503Martin, M. C., 963Más general, unifi cador, 393Vea también Unifi caciónMatrices, multiplicación de, 782-788Máximo, punto fi jo, 626-628, 630-631Maydan, D. E., 899, 901Mayor divisor común; Vea GCD límite inferior, 620, 622 orden, función de, 444Mayúsculas y minúsculas, sensibilidad a,125McArthur, R., 426McCarthy, J., 189-190, 502-503McClure, R. M., 302McCullough, W. S., 189-190McGarvey, C., 962McKellar, A. C., 900-901Maq. Indice AHO.indd 1002 11/10/07 1:10:26 AMÍndice 1003McNaughton, R., 189-190McNaughton-Yamada-Thompson,algoritmo, 159-161Medlock, C. W., 579-580, 705Mejor ajuste, 458Memorización, 823Memoria, 20, 772-773Vea también Montículo; Física,memoria; Almacenamiento;Virtual, memoriaMemoria fuga de, 25, 461 jerarquía de, 20, 454-455Menor límite superior, 621META, 300Metal, 918, 962Método, 29 invocación a, 33 llamada a; Vea LlamadaVea también Procedimiento; Virtual, métodoMGUVea Más general, unifi cadorMilanova, A., 962-963Milner, R., 426Minimización de estados, 180-185Minsky, M., 503Mirilla, optimización tipo, 549-552ML, 387, 443-445Mock, O., 426Modo de pánico recolección de basura en, 492-493. recuperación en, 195-196, 228-230, 283-284Modular expansión de variables, 758-761 tabla de reservación de recursos, 746-747, 758Monótono, framework, 624-628, 635Montículo, 428-430, 452-463, 518, 935Moore, E. F., 189-190MOPVea Reunión sobre los caminos, soluciónMorel, E., 705Morris, D., 354Morris, J. H., 189-190Moss, J. E. B., 502-503Motwani, R., 189-190, 302Mowry, T. C., 900-901Muerta, variable, 608Muerto código, 533, 535, 550, 591-592 estado, 172, 183Multiprocesador, 772-773, 895Vea también SIMD; Un solo programa, varios datosMuraoka, T., 766-767, 899, 901Mutador, 464Muy larga, palabra de instrucciónVea VLIWMuy ocupada, expresiónVea Anticipada, expresión NNAA, 690NAC, 633Natural, ciclo, 667, 673Naur, P., 300, 302Neliac, 425Nivel de frase, recuperación a, 196, 231No determinista, autómata fi nito, 147-148,152-175, 205, 257No reducible, grafo de fl ujoVea Reducible, grafo de fl ujoNo terminal, 42-43, 45, 197-198 símbolo, 349No uniforme, acceso a memoria, 773Nodo, 46 mezcla de, 953Nombre, 26-28Núcleo, 777 elemento de, 245, 272-273Nulidad, 808Nullable, 175-177Nulo, espacioNUMAVea No uniforme, acceso a memoriaMaq. Indice AHO.indd 1003 11/10/07 1:10:27 AM1004 Índice OO grande (Big-oh), 159Objeto código, 358Vea también Código, generación de programa, 427-428Objetos creación de, 937 propiedad de, 462 sensibilidad a, 950Obtener, 708Ogden, W. F., 354Olsztyn, J., 426Ondrusek, B., 962OptimizaciónVea Código, optimización deOración, 200Ordenado, BDD, 952Orientado a objetos, lenguajeVea C++; Java PPaakki, J., 354-355Padre, 46Palabra clave, 50-51, 79-80, 132-133Panini, 300Papua, D. A., 902Parafrasear, 899Paralelismo, 19-20, 707-902, 917Paralelo, recolección de basura en, 495-497Paramétrico, polimorfi smo, 391Vea también Polimorfi smoParámetro(s), 422 actual, 33, 434, 942 paso de, 33-35, 365Parcial eliminación de redundancia, 639-655 orden, 619-621, 623 recolección de basura, 483, 487-494Parcialmente muerta, variable, 655Parcialmente ordenado, conjuntoVea PosetParr, T., 302Pasada, 11Paso de mensajes, máquina de, 773, 894 directo, código de, 406Patel, J. H., 767Patrón, 111 coincidencia de, en árboles, 563-567Patterson, D. A., 38, 579-580, 766-767, 899,901Pausa corta, recolección de basura, 483-494 tiempo de, 465Vea también Pausa corta, recolección de basuraPDGVea Programa, grafo de dependencias dePelegri-Llopart, E., 580Permutación, 849-850Peso de un camino, 822Peterson, W. W., 705PFC, 899Phoenix, 38Pierce, B. C., 426Pila, 325, 518, 520Vea también Tiempo de ejecución, pila en apuntador de, 437 máquina de, 507Pincus, J. D., 962-963Pitts, W., 189-190Pnueli, A., 964PoliedroVea Convexo, poliedroPolimorfi smo, 391-395Poner en línea, 903-904, 914Por adelantado, 78, 144-145, 171-172,272-275Por la derecha, derivación de, 201Por la izquierda, derivación de, 201Por profundidad búsqueda, 57 orden, 660Porterfi eld, A., 900, 902Posdominador, 728Poset, 619Positivo, cierre, 123Postergable, expresión, 646, 649, 651-654Maq. Indice AHO.indd 1004 11/10/07 1:10:27 AMÍndice 1005Posterior, arista, 662, 664-665Postfi jo esquema de traducción, 324-327 expresión, 40, 53-54Postorden, recorrido, 58, 432Vea también Por profundidad, ordenPratt, V. R., 189-190PREVea Parcial, eliminación de redundanciaPrecedencia, 48, 121-122, 279-281, 293-294Predecesor, 529Predicada, ejecución, 718, 761Predicado, 921-922Predictivo, analizador sintáctico, 64-68,222-231, 343-348Prefi jo, 119, 918, 962 expresión, 327Preobtención, 718, 896Preobtener, 457Preorden, recorrido, 58, 432Preprocesador, 3Primer ajuste, 458Primera expresión, 649-650, 654 generación, lenguaje de, 13PRIMERO, 220-222Principal, atributo, 341Priorizado, orden topológico, 725-726Private, 31Privatizable, variable, 758Procedimiento, 29, 422-424 llamada a; Vea Llamada parámetros de, 448-449Procesadores, espacio de, 779-781, 838-841Producción, 42-43, 45, 197, 199Vea también Errores, producción deProductos, semienrejado de, 622-623Proebsting, T. A., 580Profundidad de un grafo de fl ujo, 665Programa, grafo de dependenciasde, 854-857Programación, 710-711, 716 lenguaje de, 12-14, 25-35Vea también Ada, C, C++, Fortran, Java, MLPrólogo, 742Propia, reutilización, 806-811Prosser, R. T., 705Protected, 31Proximidades, compactación de las, 736Proyección, 955PTRAN, 900Public, 31Pugh, W., 899, 902PulsoVea RelojPunto fi joVea Máximo, punto fi joPurifi car, 25, 452 QQian, F., 962Quicksort, 431-432, 585Quinta generación, lenguaje de, 13 RRabin, M. O., 189-190Raíz, 46 conjunto, 466-467, 488Rajamani, S. K., 962Randell, B., 502-503Rango completo, matriz de, 808 de una matriz, 807-809Rastreo, recolección de basura basada en470-471Vea también Marcar y compactar; Marcar y limpiarRau, B. R., 767Recolección de basura, 25, 430, 463-499Vea también Marcar y compactar;Marcar y limpiar; Pausa corta,recolección de basura conRecordado, conjunto, 491Recorrido, 56-57Vea también Por profundidad,búsqueda; Postorden, recorrido;Preorden, recorridoMaq. Indice AHO.indd 1005 11/10/07 1:10:28 AM1006 ÍndiceRecursivo analizador sintáctico de descenso, 64, 219-222 descenso, 338-343 tipo, 372Recursos restricción de, 711 tabla de reservación de, 719-720Vea también Modular, tabla de reservación de recursosReducción, 234, 324, 388-389 en fuerza, 536, 552, 592-596Reducible, grafo de fl ujo, 662, 664, 673-677,684-685Reducido, computadora con conjunto deinstruccionesVea RISCRedundancia completa, 645Referencia conteo de, 462-463, 466, 468-470 variable de, 34, 686-689Vea ApuntadorRefl exión, 944-945Refl exividad, 619Región, 672-686, 694-699, 733-734, 911Regiones, asignación basada en, 463Registro, 18, 20, 371, 376-378, 454-455,542-543, 584, 714-715 de activación, 433-452 descriptor de, 543, 545-547Vea también Seudorregistro, Giratorio, archivo de registroRegistros asignación de, 510, 556 par de, 510 renombramiento de; Vea Hardware, renombramiento de registros repartición de, 510-512, 533-557, 570-572, 716, 743Regla, 922-923Regular defi nición, 123 expresión, 116-122, 159-163, 179-180, 189, 210Rehof, J., 961, 963Reindexado, 848, 850Relación, 922, 954Relativa, dirección, 371, 373, 381Reloj, 708Renvoise, C., 705Reservada, palabraVea Palabra claveRestricciónVea Dependencia de control,restricción de; Datos, dependenciade; Recursos, restricción deResumen, análisis basado en el, 911-914Retirada, arista de, 661, 664-665Retorno, 365, 467, 518-522, 906, 942 valor de, 434Reunión, 605, 615, 618-619, 622-623, 633,678, 695 sobre los caminos, solución, 629-631ReutilizaciónVea Datos, reutilización deRevestimiento, 560-563Rinard, M., 962-963RISC, 21, 507-508Ritchie, D. M., 426, 502-503Rodeh, M., 766-767Rosen, B. K., 704Rosenkrantz, D. J., 355Rothberg, E. E., 900-901Rounds, W. C., 354Rountev, A., 962-963Roy, D., 963Russell, L. J., 502-503Ruwase, O., 962-963Ryder, B. G., 962-963 SSadgupta, S., 767Salida bloque de, 677 dependencia de, 711, 816 nodo de, 605Salto, 513, 527, 551-552 código de, 408, 974-977Samelson, K., 354-355Sarkar, V., 902Maq. Indice AHO.indd 1006 11/10/07 1:10:28 AMÍndice 1007SCCVea Fuertemente conectado, componenteScholten, C. S., 503Schorre, D. V., 302Schwartz, J. T., 579, 581, 704Scott, D., 189-190Scott, M. L., 38SDDVea Sintaxis, defi nición orientada por laSDTVea Sintaxis, traducción orientada por laSDV, 962Secuencias de comandos, lenguaje de,13-14Secundario almacenamiento, 20 efecto, 306, 314-316, 727Sedgewick, R., 585Seguimiento, 220-222Siguiente pos, 177-179Segunda generación, lenguaje de, 13SeguridadVea Conservador, análisis de fl ujo de datosSemántica, 40 Regla; Vea Sintaxis, defi nición orientada por laSemántico análisis, 8-9 error, 194Semienrejado, 618-623SensibilidadVea Contexto, sensibilidad al; Flujo, sensibilidad alSensible al contexto, análisis, 906-907,945-950Sethi, R., 38, 579, 581Seudorregistro, 713Shannon, C., 189-190Sharif, M., 964Shostak, R., 902Sielaff, D. J., 962-963Siguiente ajuste, 458-459Simbólica, constante, 793Simbólico análisis, 686-699 mapa, 690Símbolos, tabla de, 4-5, 11, 85-91, 423,970-971SIMD, 21, 895-896Simétrico, multiprocesador, 772Simple, defi nición orientada por la sintaxis,56Simulación, 23Sin , gramática, 232Sin contexto, lenguaje, 200, 215-216Sincronización, 828, 832, 853-854, 880-882Sintáctico análisis; Vea Analizador sintáctico árbol, 41, 69-70, 92-93, 318-321, 358, 367, 981-986Sintaxis, 40 defi nición orientada por la, 54-56, 304-316 error de, 194 traducción orientada por la, 40, 57-60, 324-352Vea también GramáticaSíntesis, 4Sintetizado, atributo, 54-56, 304-305SLAM, 962SLR, analizador sintáctico, 252-257, 283SMPVea Simétrico, multiprocesadorSobrecarga, 99, 390-391Software canalización por, 738-763, 895 productividad de, 23-25 vulnerabilidad del; Vea Vulnerabilidad del softwareSólido, sistema de tipos, 387Solución ideal para un problema de fl ujo dedatos, 628-630SORVea Sucesiva, sobre relajaciónSPMDVea Un solo programa, varios datosSQL, 22-23 inyección de, 918-919Maq. Indice AHO.indd 1007 11/10/07 1:10:29 AM1008 ÍndiceSSAVea Estática, forma de asignación individualStallman, R., 301Stearns, R. E., 300, 302, 355Steel, T., 426Steensgaard, B., 961, 964Steffens, E. F. M., 503Strong, J., 426Subcadena, 119Subconjuntos, construcción de, 153-154Submeta, 923Subsecuencia, 119Sucesiva, sobre relajación, 863Sucesor, 529Sufi jo, 119Súper grafo de control de fl ujo, 906Superescalar, máquina, 710Superior, elemento, 619, 622Switch, instrucción, 418-421 TT1-T2, reducción, 677TablaVea Relación; Recursos, tabla dereservación de; Símbolos, tabla de;Transiciones, tabla deTakizuka, T., 767Tamura, E., 767Tardieu, O., 961, 963Tareas, paralelismo de, 776Temporal localidad, 455-457, 777, 884-885 reutilización, 806Tercera generación, lenguaje de, 13Terminal, 42-43, 45, 197-198, 305TeX, 331Thompson, K., 189-190Tiempo, restricción de partición de, 868-875, 989-992Tiempo de ejecución, 25 entorno, 427 pila en, 428-451, 468Tipo, 938 expresión de, 371-372, 393, 395 variable de, 391Vea también Básico, tipo; Función, tipo de; Recursivo, tipoTipos comprobación de, 24, 98-99, 370, 386-398 conversión de, 388-390Vea también Coerción equivalencia de, 372-373 inferencia de, 387, 391-395 seguridad de, 19, 464-465 síntesis de, 387Tjiang, S. W. K., 579-580TMG, 300Token, 41, 43, 76, 111Tokura, N., 705Tokuro, M., 767Topológico, orden, 312Vea también Priorizado, orden topológicoTorczon, L., 580, 963Towle, R. A., 902Transferencia barrera de, 486 función de, 599-600, 603-604, 615, 623-624, 629, 634, 676-679, 691-693Transición, 205 diagrama de, 130-131, 147-148 función de, 147, 150Transiciones, tabla de, 148-149, 185-186Transitividad, 619-620Tren, algoritmo de, 483, 490-493Tres direcciones, código de, 42, 99, 363-369Triple, 367-368Tritter, A., 426Trozo, 457-459Tupla, 954 UUbicación, 26-28Ullman, J. D., 189-190, 301-302, 579, 581,705, 962, 964Última expresión, 649, 654Ultimapos, 175-177Umanee, N., 962Maq. Indice AHO.indd 1008 11/10/07 1:10:29 AMÍndice 1009Un solo programa, varios datos, 776Una sola instrucción, varios datosVea SIMDUNCOL, 425UNDEF, 633Unifi cación, 393, 395-398Unión, 119, 121-122, 605, 613, 615, 620,650, 955-957Unkel, C., 963Uso, 609 antes de la defi nición, 602 conteo de, 554-555Ustuner, A., 962Utilizada expresión, 649, 653-654 variable, 528-529 VVacía, cadena, 44, 118, 121Válido, elemento, 256Valor, 26-27Valor-l, 26, 98Vea también UbicaciónValor-número, 360-362, 390Valor-r, 26, 98Variable, 26-28Vea también No terminal; Referencia, variable deVariables expansión de; Vea Modular, expansión de variables independientes, prueba de, 820-821Vectores, máquina de, 886, 895-896Veneno, bit, 718Verdadera, dependencia, 711, 815Viable, prefi jo, 256-257Virtual máquina, 2Vea también Java, máquina virtual de memoria, 454-455 método, 904, 916-917, 934, 941-944Visualización, 449-451Viva, variable, 528-529, 608-610, 615VLIW, 19-21, 710Von Neumann, lenguaje de, 13Vulnerabilidad del software, 917-921Vyssotsky, V., 704-705 WWeber, H., 300, 302Wegman, M. N., 704Wegner, P., 704-705Wegstein, J., 426Weinberger, P. J., 189-190Weinstock, C. B., 705Wexelblat, R. L., 38Whaley, J., 961, 963-964While, instrucción, 401Widom, J., 962, 964Wilderness, trozo, 458Wilson, P. R., 502-503Wirth, N., 300, 302, 426Wolf, M. E., 900-902Wolfe, M. J., 902Wonnacott, D., 899, 902Wood, G., 767Wulf, W. A., 705 YYacc, 287-297, 354Yamada, H., 189-190Yochelson, J. C., 502-503Younger, D. H., 301-302 ZZadeck, F. K., 704Zhu, J., 961, 964Maq. Indice AHO.indd 1009 11/10/07 1:10:30 AMAlfred V. Aho es el profesor Lawrence Gussman de ciencias de la computación en la Columbia University. El profesor Aho ha obtenido varios premios, incluyendo el Great Teacher Awardpara el año 2003, de la Sociedad de graduados de Columbia y la medalla John von Neumanndel Instituto de Ingenieros en Electricidad y Electrónica (IEEE). Es miembro de la Academia Nacional de Ingeniería (NAE) y miembro titular de la Asociación de Maquinaria de laComputación (ACM), y del IEEE.Monica S. Lam es profesora de ciencias de la computación en la Stanford University, fue jefade científicos en Tensilica, y es presidenta fundadora de moka5. Dirigió el proyecto SUIF, elcual produjo uno de los compiladores de investigación más populares, y fue pionera en numerosas técnicas de compiladores que se utilizan en la industria.Ravi Sethi fundó la organización de investigación en Avaya y es presidente de Avaya Labs.Fue vicepresidente ejecutivo en Bell Laboratories y director técnico de software de comunicaciones en Lucent Technologies. Ha desempeñado varios puestos de enseñanza en la PennsylvaniaState University y en la Arizona University, y ha impartido clases en la Princeton Universityy la Rutgers University. Es miembro titular de la ACM.Jeffrey D. Ullman es presidente de Gradiance Corp., y profesor emérito Stanford W.Ascherman en ciencias de la computación en la Stanford University. Entre sus intereses deinvestigación se encuentran la teoría de bases de datos, la integración de bases de datos, laminería de datos y la educación mediante la infraestructura de información. Es miembro dela NAE, miembro titular de la ACM, y ganador de los premios Karlstrom y Knuth.Maq. Indice AHO.indd 1015 11/10/07 1:10:31 AMMaq. Indice AHO.indd 1016 11/10/07 1:10:32 AM
